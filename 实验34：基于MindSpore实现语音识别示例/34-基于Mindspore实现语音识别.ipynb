{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于MindSpore实现语音识别\n",
    "本小节介绍并展示了如何使用MindSpore实现语音识别。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、实验目的\n",
    "- 了解语音识别。\n",
    "- 掌握如何使用MindSpore进行语音识别。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、语音识别介绍\n",
    "语音识别（Speech Recognition）是机器学习领域中的一个重要应用，旨在将人类语音转化为可理解和可处理的文本形式。它可以帮助计算机理解和处理人类的语言，从而实现自然语言交互、语音命令识别、语音转写等功能。在本实验中定义了一个神经网络，用于进行语音识别。\n",
    "\n",
    "网络的结构如下：\n",
    "\n",
    "1. 输入数据通过Reshape层进行形状调整，将其转换为(batch_size, 1, 124, 129)的形状。\n",
    "2. 调整后的数据经过ResizeNearestNeighbor层进行大小调整，将其调整为(32, 32)的大小。\n",
    "3. 调整大小后的数据通过BatchNorm2d层进行归一化处理。\n",
    "4. 归一化后的数据经过一个卷积层conv1，输出通道数为32。\n",
    "5. 卷积层的输出通过ReLU激活函数进行非线性变换。\n",
    "6. 经过第二个卷积层conv2，输出通道数为64。\n",
    "7. 第二个卷积层的输出再次经过ReLU激活函数进行非线性变换。\n",
    "8. 通过最大池化层maxpool进行空间维度的下采样。\n",
    "9. 经过一个Dropout层，随机地将一部分元素置为零，以避免过拟合。\n",
    "10. 将经过池化和Dropout的数据展平为一维张量。\n",
    "11. 展平后的数据通过一个全连接层dense1，输入通道数为12544，输出通道数为128。\n",
    "12. 全连接层的输出再次经过ReLU激活函数进行非线性变换。\n",
    "13. 经过第二个Dropout层，再次进行随机丢弃操作。\n",
    "14. 最后通过一个全连接层dense2，输入通道数为128，输出通道数为8。\n",
    "15. 最终输出得到(batch_size, 8)的张量。\n",
    "\n",
    "这个网络使用了一些常用的神经网络层，例如卷积层、ReLU激活函数、最大池化层、Dropout层和全连接层。这些层的组合和堆叠形成了一个深层的神经网络结构，用于对输入数据进行特征提取和学习，并生成相应的输出。\n",
    "\n",
    "网络结构图如下所示：  \n",
    "![avatar](fig/fig_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、实验环境\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=MindSpore 2.0；Python环境=3.7\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU CUDA 10.1|Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、数据处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 处理数据集\n",
    "本实验使用到了mini speech commands数据集。mini speech commands是一个用于语音识别的小型数据集，常用于学术和教育目的。它由Google团队创建，并提供给机器学习社区进行研究和开发。该数据集主要用于进行关键词识别任务，即识别特定的一组预定义关键词。这些关键词通常是日常生活中常用的命令，如\"yes\"、\"no\"、\"up\"、\"down\"、\"go\"、\"stop\"等。mini_speech_commands数据集包含了数千个短语音片段，每个片段持续约1秒，并由多个不同说话者录制。  \n",
    "下面的代码提供了该数据集的下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# 定义要下载的zip文件的URL和保存路径\n",
    "url = \"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\"\n",
    "save_path = \"mini_speech_commands.zip\"\n",
    "\n",
    "# 发送GET请求下载zip文件\n",
    "response = requests.get(url)\n",
    "\n",
    "# 将响应内容保存到文件\n",
    "with open(save_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# 解压缩zip文件\n",
    "with zipfile.ZipFile(save_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义了一个名为 `get_spectrogram` 的函数，用于读取音频文件并进行声谱图特征提取。函数首先使用 `wav.read` 函数读取音频文件，获取采样率和波形数据。然后，它创建一个大小为 `[124, 129]` 的全零矩阵 `spectrogram`，用于存储声谱图特征。接下来，函数根据波形数据的长度计算需要添加的零填充数量，并创建一个相应长度的全零数组。然后，将波形数据和零填充数据连接起来，使其长度达到 16000。接着，函数生成一个包含从 0 到 254 的整数序列，并根据该序列计算一个窗口函数。窗口函数在数字信号处理中用于预处理音频信号。最后，函数通过遍历波形数据，并将每一帧数据进行窗口函数加窗和离散傅里叶变换，得到该帧的频谱数据。经过修改后，将频谱数据存储在 `spectrogram` 矩阵中的对应位置。最终，函数返回生成的声谱图特征 `spectrogram`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import mindspore\n",
    "\n",
    "mindspore.set_context(mode=mindspore.GRAPH_MODE, device_target=\"CPU\")\n",
    "\n",
    "# 读取文件并进行特征提取的函数\n",
    "def get_spectrogram(file_path):\n",
    "    fs, waveform = wav.read(file_path)\n",
    "    # 声谱的矩阵大小[124,129]\n",
    "    # spectrogram = np.zeros([124, 129]).astype(np.float32)\n",
    "    spectrogram = np.zeros([124, 129]).astype(np.float32)\n",
    "    # 边距\n",
    "    zero_padding = np.zeros([16000 - waveform.shape[0]], dtype=np.float32)\n",
    "    waveform = waveform.astype(np.float32)\n",
    "    # 扩充到16000\n",
    "    equal_length = np.concatenate([waveform, zero_padding])\n",
    "    # 生成0-254每个整数\n",
    "    x = np.linspace(0, 254, 255, dtype=np.int32)\n",
    "    # 在数字信号处理中，加窗是音频信号预处理重要的一步\n",
    "    window = 0.54 - 0.46 * np.cos(2 * np.pi * (x) / (255 - 1))\n",
    "    for i in range(124):\n",
    "        # 帧头\n",
    "        p_start = i * 128\n",
    "        # 帧尾\n",
    "        p_end = p_start + 255\n",
    "        frame_data = equal_length[p_start:p_end]\n",
    "        frame_data = frame_data * window\n",
    "        # 离散傅里叶变化\n",
    "        spectrum = np.fft.rfft(frame_data, n=256)\n",
    "        # 经过修改后可以使得特征输出为[124,129]\n",
    "        spectrogram[i,:] = np.abs(spectrum)\n",
    "    return spectrogram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理音频文件数据集。首先从指定的数据目录中获取命令列表，然后通过随机打乱命令列表的顺序。接下来，获取所有音频文件的文件路径，并随机打乱文件路径的顺序。然后，将文件路径划分为训练集、验证集和测试集。最后，将训练集和测试集的文件路径保存到两个文本文件中，其中训练集的文件路径保存在 train_file.txt，测试集的文件路径保存在 test_file.txt。这样可以方便后续在训练和测试模型时读取相应的文件路径。  \n",
    "请注意，在此步骤和下一步骤中，需要根据所使用的操作系统来选择合适的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "\n",
    "data_dir = 'data/mini_speech_commands'\n",
    "\n",
    "# 获取命令列表\n",
    "commands = [dir_name for dir_name in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, dir_name)) and dir_name != 'README.md']\n",
    "\n",
    "# 设置随机种子\n",
    "seed = 40\n",
    "random.seed(seed)\n",
    "\n",
    "# 打乱命令顺序\n",
    "random.shuffle(commands)\n",
    "\n",
    "# 获取所有文件名\n",
    "all_files = []\n",
    "for command in commands:\n",
    "    command_path = os.path.join(data_dir, command)\n",
    "    files = glob.glob(os.path.join(command_path, '*.wav'))\n",
    "    all_files.extend(files)\n",
    "\n",
    "# 打乱文件顺序\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# 划分训练、验证和测试集\n",
    "train_files = all_files[:6400]\n",
    "val_files = all_files[6400: 6400 + 800]\n",
    "test_files = all_files[-800:]\n",
    "\n",
    "# 保存训练和测试文件列表到文件中\n",
    "train_file_path = 'train_file.txt'\n",
    "test_file_path = 'test_file.txt'\n",
    "\n",
    "# windows系统使用如下代码\n",
    "with open(train_file_path, 'w', encoding='utf-8') as file1:\n",
    "    for f in train_files:\n",
    "        file1.write(f.replace('\\\\', '\\\\\\\\').replace('/', '\\\\\\\\') + '\\n')\n",
    "\n",
    "with open(test_file_path, 'w', encoding='utf-8') as file2:\n",
    "    for f in test_files:\n",
    "        file2.write(f.replace('\\\\', '\\\\\\\\').replace('/', '\\\\\\\\') + '\\n')\n",
    "\n",
    "# Linux系统使用如下代码\n",
    "# with open(train_file_path, 'w', encoding='utf-8') as file1:\n",
    "#     for f in train_files:\n",
    "#         file1.write(f + '\\n')\n",
    "\n",
    "# with open(test_file_path, 'w', encoding='utf-8') as file2:\n",
    "#     for f in test_files:\n",
    "#         file2.write(f + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码的功能是从文件中读取音频文件路径，并为每个文件生成对应的音频特征数据和标签。函数首先打开指定的文件，并逐行读取文件内容。对于每一行，它会去除两端的空白字符，并提取出音频文件的路径。然后，它使用之前定义的 `get_spectrogram` 函数获取该音频文件的特征数据。接着，它从路径中提取出音频的标签，并根据标签在指定的词汇列表 `commands` 中找到对应的标签 ID。最后，函数将特征数据和标签 ID 作为生成器的输出，使用 `yield` 语句逐个返回。这样，可以在需要的时候逐个获取音频数据和标签，用于模型的训练和测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 具体识别的8个词\n",
    "commands = ['yes', 'no', 'up', 'down', 'right', 'left', 'go', 'stop']\n",
    "# 获取音频标签\n",
    "# 读取特征文件中的数据\n",
    "def get_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        files = f.readlines()\n",
    "    # 逐行读取\n",
    "    for line in files:\n",
    "        line = line.strip()\n",
    "        # 提取label\n",
    "        data = get_spectrogram(line) \n",
    "        # windows系统使用如下代码\n",
    "        label = line.split('\\\\\\\\')[-2]\n",
    "        # Linux系统使用如下代码\n",
    "        # label = line.split('/')[-2]        \n",
    "        label_id = commands.index(label)\n",
    "        # print(data,label_id,\"##\")\n",
    "        yield np.array(data,dtype=np.float32), label_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对训练数据集进行处理。`GeneratorDataset` 将生成器函数返回的数据转换为 MindSpore 的数据集对象，并指定了数据集中的列名为 `'data'` 和 `'label'`。`batch(64)` 对数据集进行批处理，将数据按照批次进行组织，每个批次包含 64 条数据。这样处理后的数据集可以方便地用于模型的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "\n",
    "# 意为数据集本身每一条数据都可以通过索引直接访问\n",
    "ds_train = ds.GeneratorDataset(list(get_data(train_file_path)), column_names=['data', 'label'])\n",
    "# 批处理,分为64批\n",
    "ds_train = ds_train.batch(64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、模型构建"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把音频转换为语谱图后，其实就是把语音问题转化为图像问题，图像中就包含了声音的特征，这里定义了一个CV类的网络。\n",
    "该网络包含了一系列的神经网络层和操作，用于构建一个分类网络。以下是简要说明：\n",
    "\n",
    "- `conv2d(in_channels, out_channels)` 函数实现了二维卷积操作，根据输入和输出的通道数构建一个卷积层。\n",
    "- `maxpool()` 函数实现了池化层，使用最大池化操作对输入进行下采样。\n",
    "- `Net(Cell)` 是一个继承自 `Cell` 的网络类，用于定义网络结构。\n",
    "- 在 `Net` 类的构造函数中，定义了网络中的各个层和操作，包括卷积层、激活函数、池化层、扁平化层、全连接层和 dropout 层等。\n",
    "- `construct(input_x)` 方法是网络的前向传播函数，定义了数据在网络中的流动方式。具体的流程是：将输入 `input_x` 进行形状重塑、尺寸调整、批归一化、卷积、激活函数、池化、dropout、扁平化、全连接、激活函数、dropout、全连接等操作，最后输出网络的预测结果。\n",
    "- 网络的输出是一个包含8个元素的向量，表示对8个不同类别的分类预测。\n",
    "\n",
    "总体上，该网络包含了卷积层、池化层、扁平化层、全连接层和一些常用的激活函数和正则化操作，用于构建一个简单的分类网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.nn import Conv2d\n",
    "from mindspore.nn import MaxPool2d\n",
    "from mindspore.nn import Cell\n",
    "import mindspore.ops as P\n",
    "from mindspore.nn import Dense\n",
    "from mindspore.nn import ReLU\n",
    "from mindspore.nn import Flatten\n",
    "from mindspore.nn import Dropout\n",
    "from mindspore.nn import BatchNorm2d\n",
    "\n",
    "# 实现二维卷积操作\n",
    "def conv2d(in_channels, out_channels):\n",
    "    return Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                  kernel_size=3, stride=1, pad_mode='valid',\n",
    "                  has_bias=True, weight_init='he_normal')               \n",
    "# 池化层\n",
    "def maxpool():\n",
    "    return MaxPool2d(kernel_size=(2, 2), stride=(2, 2), pad_mode='valid')\n",
    "\n",
    "# 定义网络\n",
    "class Net(Cell):\n",
    "    def __init__(self, batch_size):\n",
    "        super(Net, self).__init__()\n",
    "        # 向网络中加层\n",
    "        self.batch_size = batch_size\n",
    "        self.reshape = P.Reshape()\n",
    "        self.resize = P.ResizeNearestNeighbor(size=(32, 32))\n",
    "        self.norm = BatchNorm2d(num_features=1)\n",
    "        self.conv1 = conv2d(1, 32)\n",
    "        self.relu1 = ReLU()\n",
    "        self.conv2 = conv2d(32,64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool = maxpool()\n",
    "        self.dropout1 = Dropout(p=0.25)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(in_channels=12544, out_channels=128)\n",
    "        self.relu3 = ReLU()\n",
    "        self.dropout2 = Dropout(p=0.5)\n",
    "        self.dense2 = Dense(in_channels=128, out_channels=8)\n",
    "    \n",
    "    def construct(self, input_x):\n",
    "        x = self.reshape(input_x, (self.batch_size,1, 124, 129))\n",
    "        x = self.resize(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建了一个模型对象 `model`，用于训练和优化神经网络模型：首先，通过 `Net(batch_size=64)` 创建了一个网络对象 `net`，其中 `batch_size` 设置为 64。接下来，使用 `Adam` 优化器构建了一个优化器对象 `opt`，指定了学习率、beta 值、权重衰减等参数。使用 `SoftmaxCrossEntropyWithLogits` 创建了一个 softmax 损失函数对象 `loss_fn`，设置了稀疏标签和均值缩减方式。最后，使用这些对象构建了一个模型对象 `model`，用于训练和优化网络模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train import Model\n",
    "from mindspore.nn import Adam\n",
    "from mindspore.nn import SoftmaxCrossEntropyWithLogits\n",
    "\n",
    "# 构建网络\n",
    "net = Net(batch_size=64)\n",
    "# 优化器\n",
    "opt = Adam(net.trainable_params(), learning_rate=0.0008,\n",
    "           beta1=0.9, beta2=0.999, eps=10e-8, weight_decay=0.01)\n",
    "# softmax损失函数\n",
    "loss_fn = SoftmaxCrossEntropyWithLogits(sparse=True,reduction='mean')\n",
    "# 利用训练数据集训练模型\n",
    "model = Model(net, loss_fn, opt)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、模型训练"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用 `model.train(10, ds_train, callbacks=[LossMonitor()])` 进行模型的训练。其中，参数 `10` 表示训练的轮数，`ds_train` 是训练数据集，`callbacks` 用于打印训练过程中的loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Loss: 2.137415885925293\n",
      "Step: 2, Loss: 2.0918924808502197\n",
      "Step: 3, Loss: 2.149198055267334\n",
      "Step: 4, Loss: 2.13702392578125\n",
      "Step: 5, Loss: 2.088939905166626\n",
      "Step: 6, Loss: 2.0989513397216797\n",
      "Step: 7, Loss: 2.0150058269500732\n",
      "Step: 8, Loss: 2.079346179962158\n",
      "Step: 9, Loss: 2.0733487606048584\n",
      "Step: 10, Loss: 2.1888091564178467\n",
      "Step: 11, Loss: 2.107536792755127\n",
      "Step: 12, Loss: 2.0101945400238037\n",
      "Step: 13, Loss: 2.002788782119751\n",
      "Step: 14, Loss: 2.075972318649292\n",
      "Step: 15, Loss: 1.9290294647216797\n",
      "Step: 16, Loss: 2.1038761138916016\n",
      "Step: 17, Loss: 2.080759048461914\n",
      "Step: 18, Loss: 2.11448335647583\n",
      "Step: 19, Loss: 2.0066120624542236\n",
      "Step: 20, Loss: 2.0047030448913574\n",
      "Step: 21, Loss: 1.9980838298797607\n",
      "Step: 22, Loss: 2.0205488204956055\n",
      "Step: 23, Loss: 2.0348238945007324\n",
      "Step: 24, Loss: 2.1152455806732178\n",
      "Step: 25, Loss: 2.0897879600524902\n",
      "Step: 26, Loss: 2.042931318283081\n",
      "Step: 27, Loss: 1.9648971557617188\n",
      "Step: 28, Loss: 2.028815984725952\n",
      "Step: 29, Loss: 1.9885966777801514\n",
      "Step: 30, Loss: 1.8715286254882812\n",
      "Step: 31, Loss: 1.9746549129486084\n",
      "Step: 32, Loss: 1.899946928024292\n",
      "Step: 33, Loss: 2.0092673301696777\n",
      "Step: 34, Loss: 1.9555020332336426\n",
      "Step: 35, Loss: 1.9388830661773682\n",
      "Step: 36, Loss: 1.9914058446884155\n",
      "Step: 37, Loss: 1.9004926681518555\n",
      "Step: 38, Loss: 1.9712352752685547\n",
      "Step: 39, Loss: 1.9607927799224854\n",
      "Step: 40, Loss: 2.017674446105957\n",
      "Step: 41, Loss: 1.9400255680084229\n",
      "Step: 42, Loss: 1.9692755937576294\n",
      "Step: 43, Loss: 1.8395326137542725\n",
      "Step: 44, Loss: 1.9403445720672607\n",
      "Step: 45, Loss: 1.8912708759307861\n",
      "Step: 46, Loss: 1.9539767503738403\n",
      "Step: 47, Loss: 1.8478177785873413\n",
      "Step: 48, Loss: 1.8498798608779907\n",
      "Step: 49, Loss: 1.6630167961120605\n",
      "Step: 50, Loss: 1.8510745763778687\n",
      "Step: 51, Loss: 1.8680646419525146\n",
      "Step: 52, Loss: 1.8931055068969727\n",
      "Step: 53, Loss: 1.965816855430603\n",
      "Step: 54, Loss: 1.8669073581695557\n",
      "Step: 55, Loss: 2.0266804695129395\n",
      "Step: 56, Loss: 1.7432246208190918\n",
      "Step: 57, Loss: 1.6875298023223877\n",
      "Step: 58, Loss: 1.8323142528533936\n",
      "Step: 59, Loss: 1.878130316734314\n",
      "Step: 60, Loss: 1.8483078479766846\n",
      "Step: 61, Loss: 1.8565846681594849\n",
      "Step: 62, Loss: 1.8872735500335693\n",
      "Step: 63, Loss: 1.9227097034454346\n",
      "Step: 64, Loss: 1.9067366123199463\n",
      "Step: 65, Loss: 1.7773168087005615\n",
      "Step: 66, Loss: 1.7942999601364136\n",
      "Step: 67, Loss: 1.7730613946914673\n",
      "Step: 68, Loss: 1.8449513912200928\n",
      "Step: 69, Loss: 1.8908908367156982\n",
      "Step: 70, Loss: 1.933464765548706\n",
      "Step: 71, Loss: 1.6858075857162476\n",
      "Step: 72, Loss: 1.7350566387176514\n",
      "Step: 73, Loss: 1.8557636737823486\n",
      "Step: 74, Loss: 1.852803349494934\n",
      "Step: 75, Loss: 1.8831703662872314\n",
      "Step: 76, Loss: 1.8349688053131104\n",
      "Step: 77, Loss: 1.919872522354126\n",
      "Step: 78, Loss: 1.8618943691253662\n",
      "Step: 79, Loss: 1.7853606939315796\n",
      "Step: 80, Loss: 1.8897910118103027\n",
      "Step: 81, Loss: 1.7322676181793213\n",
      "Step: 82, Loss: 1.775999665260315\n",
      "Step: 83, Loss: 1.7265387773513794\n",
      "Step: 84, Loss: 1.9059181213378906\n",
      "Step: 85, Loss: 1.7295857667922974\n",
      "Step: 86, Loss: 1.7934683561325073\n",
      "Step: 87, Loss: 1.6845850944519043\n",
      "Step: 88, Loss: 1.7641593217849731\n",
      "Step: 89, Loss: 1.692626953125\n",
      "Step: 90, Loss: 1.8295586109161377\n",
      "Step: 91, Loss: 1.7819468975067139\n",
      "Step: 92, Loss: 1.8158667087554932\n",
      "Step: 93, Loss: 1.5753501653671265\n",
      "Step: 94, Loss: 1.7582781314849854\n",
      "Step: 95, Loss: 1.8850091695785522\n",
      "Step: 96, Loss: 1.7404907941818237\n",
      "Step: 97, Loss: 1.7817957401275635\n",
      "Step: 98, Loss: 1.8314979076385498\n",
      "Step: 99, Loss: 1.8190442323684692\n",
      "Step: 100, Loss: 1.686877965927124\n",
      "Step: 101, Loss: 1.9138197898864746\n",
      "Step: 102, Loss: 1.8698310852050781\n",
      "Step: 103, Loss: 1.7363280057907104\n",
      "Step: 104, Loss: 1.6656484603881836\n",
      "Step: 105, Loss: 1.6607147455215454\n",
      "Step: 106, Loss: 1.7274212837219238\n",
      "Step: 107, Loss: 1.6593289375305176\n",
      "Step: 108, Loss: 1.6727135181427002\n",
      "Step: 109, Loss: 1.552285075187683\n",
      "Step: 110, Loss: 1.7607359886169434\n",
      "Step: 111, Loss: 1.7951130867004395\n",
      "Step: 112, Loss: 1.5713109970092773\n",
      "Step: 113, Loss: 1.6212774515151978\n",
      "Step: 114, Loss: 1.5955802202224731\n",
      "Step: 115, Loss: 1.684849500656128\n",
      "Step: 116, Loss: 1.6705650091171265\n",
      "Step: 117, Loss: 1.6405398845672607\n",
      "Step: 118, Loss: 1.6362478733062744\n",
      "Step: 119, Loss: 1.5981800556182861\n",
      "Step: 120, Loss: 1.6845402717590332\n",
      "Step: 121, Loss: 1.846650242805481\n",
      "Step: 122, Loss: 1.6188794374465942\n",
      "Step: 123, Loss: 1.6361303329467773\n",
      "Step: 124, Loss: 1.5471428632736206\n",
      "Step: 125, Loss: 1.6111148595809937\n",
      "Step: 126, Loss: 1.7050508260726929\n",
      "Step: 127, Loss: 1.702384114265442\n",
      "Step: 128, Loss: 1.5218335390090942\n",
      "Step: 129, Loss: 1.9821363687515259\n",
      "Step: 130, Loss: 1.71197509765625\n",
      "Step: 131, Loss: 1.7172940969467163\n",
      "Step: 132, Loss: 1.747570514678955\n",
      "Step: 133, Loss: 1.7302219867706299\n",
      "Step: 134, Loss: 1.5157296657562256\n",
      "Step: 135, Loss: 1.7506194114685059\n",
      "Step: 136, Loss: 1.596374750137329\n",
      "Step: 137, Loss: 1.6024402379989624\n",
      "Step: 138, Loss: 1.621801495552063\n",
      "Step: 139, Loss: 1.565558671951294\n",
      "Step: 140, Loss: 1.6456434726715088\n",
      "Step: 141, Loss: 1.624690294265747\n",
      "Step: 142, Loss: 1.5947540998458862\n",
      "Step: 143, Loss: 1.6201976537704468\n",
      "Step: 144, Loss: 1.613605260848999\n",
      "Step: 145, Loss: 1.4127131700515747\n",
      "Step: 146, Loss: 1.743465781211853\n",
      "Step: 147, Loss: 1.540156364440918\n",
      "Step: 148, Loss: 1.7407557964324951\n",
      "Step: 149, Loss: 1.6000723838806152\n",
      "Step: 150, Loss: 1.582165241241455\n",
      "Step: 151, Loss: 1.7389391660690308\n",
      "Step: 152, Loss: 1.8179106712341309\n",
      "Step: 153, Loss: 1.6938260793685913\n",
      "Step: 154, Loss: 1.546095609664917\n",
      "Step: 155, Loss: 1.4354047775268555\n",
      "Step: 156, Loss: 1.637608528137207\n",
      "Step: 157, Loss: 1.592000961303711\n",
      "Step: 158, Loss: 1.6459118127822876\n",
      "Step: 159, Loss: 1.528458595275879\n",
      "Step: 160, Loss: 1.7093749046325684\n",
      "Step: 161, Loss: 1.6200637817382812\n",
      "Step: 162, Loss: 1.6905148029327393\n",
      "Step: 163, Loss: 1.4450695514678955\n",
      "Step: 164, Loss: 1.4931104183197021\n",
      "Step: 165, Loss: 1.6181113719940186\n",
      "Step: 166, Loss: 1.7915304899215698\n",
      "Step: 167, Loss: 1.543298602104187\n",
      "Step: 168, Loss: 1.4467920064926147\n",
      "Step: 169, Loss: 1.5586845874786377\n",
      "Step: 170, Loss: 1.6049296855926514\n",
      "Step: 171, Loss: 1.6732728481292725\n",
      "Step: 172, Loss: 1.4430809020996094\n",
      "Step: 173, Loss: 1.5878068208694458\n",
      "Step: 174, Loss: 1.5851515531539917\n",
      "Step: 175, Loss: 1.6794630289077759\n",
      "Step: 176, Loss: 1.7406010627746582\n",
      "Step: 177, Loss: 1.6083760261535645\n",
      "Step: 178, Loss: 1.5218653678894043\n",
      "Step: 179, Loss: 1.7014362812042236\n",
      "Step: 180, Loss: 1.396355390548706\n",
      "Step: 181, Loss: 1.4635775089263916\n",
      "Step: 182, Loss: 1.5799682140350342\n",
      "Step: 183, Loss: 1.5494502782821655\n",
      "Step: 184, Loss: 1.6079587936401367\n",
      "Step: 185, Loss: 1.4600019454956055\n",
      "Step: 186, Loss: 1.47318434715271\n",
      "Step: 187, Loss: 1.3513822555541992\n",
      "Step: 188, Loss: 1.6497514247894287\n",
      "Step: 189, Loss: 1.4957202672958374\n",
      "Step: 190, Loss: 1.5446670055389404\n",
      "Step: 191, Loss: 1.5390520095825195\n",
      "Step: 192, Loss: 1.4071767330169678\n",
      "Step: 193, Loss: 1.5788965225219727\n",
      "Step: 194, Loss: 1.4849072694778442\n",
      "Step: 195, Loss: 1.5971829891204834\n",
      "Step: 196, Loss: 1.4192842245101929\n",
      "Step: 197, Loss: 1.61371648311615\n",
      "Step: 198, Loss: 1.6042747497558594\n",
      "Step: 199, Loss: 1.410258173942566\n",
      "Step: 200, Loss: 1.5104570388793945\n",
      "Step: 201, Loss: 1.5888173580169678\n",
      "Step: 202, Loss: 1.3856258392333984\n",
      "Step: 203, Loss: 1.5919536352157593\n",
      "Step: 204, Loss: 1.5289332866668701\n",
      "Step: 205, Loss: 1.2910460233688354\n",
      "Step: 206, Loss: 1.4158756732940674\n",
      "Step: 207, Loss: 1.496272087097168\n",
      "Step: 208, Loss: 1.4864706993103027\n",
      "Step: 209, Loss: 1.2951005697250366\n",
      "Step: 210, Loss: 1.5746850967407227\n",
      "Step: 211, Loss: 1.4353501796722412\n",
      "Step: 212, Loss: 1.3962302207946777\n",
      "Step: 213, Loss: 1.5466036796569824\n",
      "Step: 214, Loss: 1.2772992849349976\n",
      "Step: 215, Loss: 1.5019017457962036\n",
      "Step: 216, Loss: 1.3327503204345703\n",
      "Step: 217, Loss: 1.4154419898986816\n",
      "Step: 218, Loss: 1.5942927598953247\n",
      "Step: 219, Loss: 1.4170805215835571\n",
      "Step: 220, Loss: 1.3931498527526855\n",
      "Step: 221, Loss: 1.4894466400146484\n",
      "Step: 222, Loss: 1.5392704010009766\n",
      "Step: 223, Loss: 1.3531723022460938\n",
      "Step: 224, Loss: 1.3758389949798584\n",
      "Step: 225, Loss: 1.372085690498352\n",
      "Step: 226, Loss: 1.3250819444656372\n",
      "Step: 227, Loss: 1.5968389511108398\n",
      "Step: 228, Loss: 1.5134055614471436\n",
      "Step: 229, Loss: 1.431687355041504\n",
      "Step: 230, Loss: 1.1909351348876953\n",
      "Step: 231, Loss: 1.6860666275024414\n",
      "Step: 232, Loss: 1.287487506866455\n",
      "Step: 233, Loss: 1.3900482654571533\n",
      "Step: 234, Loss: 1.3859870433807373\n",
      "Step: 235, Loss: 1.4100258350372314\n",
      "Step: 236, Loss: 1.530510663986206\n",
      "Step: 237, Loss: 1.5815751552581787\n",
      "Step: 238, Loss: 1.5146217346191406\n",
      "Step: 239, Loss: 1.4164097309112549\n",
      "Step: 240, Loss: 1.6394591331481934\n",
      "Step: 241, Loss: 1.297706127166748\n",
      "Step: 242, Loss: 1.5677908658981323\n",
      "Step: 243, Loss: 1.5777502059936523\n",
      "Step: 244, Loss: 1.375581979751587\n",
      "Step: 245, Loss: 1.5154002904891968\n",
      "Step: 246, Loss: 1.4691753387451172\n",
      "Step: 247, Loss: 1.6160733699798584\n",
      "Step: 248, Loss: 1.4612284898757935\n",
      "Step: 249, Loss: 1.3556498289108276\n",
      "Step: 250, Loss: 1.4648497104644775\n",
      "Step: 251, Loss: 1.4178894758224487\n",
      "Step: 252, Loss: 1.4421395063400269\n",
      "Step: 253, Loss: 1.50874924659729\n",
      "Step: 254, Loss: 1.4773553609848022\n",
      "Step: 255, Loss: 1.6727133989334106\n",
      "Step: 256, Loss: 1.3384859561920166\n",
      "Step: 257, Loss: 1.3764398097991943\n",
      "Step: 258, Loss: 1.3923636674880981\n",
      "Step: 259, Loss: 1.4499495029449463\n",
      "Step: 260, Loss: 1.3986070156097412\n",
      "Step: 261, Loss: 1.2097164392471313\n",
      "Step: 262, Loss: 1.6022653579711914\n",
      "Step: 263, Loss: 1.3119478225708008\n",
      "Step: 264, Loss: 1.3920156955718994\n",
      "Step: 265, Loss: 1.2133336067199707\n",
      "Step: 266, Loss: 1.4053919315338135\n",
      "Step: 267, Loss: 1.2634012699127197\n",
      "Step: 268, Loss: 1.3474421501159668\n",
      "Step: 269, Loss: 1.4138894081115723\n",
      "Step: 270, Loss: 1.2414162158966064\n",
      "Step: 271, Loss: 1.3662399053573608\n",
      "Step: 272, Loss: 1.439122200012207\n",
      "Step: 273, Loss: 1.2641711235046387\n",
      "Step: 274, Loss: 1.481656551361084\n",
      "Step: 275, Loss: 1.4498050212860107\n",
      "Step: 276, Loss: 1.3375647068023682\n",
      "Step: 277, Loss: 1.5245227813720703\n",
      "Step: 278, Loss: 1.3893712759017944\n",
      "Step: 279, Loss: 1.4508748054504395\n",
      "Step: 280, Loss: 1.358659267425537\n",
      "Step: 281, Loss: 1.5445189476013184\n",
      "Step: 282, Loss: 1.659334421157837\n",
      "Step: 283, Loss: 1.5068082809448242\n",
      "Step: 284, Loss: 1.3635855913162231\n",
      "Step: 285, Loss: 1.4788320064544678\n",
      "Step: 286, Loss: 1.4861645698547363\n",
      "Step: 287, Loss: 1.09534752368927\n",
      "Step: 288, Loss: 1.4011962413787842\n",
      "Step: 289, Loss: 1.290789246559143\n",
      "Step: 290, Loss: 1.4008197784423828\n",
      "Step: 291, Loss: 1.2532308101654053\n",
      "Step: 292, Loss: 1.5231049060821533\n",
      "Step: 293, Loss: 1.3808939456939697\n",
      "Step: 294, Loss: 1.3238725662231445\n",
      "Step: 295, Loss: 1.1864674091339111\n",
      "Step: 296, Loss: 1.4877231121063232\n",
      "Step: 297, Loss: 1.4026367664337158\n",
      "Step: 298, Loss: 1.4450783729553223\n",
      "Step: 299, Loss: 1.394707441329956\n",
      "Step: 300, Loss: 1.3812931776046753\n",
      "Step: 301, Loss: 1.3459091186523438\n",
      "Step: 302, Loss: 1.1888256072998047\n",
      "Step: 303, Loss: 1.4413678646087646\n",
      "Step: 304, Loss: 1.2954449653625488\n",
      "Step: 305, Loss: 1.3762331008911133\n",
      "Step: 306, Loss: 1.1103068590164185\n",
      "Step: 307, Loss: 1.2679274082183838\n",
      "Step: 308, Loss: 1.3495848178863525\n",
      "Step: 309, Loss: 1.3189940452575684\n",
      "Step: 310, Loss: 1.3866908550262451\n",
      "Step: 311, Loss: 1.2522916793823242\n",
      "Step: 312, Loss: 1.3795217275619507\n",
      "Step: 313, Loss: 1.3179926872253418\n",
      "Step: 314, Loss: 1.3034353256225586\n",
      "Step: 315, Loss: 1.2836558818817139\n",
      "Step: 316, Loss: 1.1834776401519775\n",
      "Step: 317, Loss: 1.1262938976287842\n",
      "Step: 318, Loss: 1.3234844207763672\n",
      "Step: 319, Loss: 1.2527782917022705\n",
      "Step: 320, Loss: 1.2689757347106934\n",
      "Step: 321, Loss: 1.301910638809204\n",
      "Step: 322, Loss: 1.3212425708770752\n",
      "Step: 323, Loss: 1.3218069076538086\n",
      "Step: 324, Loss: 1.4241383075714111\n",
      "Step: 325, Loss: 1.1800713539123535\n",
      "Step: 326, Loss: 1.1592381000518799\n",
      "Step: 327, Loss: 1.2401195764541626\n",
      "Step: 328, Loss: 1.3046879768371582\n",
      "Step: 329, Loss: 1.2211098670959473\n",
      "Step: 330, Loss: 1.2302683591842651\n",
      "Step: 331, Loss: 1.3065768480300903\n",
      "Step: 332, Loss: 1.2641851902008057\n",
      "Step: 333, Loss: 1.4711568355560303\n",
      "Step: 334, Loss: 1.441156268119812\n",
      "Step: 335, Loss: 1.3470057249069214\n",
      "Step: 336, Loss: 1.2314865589141846\n",
      "Step: 337, Loss: 1.4123544692993164\n",
      "Step: 338, Loss: 1.343186616897583\n",
      "Step: 339, Loss: 1.5072548389434814\n",
      "Step: 340, Loss: 1.5806031227111816\n",
      "Step: 341, Loss: 1.525865077972412\n",
      "Step: 342, Loss: 1.3905457258224487\n",
      "Step: 343, Loss: 1.4737697839736938\n",
      "Step: 344, Loss: 1.2731306552886963\n",
      "Step: 345, Loss: 1.3714098930358887\n",
      "Step: 346, Loss: 1.5183525085449219\n",
      "Step: 347, Loss: 1.191950798034668\n",
      "Step: 348, Loss: 1.493051528930664\n",
      "Step: 349, Loss: 1.3071391582489014\n",
      "Step: 350, Loss: 1.5246968269348145\n",
      "Step: 351, Loss: 1.3865516185760498\n",
      "Step: 352, Loss: 1.3304040431976318\n",
      "Step: 353, Loss: 1.0802123546600342\n",
      "Step: 354, Loss: 1.311615228652954\n",
      "Step: 355, Loss: 1.354325532913208\n",
      "Step: 356, Loss: 1.226311206817627\n",
      "Step: 357, Loss: 1.217889666557312\n",
      "Step: 358, Loss: 1.3900468349456787\n",
      "Step: 359, Loss: 1.2301479578018188\n",
      "Step: 360, Loss: 1.196494460105896\n",
      "Step: 361, Loss: 1.2135038375854492\n",
      "Step: 362, Loss: 1.1919426918029785\n",
      "Step: 363, Loss: 1.1611146926879883\n",
      "Step: 364, Loss: 1.3361183404922485\n",
      "Step: 365, Loss: 1.3974624872207642\n",
      "Step: 366, Loss: 1.1645164489746094\n",
      "Step: 367, Loss: 1.5348553657531738\n",
      "Step: 368, Loss: 1.3391427993774414\n",
      "Step: 369, Loss: 1.1665599346160889\n",
      "Step: 370, Loss: 1.1842724084854126\n",
      "Step: 371, Loss: 1.2980778217315674\n",
      "Step: 372, Loss: 1.5190181732177734\n",
      "Step: 373, Loss: 1.16090726852417\n",
      "Step: 374, Loss: 1.3256018161773682\n",
      "Step: 375, Loss: 1.3652472496032715\n",
      "Step: 376, Loss: 1.3533602952957153\n",
      "Step: 377, Loss: 1.0241796970367432\n",
      "Step: 378, Loss: 1.4013135433197021\n",
      "Step: 379, Loss: 1.4137771129608154\n",
      "Step: 380, Loss: 1.4289355278015137\n",
      "Step: 381, Loss: 1.4348857402801514\n",
      "Step: 382, Loss: 1.3751473426818848\n",
      "Step: 383, Loss: 1.200571894645691\n",
      "Step: 384, Loss: 1.2091820240020752\n",
      "Step: 385, Loss: 1.185070276260376\n",
      "Step: 386, Loss: 1.4305531978607178\n",
      "Step: 387, Loss: 1.3611373901367188\n",
      "Step: 388, Loss: 1.2678179740905762\n",
      "Step: 389, Loss: 1.3140664100646973\n",
      "Step: 390, Loss: 1.3776332139968872\n",
      "Step: 391, Loss: 1.2990972995758057\n",
      "Step: 392, Loss: 1.2614750862121582\n",
      "Step: 393, Loss: 1.2351785898208618\n",
      "Step: 394, Loss: 1.3441094160079956\n",
      "Step: 395, Loss: 1.2667443752288818\n",
      "Step: 396, Loss: 1.429046630859375\n",
      "Step: 397, Loss: 1.2974258661270142\n",
      "Step: 398, Loss: 1.2247929573059082\n",
      "Step: 399, Loss: 1.3126630783081055\n",
      "Step: 400, Loss: 1.2897672653198242\n",
      "Step: 401, Loss: 1.2112778425216675\n",
      "Step: 402, Loss: 1.1591593027114868\n",
      "Step: 403, Loss: 1.270195484161377\n",
      "Step: 404, Loss: 1.208846092224121\n",
      "Step: 405, Loss: 1.2130529880523682\n",
      "Step: 406, Loss: 1.2739498615264893\n",
      "Step: 407, Loss: 1.2159358263015747\n",
      "Step: 408, Loss: 1.1698813438415527\n",
      "Step: 409, Loss: 1.406631588935852\n",
      "Step: 410, Loss: 1.2141071557998657\n",
      "Step: 411, Loss: 1.2105457782745361\n",
      "Step: 412, Loss: 1.136922836303711\n",
      "Step: 413, Loss: 1.4477603435516357\n",
      "Step: 414, Loss: 1.1169438362121582\n",
      "Step: 415, Loss: 1.2492494583129883\n",
      "Step: 416, Loss: 1.2152044773101807\n",
      "Step: 417, Loss: 1.3582353591918945\n",
      "Step: 418, Loss: 1.3957085609436035\n",
      "Step: 419, Loss: 1.1021384000778198\n",
      "Step: 420, Loss: 1.230320692062378\n",
      "Step: 421, Loss: 1.2574248313903809\n",
      "Step: 422, Loss: 1.2771716117858887\n",
      "Step: 423, Loss: 1.2661709785461426\n",
      "Step: 424, Loss: 1.417292833328247\n",
      "Step: 425, Loss: 1.3477903604507446\n",
      "Step: 426, Loss: 1.3581725358963013\n",
      "Step: 427, Loss: 1.2128911018371582\n",
      "Step: 428, Loss: 1.131460189819336\n",
      "Step: 429, Loss: 1.2097688913345337\n",
      "Step: 430, Loss: 1.0713037252426147\n",
      "Step: 431, Loss: 1.3086347579956055\n",
      "Step: 432, Loss: 1.1748173236846924\n",
      "Step: 433, Loss: 1.2007986307144165\n",
      "Step: 434, Loss: 1.2881532907485962\n",
      "Step: 435, Loss: 1.1991214752197266\n",
      "Step: 436, Loss: 1.2290233373641968\n",
      "Step: 437, Loss: 1.4146867990493774\n",
      "Step: 438, Loss: 1.2725930213928223\n",
      "Step: 439, Loss: 1.301417350769043\n",
      "Step: 440, Loss: 1.3379393815994263\n",
      "Step: 441, Loss: 1.157065749168396\n",
      "Step: 442, Loss: 1.1520684957504272\n",
      "Step: 443, Loss: 1.1660617589950562\n",
      "Step: 444, Loss: 1.3048640489578247\n",
      "Step: 445, Loss: 1.2762917280197144\n",
      "Step: 446, Loss: 1.136649250984192\n",
      "Step: 447, Loss: 1.3145334720611572\n",
      "Step: 448, Loss: 1.2272522449493408\n",
      "Step: 449, Loss: 1.122894525527954\n",
      "Step: 450, Loss: 1.2097034454345703\n",
      "Step: 451, Loss: 1.2501314878463745\n",
      "Step: 452, Loss: 1.3262298107147217\n",
      "Step: 453, Loss: 1.3901416063308716\n",
      "Step: 454, Loss: 1.27783203125\n",
      "Step: 455, Loss: 1.315000295639038\n",
      "Step: 456, Loss: 1.2713747024536133\n",
      "Step: 457, Loss: 1.2994624376296997\n",
      "Step: 458, Loss: 1.2144798040390015\n",
      "Step: 459, Loss: 1.127482533454895\n",
      "Step: 460, Loss: 1.3458781242370605\n",
      "Step: 461, Loss: 1.1989253759384155\n",
      "Step: 462, Loss: 1.2588707208633423\n",
      "Step: 463, Loss: 1.2049554586410522\n",
      "Step: 464, Loss: 1.0503928661346436\n",
      "Step: 465, Loss: 1.357954502105713\n",
      "Step: 466, Loss: 1.377202033996582\n",
      "Step: 467, Loss: 1.2093031406402588\n",
      "Step: 468, Loss: 1.2271997928619385\n",
      "Step: 469, Loss: 1.0663270950317383\n",
      "Step: 470, Loss: 1.0696909427642822\n",
      "Step: 471, Loss: 1.078392505645752\n",
      "Step: 472, Loss: 1.3107280731201172\n",
      "Step: 473, Loss: 1.0879004001617432\n",
      "Step: 474, Loss: 1.3173681497573853\n",
      "Step: 475, Loss: 1.3841010332107544\n",
      "Step: 476, Loss: 1.2225310802459717\n",
      "Step: 477, Loss: 1.3547635078430176\n",
      "Step: 478, Loss: 1.2488489151000977\n",
      "Step: 479, Loss: 1.167755365371704\n",
      "Step: 480, Loss: 1.374159812927246\n",
      "Step: 481, Loss: 1.3729336261749268\n",
      "Step: 482, Loss: 1.3450106382369995\n",
      "Step: 483, Loss: 1.133754849433899\n",
      "Step: 484, Loss: 1.5118826627731323\n",
      "Step: 485, Loss: 1.096207857131958\n",
      "Step: 486, Loss: 1.159412145614624\n",
      "Step: 487, Loss: 1.0872972011566162\n",
      "Step: 488, Loss: 1.226396083831787\n",
      "Step: 489, Loss: 1.228485345840454\n",
      "Step: 490, Loss: 1.345134973526001\n",
      "Step: 491, Loss: 1.1902225017547607\n",
      "Step: 492, Loss: 1.1439039707183838\n",
      "Step: 493, Loss: 1.0164875984191895\n",
      "Step: 494, Loss: 1.092221736907959\n",
      "Step: 495, Loss: 1.0906226634979248\n",
      "Step: 496, Loss: 0.9422469139099121\n",
      "Step: 497, Loss: 1.1321101188659668\n",
      "Step: 498, Loss: 1.2815308570861816\n",
      "Step: 499, Loss: 1.192511796951294\n",
      "Step: 500, Loss: 1.2774474620819092\n",
      "Step: 501, Loss: 1.1861611604690552\n",
      "Step: 502, Loss: 1.279348611831665\n",
      "Step: 503, Loss: 1.1984798908233643\n",
      "Step: 504, Loss: 1.1323753595352173\n",
      "Step: 505, Loss: 1.060187816619873\n",
      "Step: 506, Loss: 1.0530850887298584\n",
      "Step: 507, Loss: 0.972525417804718\n",
      "Step: 508, Loss: 1.0501869916915894\n",
      "Step: 509, Loss: 1.0667446851730347\n",
      "Step: 510, Loss: 0.9796376824378967\n",
      "Step: 511, Loss: 1.0218406915664673\n",
      "Step: 512, Loss: 0.9935392141342163\n",
      "Step: 513, Loss: 1.249711275100708\n",
      "Step: 514, Loss: 1.3724567890167236\n",
      "Step: 515, Loss: 1.1729801893234253\n",
      "Step: 516, Loss: 0.8820114135742188\n",
      "Step: 517, Loss: 1.2368505001068115\n",
      "Step: 518, Loss: 1.1418728828430176\n",
      "Step: 519, Loss: 1.2517824172973633\n",
      "Step: 520, Loss: 1.146056890487671\n",
      "Step: 521, Loss: 1.2778006792068481\n",
      "Step: 522, Loss: 1.1925947666168213\n",
      "Step: 523, Loss: 1.1997289657592773\n",
      "Step: 524, Loss: 1.2298393249511719\n",
      "Step: 525, Loss: 1.1276686191558838\n",
      "Step: 526, Loss: 1.4293498992919922\n",
      "Step: 527, Loss: 1.4472641944885254\n",
      "Step: 528, Loss: 1.1979780197143555\n",
      "Step: 529, Loss: 1.3184747695922852\n",
      "Step: 530, Loss: 1.0819278955459595\n",
      "Step: 531, Loss: 1.0595190525054932\n",
      "Step: 532, Loss: 1.2089178562164307\n",
      "Step: 533, Loss: 1.0148859024047852\n",
      "Step: 534, Loss: 1.2377119064331055\n",
      "Step: 535, Loss: 1.0737385749816895\n",
      "Step: 536, Loss: 1.2051596641540527\n",
      "Step: 537, Loss: 1.1845107078552246\n",
      "Step: 538, Loss: 0.8482340574264526\n",
      "Step: 539, Loss: 1.2281047105789185\n",
      "Step: 540, Loss: 1.0718555450439453\n",
      "Step: 541, Loss: 1.0933490991592407\n",
      "Step: 542, Loss: 1.2988226413726807\n",
      "Step: 543, Loss: 1.147492527961731\n",
      "Step: 544, Loss: 1.1209924221038818\n",
      "Step: 545, Loss: 0.922240138053894\n",
      "Step: 546, Loss: 1.2876989841461182\n",
      "Step: 547, Loss: 1.00373375415802\n",
      "Step: 548, Loss: 0.9477332234382629\n",
      "Step: 549, Loss: 1.2828567028045654\n",
      "Step: 550, Loss: 1.0923914909362793\n",
      "Step: 551, Loss: 1.1196731328964233\n",
      "Step: 552, Loss: 1.107729196548462\n",
      "Step: 553, Loss: 1.0604963302612305\n",
      "Step: 554, Loss: 1.243080973625183\n",
      "Step: 555, Loss: 1.2187182903289795\n",
      "Step: 556, Loss: 1.1459500789642334\n",
      "Step: 557, Loss: 1.1363314390182495\n",
      "Step: 558, Loss: 1.0069081783294678\n",
      "Step: 559, Loss: 0.9284936189651489\n",
      "Step: 560, Loss: 1.2926881313323975\n",
      "Step: 561, Loss: 1.1165200471878052\n",
      "Step: 562, Loss: 1.1130774021148682\n",
      "Step: 563, Loss: 1.0806688070297241\n",
      "Step: 564, Loss: 1.1755515336990356\n",
      "Step: 565, Loss: 0.9084999561309814\n",
      "Step: 566, Loss: 1.1299359798431396\n",
      "Step: 567, Loss: 1.2234177589416504\n",
      "Step: 568, Loss: 1.0721330642700195\n",
      "Step: 569, Loss: 1.2074617147445679\n",
      "Step: 570, Loss: 1.1407697200775146\n",
      "Step: 571, Loss: 1.0751996040344238\n",
      "Step: 572, Loss: 1.3056187629699707\n",
      "Step: 573, Loss: 1.3891396522521973\n",
      "Step: 574, Loss: 0.9746541380882263\n",
      "Step: 575, Loss: 1.1117541790008545\n",
      "Step: 576, Loss: 0.9814951419830322\n",
      "Step: 577, Loss: 1.1154546737670898\n",
      "Step: 578, Loss: 1.1109695434570312\n",
      "Step: 579, Loss: 1.1839547157287598\n",
      "Step: 580, Loss: 1.2834792137145996\n",
      "Step: 581, Loss: 1.4338538646697998\n",
      "Step: 582, Loss: 1.2179789543151855\n",
      "Step: 583, Loss: 1.3225997686386108\n",
      "Step: 584, Loss: 1.1109893321990967\n",
      "Step: 585, Loss: 1.1674829721450806\n",
      "Step: 586, Loss: 1.2141233682632446\n",
      "Step: 587, Loss: 1.1799840927124023\n",
      "Step: 588, Loss: 1.3980762958526611\n",
      "Step: 589, Loss: 1.2920331954956055\n",
      "Step: 590, Loss: 1.218703269958496\n",
      "Step: 591, Loss: 1.1935069561004639\n",
      "Step: 592, Loss: 1.0017578601837158\n",
      "Step: 593, Loss: 1.1120169162750244\n",
      "Step: 594, Loss: 1.3317557573318481\n",
      "Step: 595, Loss: 1.309676170349121\n",
      "Step: 596, Loss: 1.123213768005371\n",
      "Step: 597, Loss: 1.2661696672439575\n",
      "Step: 598, Loss: 1.1662194728851318\n",
      "Step: 599, Loss: 1.2251828908920288\n",
      "Step: 600, Loss: 1.2983464002609253\n",
      "Step: 601, Loss: 1.1544520854949951\n",
      "Step: 602, Loss: 1.2732129096984863\n",
      "Step: 603, Loss: 0.973722517490387\n",
      "Step: 604, Loss: 1.1579277515411377\n",
      "Step: 605, Loss: 0.9194639921188354\n",
      "Step: 606, Loss: 1.3485153913497925\n",
      "Step: 607, Loss: 1.0654629468917847\n",
      "Step: 608, Loss: 0.95894455909729\n",
      "Step: 609, Loss: 1.0784099102020264\n",
      "Step: 610, Loss: 1.2140793800354004\n",
      "Step: 611, Loss: 1.0552972555160522\n",
      "Step: 612, Loss: 0.9593204855918884\n",
      "Step: 613, Loss: 1.3090184926986694\n",
      "Step: 614, Loss: 1.0588172674179077\n",
      "Step: 615, Loss: 0.9697906970977783\n",
      "Step: 616, Loss: 1.1755046844482422\n",
      "Step: 617, Loss: 0.9767153859138489\n",
      "Step: 618, Loss: 1.1973822116851807\n",
      "Step: 619, Loss: 0.9603471755981445\n",
      "Step: 620, Loss: 1.1924411058425903\n",
      "Step: 621, Loss: 0.9927869439125061\n",
      "Step: 622, Loss: 1.2733328342437744\n",
      "Step: 623, Loss: 0.9753877520561218\n",
      "Step: 624, Loss: 0.9943920373916626\n",
      "Step: 625, Loss: 1.1593399047851562\n",
      "Step: 626, Loss: 1.34051513671875\n",
      "Step: 627, Loss: 0.862116277217865\n",
      "Step: 628, Loss: 0.9792475700378418\n",
      "Step: 629, Loss: 0.9733709096908569\n",
      "Step: 630, Loss: 1.0550545454025269\n",
      "Step: 631, Loss: 1.1754300594329834\n",
      "Step: 632, Loss: 1.0925817489624023\n",
      "Step: 633, Loss: 1.0395989418029785\n",
      "Step: 634, Loss: 1.2390912771224976\n",
      "Step: 635, Loss: 1.0694093704223633\n",
      "Step: 636, Loss: 1.1963683366775513\n",
      "Step: 637, Loss: 0.987610399723053\n",
      "Step: 638, Loss: 1.2705581188201904\n",
      "Step: 639, Loss: 1.000336766242981\n",
      "Step: 640, Loss: 1.3437527418136597\n",
      "Step: 641, Loss: 1.0517027378082275\n",
      "Step: 642, Loss: 1.0008546113967896\n",
      "Step: 643, Loss: 1.0870492458343506\n",
      "Step: 644, Loss: 0.9870419502258301\n",
      "Step: 645, Loss: 1.1994973421096802\n",
      "Step: 646, Loss: 1.2900313138961792\n",
      "Step: 647, Loss: 1.3273756504058838\n",
      "Step: 648, Loss: 0.9857096672058105\n",
      "Step: 649, Loss: 1.325610876083374\n",
      "Step: 650, Loss: 1.0450998544692993\n",
      "Step: 651, Loss: 1.2136101722717285\n",
      "Step: 652, Loss: 1.238379955291748\n",
      "Step: 653, Loss: 0.9463703036308289\n",
      "Step: 654, Loss: 1.111449956893921\n",
      "Step: 655, Loss: 0.7892329096794128\n",
      "Step: 656, Loss: 1.2901197671890259\n",
      "Step: 657, Loss: 1.1107351779937744\n",
      "Step: 658, Loss: 1.319258451461792\n",
      "Step: 659, Loss: 1.0963389873504639\n",
      "Step: 660, Loss: 1.0406358242034912\n",
      "Step: 661, Loss: 1.1206727027893066\n",
      "Step: 662, Loss: 1.1373250484466553\n",
      "Step: 663, Loss: 1.2406928539276123\n",
      "Step: 664, Loss: 1.1181617975234985\n",
      "Step: 665, Loss: 1.0800886154174805\n",
      "Step: 666, Loss: 0.9461723566055298\n",
      "Step: 667, Loss: 1.0084424018859863\n",
      "Step: 668, Loss: 1.1363844871520996\n",
      "Step: 669, Loss: 0.9174594879150391\n",
      "Step: 670, Loss: 0.8678732514381409\n",
      "Step: 671, Loss: 0.9651785492897034\n",
      "Step: 672, Loss: 0.816445529460907\n",
      "Step: 673, Loss: 1.0058159828186035\n",
      "Step: 674, Loss: 1.2089629173278809\n",
      "Step: 675, Loss: 1.1406950950622559\n",
      "Step: 676, Loss: 1.0268293619155884\n",
      "Step: 677, Loss: 0.8445900082588196\n",
      "Step: 678, Loss: 0.8405451774597168\n",
      "Step: 679, Loss: 1.2453097105026245\n",
      "Step: 680, Loss: 1.0964581966400146\n",
      "Step: 681, Loss: 0.8904348015785217\n",
      "Step: 682, Loss: 1.2346241474151611\n",
      "Step: 683, Loss: 1.2487905025482178\n",
      "Step: 684, Loss: 1.1070400476455688\n",
      "Step: 685, Loss: 0.9854158163070679\n",
      "Step: 686, Loss: 1.2161810398101807\n",
      "Step: 687, Loss: 1.0937037467956543\n",
      "Step: 688, Loss: 1.1049556732177734\n",
      "Step: 689, Loss: 1.118736982345581\n",
      "Step: 690, Loss: 1.0650001764297485\n",
      "Step: 691, Loss: 1.0982211828231812\n",
      "Step: 692, Loss: 1.2475295066833496\n",
      "Step: 693, Loss: 1.1199796199798584\n",
      "Step: 694, Loss: 1.0875134468078613\n",
      "Step: 695, Loss: 1.2327940464019775\n",
      "Step: 696, Loss: 1.0383797883987427\n",
      "Step: 697, Loss: 0.9177960753440857\n",
      "Step: 698, Loss: 1.1135081052780151\n",
      "Step: 699, Loss: 1.2763338088989258\n",
      "Step: 700, Loss: 1.1451603174209595\n",
      "Step: 701, Loss: 0.963779091835022\n",
      "Step: 702, Loss: 1.078662633895874\n",
      "Step: 703, Loss: 1.020758867263794\n",
      "Step: 704, Loss: 1.065804123878479\n",
      "Step: 705, Loss: 1.0617061853408813\n",
      "Step: 706, Loss: 1.0638154745101929\n",
      "Step: 707, Loss: 0.9660239219665527\n",
      "Step: 708, Loss: 1.2394442558288574\n",
      "Step: 709, Loss: 0.9681378602981567\n",
      "Step: 710, Loss: 0.9273990988731384\n",
      "Step: 711, Loss: 0.9185075759887695\n",
      "Step: 712, Loss: 1.024755597114563\n",
      "Step: 713, Loss: 0.9904682636260986\n",
      "Step: 714, Loss: 0.8353233933448792\n",
      "Step: 715, Loss: 1.083651065826416\n",
      "Step: 716, Loss: 1.2446268796920776\n",
      "Step: 717, Loss: 1.0917956829071045\n",
      "Step: 718, Loss: 1.0155619382858276\n",
      "Step: 719, Loss: 1.0279884338378906\n",
      "Step: 720, Loss: 1.0618717670440674\n",
      "Step: 721, Loss: 1.2350643873214722\n",
      "Step: 722, Loss: 0.874213457107544\n",
      "Step: 723, Loss: 1.1672358512878418\n",
      "Step: 724, Loss: 1.0533888339996338\n",
      "Step: 725, Loss: 1.061829686164856\n",
      "Step: 726, Loss: 1.168456792831421\n",
      "Step: 727, Loss: 1.135115146636963\n",
      "Step: 728, Loss: 1.0066795349121094\n",
      "Step: 729, Loss: 1.20241379737854\n",
      "Step: 730, Loss: 0.9734771251678467\n",
      "Step: 731, Loss: 0.994653582572937\n",
      "Step: 732, Loss: 1.0947993993759155\n",
      "Step: 733, Loss: 1.2418104410171509\n",
      "Step: 734, Loss: 0.7958207130432129\n",
      "Step: 735, Loss: 0.9222044944763184\n",
      "Step: 736, Loss: 1.5841341018676758\n",
      "Step: 737, Loss: 0.9774179458618164\n",
      "Step: 738, Loss: 0.9277509450912476\n",
      "Step: 739, Loss: 1.1216444969177246\n",
      "Step: 740, Loss: 1.0152606964111328\n",
      "Step: 741, Loss: 1.024261474609375\n",
      "Step: 742, Loss: 0.8642994165420532\n",
      "Step: 743, Loss: 1.0257575511932373\n",
      "Step: 744, Loss: 0.9997650384902954\n",
      "Step: 745, Loss: 0.9780881404876709\n",
      "Step: 746, Loss: 1.1734185218811035\n",
      "Step: 747, Loss: 1.3159770965576172\n",
      "Step: 748, Loss: 1.0666115283966064\n",
      "Step: 749, Loss: 1.0180275440216064\n",
      "Step: 750, Loss: 1.2127493619918823\n",
      "Step: 751, Loss: 0.9701389670372009\n",
      "Step: 752, Loss: 1.2309496402740479\n",
      "Step: 753, Loss: 1.1207648515701294\n",
      "Step: 754, Loss: 1.0159271955490112\n",
      "Step: 755, Loss: 1.0744246244430542\n",
      "Step: 756, Loss: 1.1140594482421875\n",
      "Step: 757, Loss: 0.9304640293121338\n",
      "Step: 758, Loss: 1.0258893966674805\n",
      "Step: 759, Loss: 1.0062931776046753\n",
      "Step: 760, Loss: 1.3598006963729858\n",
      "Step: 761, Loss: 0.8827689290046692\n",
      "Step: 762, Loss: 0.962456226348877\n",
      "Step: 763, Loss: 0.9475224018096924\n",
      "Step: 764, Loss: 1.041459321975708\n",
      "Step: 765, Loss: 1.2142595052719116\n",
      "Step: 766, Loss: 1.0853053331375122\n",
      "Step: 767, Loss: 0.8869519233703613\n",
      "Step: 768, Loss: 1.197011947631836\n",
      "Step: 769, Loss: 0.8866106271743774\n",
      "Step: 770, Loss: 1.2249435186386108\n",
      "Step: 771, Loss: 1.0237486362457275\n",
      "Step: 772, Loss: 1.1081258058547974\n",
      "Step: 773, Loss: 1.0644073486328125\n",
      "Step: 774, Loss: 1.1217161417007446\n",
      "Step: 775, Loss: 1.1252225637435913\n",
      "Step: 776, Loss: 1.0774896144866943\n",
      "Step: 777, Loss: 1.082236647605896\n",
      "Step: 778, Loss: 1.2590947151184082\n",
      "Step: 779, Loss: 0.940199613571167\n",
      "Step: 780, Loss: 1.1104176044464111\n",
      "Step: 781, Loss: 0.8701392412185669\n",
      "Step: 782, Loss: 0.9038294553756714\n",
      "Step: 783, Loss: 1.0998847484588623\n",
      "Step: 784, Loss: 1.102103352546692\n",
      "Step: 785, Loss: 1.0401802062988281\n",
      "Step: 786, Loss: 0.8668404221534729\n",
      "Step: 787, Loss: 1.278123378753662\n",
      "Step: 788, Loss: 0.9860852956771851\n",
      "Step: 789, Loss: 1.0623507499694824\n",
      "Step: 790, Loss: 1.0408579111099243\n",
      "Step: 791, Loss: 1.1063075065612793\n",
      "Step: 792, Loss: 1.0796376466751099\n",
      "Step: 793, Loss: 1.1545722484588623\n",
      "Step: 794, Loss: 1.0222779512405396\n",
      "Step: 795, Loss: 1.0023350715637207\n",
      "Step: 796, Loss: 0.9813966155052185\n",
      "Step: 797, Loss: 1.1264734268188477\n",
      "Step: 798, Loss: 1.0094892978668213\n",
      "Step: 799, Loss: 0.9507616758346558\n",
      "Step: 800, Loss: 1.144953727722168\n",
      "Step: 801, Loss: 0.8628479242324829\n",
      "Step: 802, Loss: 1.0889933109283447\n",
      "Step: 803, Loss: 1.2906677722930908\n",
      "Step: 804, Loss: 1.1182997226715088\n",
      "Step: 805, Loss: 0.9834384918212891\n",
      "Step: 806, Loss: 0.9667088985443115\n",
      "Step: 807, Loss: 0.9160944819450378\n",
      "Step: 808, Loss: 1.1307251453399658\n",
      "Step: 809, Loss: 1.0143446922302246\n",
      "Step: 810, Loss: 1.0151915550231934\n",
      "Step: 811, Loss: 1.0171937942504883\n",
      "Step: 812, Loss: 0.8063927888870239\n",
      "Step: 813, Loss: 0.9669824838638306\n",
      "Step: 814, Loss: 0.9987306594848633\n",
      "Step: 815, Loss: 0.9930061101913452\n",
      "Step: 816, Loss: 1.274684190750122\n",
      "Step: 817, Loss: 0.8686628341674805\n",
      "Step: 818, Loss: 1.0010195970535278\n",
      "Step: 819, Loss: 0.7330455780029297\n",
      "Step: 820, Loss: 1.1293153762817383\n",
      "Step: 821, Loss: 0.9160616993904114\n",
      "Step: 822, Loss: 1.2360756397247314\n",
      "Step: 823, Loss: 0.8409843444824219\n",
      "Step: 824, Loss: 1.023934006690979\n",
      "Step: 825, Loss: 1.0449607372283936\n",
      "Step: 826, Loss: 1.1252517700195312\n",
      "Step: 827, Loss: 1.0970524549484253\n",
      "Step: 828, Loss: 0.8609979152679443\n",
      "Step: 829, Loss: 0.9558703303337097\n",
      "Step: 830, Loss: 0.9917991161346436\n",
      "Step: 831, Loss: 1.1115041971206665\n",
      "Step: 832, Loss: 1.2239258289337158\n",
      "Step: 833, Loss: 0.9696216583251953\n",
      "Step: 834, Loss: 0.811886191368103\n",
      "Step: 835, Loss: 1.0449985265731812\n",
      "Step: 836, Loss: 0.9298102259635925\n",
      "Step: 837, Loss: 1.028343915939331\n",
      "Step: 838, Loss: 0.9326732754707336\n",
      "Step: 839, Loss: 0.93044114112854\n",
      "Step: 840, Loss: 1.094106674194336\n",
      "Step: 841, Loss: 0.9881771802902222\n",
      "Step: 842, Loss: 0.8413534760475159\n",
      "Step: 843, Loss: 1.102092981338501\n",
      "Step: 844, Loss: 1.084536075592041\n",
      "Step: 845, Loss: 0.8541774749755859\n",
      "Step: 846, Loss: 1.051818609237671\n",
      "Step: 847, Loss: 0.82024085521698\n",
      "Step: 848, Loss: 0.9655234813690186\n",
      "Step: 849, Loss: 1.1488375663757324\n",
      "Step: 850, Loss: 1.040682077407837\n",
      "Step: 851, Loss: 1.089076280593872\n",
      "Step: 852, Loss: 1.016613245010376\n",
      "Step: 853, Loss: 1.2323799133300781\n",
      "Step: 854, Loss: 0.9157097339630127\n",
      "Step: 855, Loss: 1.258545160293579\n",
      "Step: 856, Loss: 1.2717196941375732\n",
      "Step: 857, Loss: 1.1161609888076782\n",
      "Step: 858, Loss: 0.9863730669021606\n",
      "Step: 859, Loss: 1.0187240839004517\n",
      "Step: 860, Loss: 0.9898315668106079\n",
      "Step: 861, Loss: 0.9134799838066101\n",
      "Step: 862, Loss: 1.398697018623352\n",
      "Step: 863, Loss: 1.0076074600219727\n",
      "Step: 864, Loss: 0.8240557909011841\n",
      "Step: 865, Loss: 1.1604191064834595\n",
      "Step: 866, Loss: 1.2367539405822754\n",
      "Step: 867, Loss: 1.0240834951400757\n",
      "Step: 868, Loss: 1.0637757778167725\n",
      "Step: 869, Loss: 0.952430248260498\n",
      "Step: 870, Loss: 0.8114128708839417\n",
      "Step: 871, Loss: 1.2300527095794678\n",
      "Step: 872, Loss: 0.9698813557624817\n",
      "Step: 873, Loss: 0.9904136061668396\n",
      "Step: 874, Loss: 0.8851466178894043\n",
      "Step: 875, Loss: 0.8526291251182556\n",
      "Step: 876, Loss: 1.0386896133422852\n",
      "Step: 877, Loss: 1.1828994750976562\n",
      "Step: 878, Loss: 0.9114999175071716\n",
      "Step: 879, Loss: 1.1241397857666016\n",
      "Step: 880, Loss: 1.191752552986145\n",
      "Step: 881, Loss: 1.0237364768981934\n",
      "Step: 882, Loss: 1.152052640914917\n",
      "Step: 883, Loss: 0.9827665686607361\n",
      "Step: 884, Loss: 1.0143022537231445\n",
      "Step: 885, Loss: 0.836807370185852\n",
      "Step: 886, Loss: 1.0552304983139038\n",
      "Step: 887, Loss: 0.8753451108932495\n",
      "Step: 888, Loss: 1.172680139541626\n",
      "Step: 889, Loss: 1.1549915075302124\n",
      "Step: 890, Loss: 1.2218377590179443\n",
      "Step: 891, Loss: 0.928378701210022\n",
      "Step: 892, Loss: 0.9072038531303406\n",
      "Step: 893, Loss: 0.8805874586105347\n",
      "Step: 894, Loss: 1.109027624130249\n",
      "Step: 895, Loss: 0.9464676380157471\n",
      "Step: 896, Loss: 1.0357165336608887\n",
      "Step: 897, Loss: 0.9217427372932434\n",
      "Step: 898, Loss: 1.1464167833328247\n",
      "Step: 899, Loss: 0.9712276458740234\n",
      "Step: 900, Loss: 0.7413008809089661\n",
      "Step: 901, Loss: 0.9057708382606506\n",
      "Step: 902, Loss: 1.0975592136383057\n",
      "Step: 903, Loss: 0.9477803707122803\n",
      "Step: 904, Loss: 1.0322279930114746\n",
      "Step: 905, Loss: 0.8172159194946289\n",
      "Step: 906, Loss: 1.1252107620239258\n",
      "Step: 907, Loss: 1.1052258014678955\n",
      "Step: 908, Loss: 1.1069835424423218\n",
      "Step: 909, Loss: 1.1214807033538818\n",
      "Step: 910, Loss: 0.8818759918212891\n",
      "Step: 911, Loss: 0.9060059785842896\n",
      "Step: 912, Loss: 0.9007360339164734\n",
      "Step: 913, Loss: 0.783591628074646\n",
      "Step: 914, Loss: 0.9354561567306519\n",
      "Step: 915, Loss: 1.002084493637085\n",
      "Step: 916, Loss: 1.062787413597107\n",
      "Step: 917, Loss: 0.8579913377761841\n",
      "Step: 918, Loss: 0.8705728054046631\n",
      "Step: 919, Loss: 1.1480664014816284\n",
      "Step: 920, Loss: 0.8914847373962402\n",
      "Step: 921, Loss: 1.0647919178009033\n",
      "Step: 922, Loss: 0.7652508020401001\n",
      "Step: 923, Loss: 0.9864838123321533\n",
      "Step: 924, Loss: 1.0960519313812256\n",
      "Step: 925, Loss: 0.7616879343986511\n",
      "Step: 926, Loss: 0.9522074460983276\n",
      "Step: 927, Loss: 0.9384475350379944\n",
      "Step: 928, Loss: 1.0077202320098877\n",
      "Step: 929, Loss: 0.9087451696395874\n",
      "Step: 930, Loss: 0.8641995191574097\n",
      "Step: 931, Loss: 1.1167621612548828\n",
      "Step: 932, Loss: 1.036620020866394\n",
      "Step: 933, Loss: 1.2745479345321655\n",
      "Step: 934, Loss: 1.074927806854248\n",
      "Step: 935, Loss: 0.8890168070793152\n",
      "Step: 936, Loss: 1.0989296436309814\n",
      "Step: 937, Loss: 1.3301873207092285\n",
      "Step: 938, Loss: 0.9931529760360718\n",
      "Step: 939, Loss: 0.9554717540740967\n",
      "Step: 940, Loss: 1.0223474502563477\n",
      "Step: 941, Loss: 1.0488462448120117\n",
      "Step: 942, Loss: 1.0659949779510498\n",
      "Step: 943, Loss: 0.6941905617713928\n",
      "Step: 944, Loss: 0.8453893661499023\n",
      "Step: 945, Loss: 1.0860731601715088\n",
      "Step: 946, Loss: 0.9867523908615112\n",
      "Step: 947, Loss: 0.8346686363220215\n",
      "Step: 948, Loss: 0.9123536348342896\n",
      "Step: 949, Loss: 0.9897921085357666\n",
      "Step: 950, Loss: 1.0674760341644287\n",
      "Step: 951, Loss: 0.8723554611206055\n",
      "Step: 952, Loss: 1.0228973627090454\n",
      "Step: 953, Loss: 0.7690014243125916\n",
      "Step: 954, Loss: 0.7316648960113525\n",
      "Step: 955, Loss: 0.9887328743934631\n",
      "Step: 956, Loss: 0.8800852298736572\n",
      "Step: 957, Loss: 0.8040180802345276\n",
      "Step: 958, Loss: 1.0282790660858154\n",
      "Step: 959, Loss: 0.9680887460708618\n",
      "Step: 960, Loss: 1.060123324394226\n",
      "Step: 961, Loss: 0.9881837368011475\n",
      "Step: 962, Loss: 0.9106993675231934\n",
      "Step: 963, Loss: 0.9186786413192749\n",
      "Step: 964, Loss: 0.9031304121017456\n",
      "Step: 965, Loss: 0.8883267641067505\n",
      "Step: 966, Loss: 0.9812294244766235\n",
      "Step: 967, Loss: 1.1339199542999268\n",
      "Step: 968, Loss: 1.0106019973754883\n",
      "Step: 969, Loss: 0.8314459323883057\n",
      "Step: 970, Loss: 1.014744758605957\n",
      "Step: 971, Loss: 0.9177881479263306\n",
      "Step: 972, Loss: 0.931381344795227\n",
      "Step: 973, Loss: 1.0664385557174683\n",
      "Step: 974, Loss: 1.056056022644043\n",
      "Step: 975, Loss: 0.970696747303009\n",
      "Step: 976, Loss: 1.2191941738128662\n",
      "Step: 977, Loss: 0.8770825862884521\n",
      "Step: 978, Loss: 1.0904524326324463\n",
      "Step: 979, Loss: 0.8905177116394043\n",
      "Step: 980, Loss: 0.9313511848449707\n",
      "Step: 981, Loss: 0.9620744585990906\n",
      "Step: 982, Loss: 1.0637056827545166\n",
      "Step: 983, Loss: 1.0314840078353882\n",
      "Step: 984, Loss: 0.9607421159744263\n",
      "Step: 985, Loss: 0.8274967074394226\n",
      "Step: 986, Loss: 0.8551946878433228\n",
      "Step: 987, Loss: 1.162766456604004\n",
      "Step: 988, Loss: 1.0412449836730957\n",
      "Step: 989, Loss: 0.9970436096191406\n",
      "Step: 990, Loss: 1.0244174003601074\n",
      "Step: 991, Loss: 0.885749101638794\n",
      "Step: 992, Loss: 0.9697362184524536\n",
      "Step: 993, Loss: 1.126200556755066\n",
      "Step: 994, Loss: 0.952217698097229\n",
      "Step: 995, Loss: 0.9671628475189209\n",
      "Step: 996, Loss: 0.8522768020629883\n",
      "Step: 997, Loss: 1.029703140258789\n",
      "Step: 998, Loss: 1.0907821655273438\n",
      "Step: 999, Loss: 0.7214469909667969\n",
      "Step: 1000, Loss: 0.9983406066894531\n"
     ]
    }
   ],
   "source": [
    "from mindspore.train import Callback\n",
    "\n",
    "# 定义LossMonitor回调函数\n",
    "class LossMonitor(Callback):\n",
    "    def __init__(self):\n",
    "        super(LossMonitor, self).__init__()\n",
    "\n",
    "    def step_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        print(\"Step: {}, Loss: {}\".format(cb_params.cur_step_num, cb_params.net_outputs.asnumpy()))\n",
    "\n",
    "# 进行模型训练\n",
    "model.train(10, ds_train, callbacks=[LossMonitor()])\n",
    "\n",
    "# 保存模型\n",
    "from mindspore import save_checkpoint\n",
    "save_checkpoint(net, 'model.ckpt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、模型预测"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载已经训练好的模型，并使用该模型进行预测和计算准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****start test****\n",
      "<generator object get_data at 0xffff4ea17a50>\n",
      "left left\n",
      "up stop\n",
      "down down\n",
      "right go\n",
      "stop stop\n",
      "go down\n",
      "stop stop\n",
      "left left\n",
      "stop up\n",
      "left left\n",
      "down down\n",
      "right right\n",
      "stop go\n",
      "go go\n",
      "yes yes\n",
      "stop stop\n",
      "no no\n",
      "yes yes\n",
      "left right\n",
      "stop stop\n",
      "no no\n",
      "left yes\n",
      "up up\n",
      "go go\n",
      "stop stop\n",
      "down no\n",
      "right right\n",
      "stop stop\n",
      "up up\n",
      "no no\n",
      "no no\n",
      "yes yes\n",
      "up go\n",
      "right right\n",
      "left left\n",
      "up up\n",
      "go down\n",
      "down down\n",
      "no no\n",
      "down go\n",
      "right right\n",
      "up up\n",
      "up no\n",
      "stop stop\n",
      "down no\n",
      "down down\n",
      "no no\n",
      "go down\n",
      "right right\n",
      "no no\n",
      "left left\n",
      "stop stop\n",
      "right right\n",
      "stop stop\n",
      "left left\n",
      "up up\n",
      "up down\n",
      "left left\n",
      "right right\n",
      "down up\n",
      "down no\n",
      "up stop\n",
      "down down\n",
      "no go\n",
      "up up\n",
      "down go\n",
      "up stop\n",
      "down go\n",
      "up up\n",
      "down down\n",
      "go go\n",
      "no no\n",
      "up up\n",
      "left left\n",
      "down down\n",
      "up up\n",
      "stop stop\n",
      "go down\n",
      "no up\n",
      "stop stop\n",
      "go go\n",
      "up up\n",
      "up left\n",
      "up up\n",
      "right right\n",
      "up up\n",
      "yes yes\n",
      "down down\n",
      "down down\n",
      "right right\n",
      "go go\n",
      "stop go\n",
      "stop stop\n",
      "down down\n",
      "go go\n",
      "right right\n",
      "go go\n",
      "right right\n",
      "go go\n",
      "right right\n",
      "down no\n",
      "go go\n",
      "go down\n",
      "stop stop\n",
      "right right\n",
      "stop stop\n",
      "down down\n",
      "up up\n",
      "go no\n",
      "no no\n",
      "yes left\n",
      "left yes\n",
      "yes yes\n",
      "no no\n",
      "right right\n",
      "go stop\n",
      "left left\n",
      "left left\n",
      "stop stop\n",
      "down go\n",
      "up up\n",
      "right right\n",
      "yes yes\n",
      "no down\n",
      "go go\n",
      "right right\n",
      "left left\n",
      "up go\n",
      "no no\n",
      "down no\n",
      "down go\n",
      "no left\n",
      "down go\n",
      "left left\n",
      "go stop\n",
      "stop stop\n",
      "left left\n",
      "yes yes\n",
      "go go\n",
      "left left\n",
      "go go\n",
      "yes yes\n",
      "yes yes\n",
      "go go\n",
      "left left\n",
      "yes yes\n",
      "left left\n",
      "up up\n",
      "up up\n",
      "yes yes\n",
      "down down\n",
      "yes yes\n",
      "right right\n",
      "go go\n",
      "down no\n",
      "up stop\n",
      "right right\n",
      "yes yes\n",
      "up up\n",
      "no go\n",
      "left left\n",
      "down down\n",
      "up stop\n",
      "yes yes\n",
      "stop up\n",
      "yes yes\n",
      "left left\n",
      "up up\n",
      "up stop\n",
      "up up\n",
      "right right\n",
      "up up\n",
      "go go\n",
      "yes left\n",
      "right right\n",
      "down down\n",
      "go go\n",
      "no go\n",
      "up up\n",
      "yes yes\n",
      "stop stop\n",
      "left left\n",
      "up up\n",
      "no no\n",
      "up up\n",
      "no no\n",
      "no down\n",
      "go go\n",
      "right right\n",
      "go go\n",
      "yes yes\n",
      "stop stop\n",
      "left left\n",
      "yes yes\n",
      "yes no\n",
      "up up\n",
      "up left\n",
      "down down\n",
      "yes yes\n",
      "yes yes\n",
      "up up\n",
      "left left\n",
      "stop stop\n",
      "up up\n",
      "go no\n",
      "right right\n",
      "no no\n",
      "right right\n",
      "down down\n",
      "no no\n",
      "left left\n",
      "down down\n",
      "right right\n",
      "go go\n",
      "left left\n",
      "up up\n",
      "up up\n",
      "stop stop\n",
      "no no\n",
      "left up\n",
      "go go\n",
      "down right\n",
      "down down\n",
      "yes left\n",
      "up up\n",
      "up up\n",
      "yes yes\n",
      "go no\n",
      "go down\n",
      "down no\n",
      "stop right\n",
      "down down\n",
      "go go\n",
      "go down\n",
      "no no\n",
      "stop stop\n",
      "yes yes\n",
      "up stop\n",
      "no go\n",
      "up up\n",
      "go go\n",
      "up up\n",
      "down stop\n",
      "stop stop\n",
      "up up\n",
      "down go\n",
      "yes yes\n",
      "no down\n",
      "up stop\n",
      "up up\n",
      "no stop\n",
      "go no\n",
      "up left\n",
      "go down\n",
      "no go\n",
      "down down\n",
      "no no\n",
      "no no\n",
      "down down\n",
      "go down\n",
      "up yes\n",
      "right left\n",
      "yes left\n",
      "up up\n",
      "no go\n",
      "down no\n",
      "yes yes\n",
      "go go\n",
      "go down\n",
      "right right\n",
      "yes yes\n",
      "yes yes\n",
      "up up\n",
      "stop stop\n",
      "right right\n",
      "go no\n",
      "up up\n",
      "down down\n",
      "no no\n",
      "left right\n",
      "stop stop\n",
      "right right\n",
      "up up\n",
      "stop stop\n",
      "yes yes\n",
      "yes yes\n",
      "yes yes\n",
      "right right\n",
      "no no\n",
      "yes yes\n",
      "right right\n",
      "up up\n",
      "down down\n",
      "down go\n",
      "left left\n",
      "no no\n",
      "go down\n",
      "down down\n",
      "up no\n",
      "yes yes\n",
      "go no\n",
      "stop stop\n",
      "down down\n",
      "yes yes\n",
      "right right\n",
      "up up\n",
      "left left\n",
      "no down\n",
      "down down\n",
      "no go\n",
      "up up\n",
      "right right\n",
      "up up\n",
      "go go\n",
      "up up\n",
      "left left\n",
      "left stop\n",
      "up up\n",
      "down down\n",
      "left yes\n",
      "up up\n",
      "right left\n",
      "up down\n",
      "up up\n",
      "go go\n",
      "right right\n",
      "stop stop\n",
      "no no\n",
      "yes yes\n",
      "up no\n",
      "go go\n",
      "stop stop\n",
      "stop stop\n",
      "right right\n",
      "no down\n",
      "up left\n",
      "right right\n",
      "right right\n",
      "stop stop\n",
      "yes no\n",
      "yes no\n",
      "down down\n",
      "left left\n",
      "no no\n",
      "right right\n",
      "no no\n",
      "go go\n",
      "right right\n",
      "up up\n",
      "up up\n",
      "no go\n",
      "up up\n",
      "stop stop\n",
      "up up\n",
      "stop down\n",
      "up go\n",
      "no no\n",
      "right right\n",
      "no no\n",
      "left left\n",
      "up up\n",
      "no down\n",
      "down down\n",
      "stop stop\n",
      "down go\n",
      "left left\n",
      "stop stop\n",
      "yes yes\n",
      "yes yes\n",
      "left right\n",
      "go no\n",
      "yes yes\n",
      "up go\n",
      "up left\n",
      "yes yes\n",
      "go down\n",
      "go go\n",
      "no no\n",
      "up stop\n",
      "up up\n",
      "right right\n",
      "right right\n",
      "no no\n",
      "up up\n",
      "up left\n",
      "no go\n",
      "go no\n",
      "no go\n",
      "left left\n",
      "up stop\n",
      "yes left\n",
      "no go\n",
      "yes yes\n",
      "no no\n",
      "yes yes\n",
      "left left\n",
      "yes yes\n",
      "right right\n",
      "stop stop\n",
      "go go\n",
      "go go\n",
      "down down\n",
      "go go\n",
      "left left\n",
      "go down\n",
      "no go\n",
      "up up\n",
      "down down\n",
      "up left\n",
      "stop stop\n",
      "no no\n",
      "up right\n",
      "go go\n",
      "left left\n",
      "stop left\n",
      "left left\n",
      "no down\n",
      "yes yes\n",
      "left yes\n",
      "yes yes\n",
      "up up\n",
      "yes yes\n",
      "down down\n",
      "yes yes\n",
      "up up\n",
      "down down\n",
      "go stop\n",
      "go go\n",
      "right right\n",
      "stop stop\n",
      "up go\n",
      "up up\n",
      "right right\n",
      "up up\n",
      "down down\n",
      "go go\n",
      "down down\n",
      "down go\n",
      "left left\n",
      "up up\n",
      "left left\n",
      "right right\n",
      "yes yes\n",
      "left up\n",
      "up up\n",
      "go go\n",
      "stop stop\n",
      "up go\n",
      "down down\n",
      "go no\n",
      "no no\n",
      "right left\n",
      "yes yes\n",
      "up no\n",
      "up up\n",
      "yes yes\n",
      "no no\n",
      "go go\n",
      "yes yes\n",
      "no no\n",
      "up up\n",
      "down down\n",
      "down go\n",
      "down down\n",
      "up up\n",
      "down go\n",
      "no go\n",
      "no down\n",
      "yes yes\n",
      "right right\n",
      "up right\n",
      "yes yes\n",
      "yes yes\n",
      "yes yes\n",
      "no go\n",
      "go go\n",
      "no down\n",
      "yes yes\n",
      "up left\n",
      "yes yes\n",
      "up up\n",
      "stop stop\n",
      "go go\n",
      "left left\n",
      "down down\n",
      "down down\n",
      "stop stop\n",
      "yes yes\n",
      "no no\n",
      "up up\n",
      "down down\n",
      "yes yes\n",
      "right right\n",
      "up stop\n",
      "up up\n",
      "go go\n",
      "right right\n",
      "up up\n",
      "down down\n",
      "yes yes\n",
      "right right\n",
      "yes left\n",
      "up up\n",
      "no no\n",
      "up stop\n",
      "no no\n",
      "no down\n",
      "left left\n",
      "yes yes\n",
      "yes yes\n",
      "up up\n",
      "no no\n",
      "no down\n",
      "stop stop\n",
      "yes up\n",
      "left yes\n",
      "go go\n",
      "go go\n",
      "no no\n",
      "down down\n",
      "left left\n",
      "left left\n",
      "right right\n",
      "stop stop\n",
      "go go\n",
      "down down\n",
      "yes yes\n",
      "go go\n",
      "stop stop\n",
      "stop stop\n",
      "left left\n",
      "go no\n",
      "go go\n",
      "up no\n",
      "up no\n",
      "go no\n",
      "left up\n",
      "no right\n",
      "up go\n",
      "stop stop\n",
      "up up\n",
      "down go\n",
      "down down\n",
      "no go\n",
      "left left\n",
      "no no\n",
      "right right\n",
      "go go\n",
      "go go\n",
      "go no\n",
      "up right\n",
      "yes yes\n",
      "up go\n",
      "no no\n",
      "left right\n",
      "down down\n",
      "stop stop\n",
      "up left\n",
      "yes yes\n",
      "go go\n",
      "go go\n",
      "up up\n",
      "up no\n",
      "right right\n",
      "down down\n",
      "up stop\n",
      "left right\n",
      "left left\n",
      "stop stop\n",
      "left yes\n",
      "yes yes\n",
      "up up\n",
      "left up\n",
      "left down\n",
      "stop stop\n",
      "down down\n",
      "yes yes\n",
      "right right\n",
      "left yes\n",
      "right right\n",
      "right yes\n",
      "up up\n",
      "no no\n",
      "up up\n",
      "up up\n",
      "up up\n",
      "yes yes\n",
      "yes yes\n",
      "stop stop\n",
      "left left\n",
      "right right\n",
      "right right\n",
      "up up\n",
      "yes yes\n",
      "up up\n",
      "down down\n",
      "no no\n",
      "up up\n",
      "yes yes\n",
      "up up\n",
      "up no\n",
      "left left\n",
      "left left\n",
      "right right\n",
      "go go\n",
      "go no\n",
      "right right\n",
      "up up\n",
      "left left\n",
      "no no\n",
      "no no\n",
      "up up\n",
      "yes yes\n",
      "yes left\n",
      "up up\n",
      "right right\n",
      "down go\n",
      "stop stop\n",
      "left left\n",
      "left left\n",
      "down go\n",
      "right right\n",
      "left no\n",
      "yes yes\n",
      "stop stop\n",
      "stop stop\n",
      "right right\n",
      "down down\n",
      "stop stop\n",
      "left left\n",
      "go down\n",
      "up stop\n",
      "yes yes\n",
      "stop stop\n",
      "no no\n",
      "go go\n",
      "stop stop\n",
      "left left\n",
      "stop stop\n",
      "left yes\n",
      "right right\n",
      "up up\n",
      "right right\n",
      "right right\n",
      "go go\n",
      "down go\n",
      "left left\n",
      "no go\n",
      "down down\n",
      "up down\n",
      "up up\n",
      "stop stop\n",
      "left yes\n",
      "go down\n",
      "go down\n",
      "yes yes\n",
      "left right\n",
      "stop stop\n",
      "go down\n",
      "right right\n",
      "stop stop\n",
      "down down\n",
      "yes yes\n",
      "up left\n",
      "right right\n",
      "go no\n",
      "right right\n",
      "down down\n",
      "down down\n",
      "down down\n",
      "left left\n",
      "go stop\n",
      "up stop\n",
      "go go\n",
      "down down\n",
      "no no\n",
      "down down\n",
      "left no\n",
      "up up\n",
      "go go\n",
      "go go\n",
      "yes yes\n",
      "up up\n",
      "down down\n",
      "go go\n",
      "stop stop\n",
      "stop stop\n",
      "yes yes\n",
      "go go\n",
      "left left\n",
      "no no\n",
      "up stop\n",
      "down down\n",
      "go go\n",
      "up up\n",
      "down down\n",
      "stop stop\n",
      "yes yes\n",
      "right no\n",
      "yes yes\n",
      "left right\n",
      "up stop\n",
      "up up\n",
      "go no\n",
      "up up\n",
      "stop stop\n",
      "right right\n",
      "up go\n",
      "down no\n",
      "up left\n",
      "right right\n",
      "down no\n",
      "no no\n",
      "right left\n",
      "go go\n",
      "yes yes\n",
      "no go\n",
      "go go\n",
      "down down\n",
      "stop stop\n",
      "go down\n",
      "yes yes\n",
      "right right\n",
      "left left\n",
      "no go\n",
      "yes yes\n",
      "yes yes\n",
      "left up\n",
      "down stop\n",
      "up up\n",
      "down down\n",
      "up up\n",
      "down down\n",
      "yes yes\n",
      "up up\n",
      "no go\n",
      "up up\n",
      "yes down\n",
      "up up\n",
      "no no\n",
      "down down\n",
      "up stop\n",
      "go go\n",
      "right right\n",
      "stop stop\n",
      "no left\n",
      "right right\n",
      "go up\n",
      "down down\n",
      "right right\n",
      "down down\n",
      "left stop\n",
      "down down\n",
      "right right\n",
      "stop stop\n",
      "left left\n",
      "right right\n",
      "up stop\n",
      "stop stop\n",
      "yes yes\n",
      "go no\n",
      "go go\n",
      "yes yes\n",
      "no no\n",
      "up up\n",
      "yes stop\n",
      "left left\n",
      "up up\n",
      "stop stop\n",
      "down down\n",
      "right right\n",
      "no no\n",
      "right right\n",
      "yes yes\n",
      "right right\n",
      "up up\n",
      "stop stop\n",
      "down down\n",
      "yes yes\n",
      "no no\n",
      "stop stop\n",
      "right go\n",
      "left left\n",
      "up left\n",
      "down down\n",
      "yes left\n",
      "yes yes\n",
      "down down\n",
      "stop stop\n",
      "go no\n",
      "up up\n",
      "yes yes\n",
      "stop stop\n",
      "no go\n",
      "stop stop\n",
      "no no\n",
      "right right\n",
      "right right\n",
      "stop stop\n",
      "down down\n",
      "accuracy:  0.73\n"
     ]
    }
   ],
   "source": [
    "net = Net(batch_size=1)\n",
    "\n",
    "from mindspore import load_checkpoint\n",
    "from mindspore import Tensor\n",
    "\n",
    "# 读取训练的模型文件\n",
    "ckpt_file_name = \"./model.ckpt\"\n",
    "param_dict = load_checkpoint(ckpt_file_name, net)\n",
    "\n",
    "print(\"****start test****\")\n",
    "# 获取测试文件\n",
    "batch = get_data(test_file_path) \n",
    "print(batch)\n",
    "# 初始化准确率\n",
    "accu = 0 \n",
    "size=800\n",
    "\n",
    "# 根据训练好的模型进行预测\n",
    "for i in range(size):\n",
    "    input_x, label = next(batch)\n",
    "    output = net(Tensor(input_x))\n",
    "    index = np.argmax(output.asnumpy())\n",
    "    # 输出期望值、预测值\n",
    "    print(commands[index], commands[label]) \n",
    "    if index == label:\n",
    "        # 若预测成功则成功数量+1，记录预测成功的样本数量\n",
    "        accu += 1      \n",
    "# 准确率\n",
    "print(\"accuracy: \", accu*1.0 / size )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，最终测试集的准确率表示训练效果良好，基本达到语音识别分类目的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
