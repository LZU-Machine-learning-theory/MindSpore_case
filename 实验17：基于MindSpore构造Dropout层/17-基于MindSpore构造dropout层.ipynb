{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2509b842",
   "metadata": {},
   "source": [
    "# 17  基于MindSpore构造Dropout层\n",
    "\n",
    "本实验主要介绍使用MindSpore深度学习框架中使用Dropout层，并对比Dropout层对深度学习模型性能的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8175d5a",
   "metadata": {},
   "source": [
    "# 1 实验目的\n",
    "- 掌握如何使用Mindspore深度学习框架构造Dropout层；\n",
    "- 了解Dropout层的相关原理以及应用；\n",
    "- 了解dropout层对于深度学习模型性能的影响;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddd775",
   "metadata": {},
   "source": [
    "# 2 Dropout层背景及知识点介绍\n",
    "## 2.1背景知识\n",
    "2012年，Hinton在论文[《Improving neural networks by preventing co-adaptation of feature detectors》](https://arxiv.org/pdf/1207.0580.pdf)中提出Dropout。当一个复杂的前馈神经网络被训练在小数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。同年，Alex、Hinton在论文[《ImageNet Classification with Deep Convolutional Neural Networks》](https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1166C42C86BD3A3C6C75B53E2CD2E14F?doi=10.1.1.299.205&rep=rep1&type=pdf)中使用了Dropout算法，用于防止过拟合。该论文赢得了2012年图像识别大赛冠军。随后，关于Dropout的论文[《Dropout:A Simple Way to Prevent Neural Networks from Overfitting》](http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)、[《Improving Neural Networks with Dropout》](http://www.cs.toronto.edu/~nitish/msc_thesis.pdf)、[《Dropout as data augmentation》](https://arxiv.org/pdf/1506.08700.pdf)等不断提高Dropout层的应用。\n",
    "\n",
    "## 2.2Dropout原理介绍\n",
    "\n",
    "Dropout作为训练深度神经网络的trick供开发者选择。其原理是在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），从而地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用（检测器相互作用是指某些检测器依赖其他检测器才能发挥作用）。具体的说：在前向传播时，让某个神经元的激活值以一定的概率p停止工作，从而使模型泛化性更强，减少对某些局部的特征的依赖。\n",
    "\n",
    "![avatar](fig/fig1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5823dd",
   "metadata": {},
   "source": [
    "# 3 实验环境\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=MindSpore 2.0；Python环境=3.7。\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU CUDA 10.1|Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e191b6",
   "metadata": {},
   "source": [
    "# 4. 数据处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809458b1",
   "metadata": {},
   "source": [
    "## 4.1 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30144e4c",
   "metadata": {},
   "source": [
    "MNIST数据集是机器学习领域中经典的数据集，由60,000个训练样本和10,000个测试样本组成，每张图片都是28x28像素大小的灰度图像，数字从0到9。\n",
    "\n",
    "MNIST数据集主要用于图像识别领域的研究，特别是用于机器学习算法的测试和比较。由于它的简单性和易于使用，MNIST已经成为机器学习社区中的标准数据集之一。\n",
    "\n",
    "以下是几个示例图像：\n",
    "\n",
    "![jupyter](./fig/Fig001.png)\n",
    "\n",
    "每个图像都被转换成784个数字的一维向量（28×28=784）。这些数字表示像素的灰度值，值的范围从0（黑色）到255（白色）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7ef77",
   "metadata": {},
   "source": [
    "从开放数据集中下载MNIST数据集的压缩包,并解压储存在项目的根目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1162ceb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip (10.3 MB)\n",
      "\n",
      "file_sizes: 100%|██████████████████████████| 10.8M/10.8M [00:01<00:00, 10.2MB/s]\n",
      "Extracting zip file...\n",
      "Successfully downloaded / unzipped to ./\n"
     ]
    }
   ],
   "source": [
    "# 从开放数据集中下载MNIST数据集\n",
    "from download import download\n",
    "\n",
    "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/\" \\\n",
    "      \"notebook/datasets/MNIST_Data.zip\"\n",
    "path = download(url, \"./\", kind=\"zip\", replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e57aec",
   "metadata": {},
   "source": [
    "下载后的数据集目录结构如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2fa8f",
   "metadata": {},
   "source": [
    "/MNIST_Data\\\n",
    "├── train\\\n",
    "&emsp;├──train-images-idx3-ubyte\\\n",
    "&emsp;└──train-labels-idx1-ubyte\\\n",
    "├──text\\\n",
    "&emsp;├──t10k-images-idx3-ubyte\\\n",
    "&emsp;└──t10k-labels-idx1-ubyte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abc5cb",
   "metadata": {},
   "source": [
    "## 4.2 设置超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f6ed1",
   "metadata": {},
   "source": [
    "集中设置在模型训练过程中需要使用到的超参数的具体数值，方便在后续调试代码时修改超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b95181c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE= 64       # batch的大小\n",
    "LEARNING_RATE = 1e-2 # 学习率\n",
    "EPOCH = 200      # 迭代次数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0497d5",
   "metadata": {},
   "source": [
    "## 4.3 获取数据集对象\n",
    "利用mindspore中的dataset相关的函数读取数据集中的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7b1cc920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.dataset import transforms\n",
    "# 读取和解析Manifest数据文件构建数据集\n",
    "from mindspore.dataset import MnistDataset\n",
    "train_dataset = MnistDataset('MNIST_Data/train')\n",
    "test_dataset = MnistDataset('MNIST_Data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d28a0",
   "metadata": {},
   "source": [
    "## 4.4 数据处理 \n",
    "MindSpore的dataset使用数据处理流水线（Data Processing Pipeline），需指定map、batch、shuffle等操作。这里我们使用map对图像数据及标签进行变换处理，然后将处理好的数据集打包为大小为64的batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c12fc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MindSpore库\n",
    "import mindspore\n",
    "# 神经网络模块\n",
    "from mindspore import nn\n",
    "# 常见算子操作\n",
    "from mindspore import ops\n",
    "# 图像增强模块\n",
    "from mindspore.dataset import vision\n",
    "def datapipe(dataset, batch_size):\n",
    "    image_transforms = [\n",
    "        # 基于给定的缩放和平移因子调整图像的像素大小。输出图像的像素大小为：output = image * rescale + shift。\n",
    "        # 此处rescale取1.0 / 255.0，shift取0\n",
    "        vision.Rescale(1.0 / 255.0, 0),\n",
    "        # 正则化 均值为0.1307，标准差为0.3081（查自官网）\n",
    "        vision.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "        # 将输入图像的shape从 <H, W, C> 转换为 <C, H, W>\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "    # 将输入的Tensor转换为指定的数据类型。\n",
    "    label_transform = transforms.TypeCast(mindspore.int32)\n",
    "\n",
    "    # map给定一组数据增强列表，按顺序将数据增强作用在数据集对象上。\n",
    "    dataset = dataset.map(image_transforms, 'image')\n",
    "    dataset = dataset.map(label_transform, 'label')\n",
    "    # 将数据集中连续 batch_size 条数据组合为一个批数据\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "# 对数据集进行transfrom和batch\n",
    "train_dataset = datapipe(train_dataset, BATCH_SIZE)\n",
    "test_dataset = datapipe(test_dataset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f311a",
   "metadata": {},
   "source": [
    "# 5. 实验过程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030dceb8",
   "metadata": {},
   "source": [
    "## 5.1 导入所需库和函数\n",
    "在代码最开始集中导入整个实验中所需要使用的库和函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7c481114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通用数据增强\n",
    "from mindspore.dataset import transforms\n",
    "# 读取和解析Manifest数据文件构建数据集\n",
    "from mindspore.dataset import MnistDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4d5eb",
   "metadata": {},
   "source": [
    "## 5.2 模型构建\n",
    "使用mindspore中提供的相关函数构建训练使用的神经网络模型，包括平图层，致密连接层等，并在construct中完成神经网络的前向构造。\\\n",
    "为了对比dropout层对深度学习的影响，做如下设置：\n",
    "1. 构建两个除了dropout层完全相同的网络模型。\n",
    "2. 采用完全相同的超参数及相关设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0fd8360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): Dropout<keep_prob=0.5>\n",
      "    (4): ReLU<>\n",
      "    (5): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# 定义附加dropout层的模型\n",
    "# MindSpore 中提供用户通过继承 nn.Cell 来方便用户创建和执行自己的网络\n",
    "class Network(nn.Cell): \n",
    "    # 自定义的网络中，需要在__init__构造函数中申明各个层的定义\n",
    "    def __init__(self): \n",
    "         # 继承父类nn.cell的__init__方法\n",
    "        super().__init__()         \n",
    "        # nn.Flatten为输入展成平图层，即去掉那些空的维度\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 使用SequentialCell对网络进行管理\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            # nn.Dense为致密连接层，它的第一个参数为输入层的维度，第二个参数为输出的维度，\n",
    "            # 第三个参数为神经网络可训练参数W权重矩阵的初始化方式，默认为normal\n",
    "            # nn.ReLU()非线性激活函数，它往往比论文中的sigmoid激活函数具有更好的效益\n",
    "            nn.Dense(28 * 28, 512), # 致密连接层 输入28*28 输出512\n",
    "            nn.ReLU(),              # ReLU层\n",
    "            nn.Dense(512, 512),     # 致密连接层 输入512 输出512\n",
    "            nn.Dropout(keep_prob=0.5),# Dropout层，舍弃率为0.5\n",
    "            nn.ReLU(),              # ReLu层\n",
    "            # nn.Dropout(keep_prob=0.1),# Dropout层，舍弃率为0.1\n",
    "            nn.Dense(512, 10)       # 致密连接层 输入512 输出10\n",
    "            # nn.Dropout(keep_prob=0.1)# Dropout层，舍弃率为0.1\n",
    "        \n",
    "        )\n",
    "    # 在construct中实现层之间的连接关系，完成神经网络的前向构造\n",
    "    def construct(self, x):\n",
    "         #调用init中定义的self.flatten()方法\n",
    "        x = self.flatten(x)\n",
    "        #调用init中的self.dense_relu_sequential()方法\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        # 返回模型\n",
    "        return logits\n",
    "model = Network()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c668f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network2<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# 定义不附加dropout层的模型\n",
    "class Network2(nn.Cell): \n",
    "    # 自定义的网络中，需要在__init__构造函数中申明各个层的定义\n",
    "    def __init__(self): \n",
    "         # 继承父类nn.cell的__init__方法\n",
    "        super().__init__()         \n",
    "        # nn.Flatten为输入展成平图层，即去掉那些空的维度\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 使用SequentialCell对网络进行管理\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            # nn.Dense为致密连接层，它的第一个参数为输入层的维度，第二个参数为输出的维度，\n",
    "            # 第三个参数为神经网络可训练参数W权重矩阵的初始化方式，默认为normal\n",
    "            # nn.ReLU()非线性激活函数，它往往比论文中的sigmoid激活函数具有更好的效益\n",
    "            nn.Dense(28 * 28, 512), # 致密连接层 输入28*28 输出512\n",
    "            # nn.Dropout(keep_prob=0.3),# Dropout层，舍弃率为0.3\n",
    "            nn.ReLU(),              # ReLU层\n",
    "            nn.Dense(512, 512),     # 致密连接层 输入512 输出512\n",
    "            nn.ReLU(),              # ReLu层\n",
    "            nn.Dense(512, 10)       # 致密连接层 输入512 输出10\n",
    "        \n",
    "        )\n",
    "    # 在construct中实现层之间的连接关系，完成神经网络的前向构造\n",
    "    def construct(self, x):\n",
    "         #调用init中定义的self.flatten()方法\n",
    "        x = self.flatten(x)\n",
    "        #调用init中的self.dense_relu_sequential()方法\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        # 返回模型\n",
    "        return logits\n",
    "model2 = Network2()\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74bdbc",
   "metadata": {},
   "source": [
    "# 6、模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845bc2a",
   "metadata": {},
   "source": [
    "在模型训练中，一个完整的训练过程（step）需要实现以下三步：\n",
    "1. 正向计算：模型预测结果（logits），并与正确标签（label）求预测损失（loss）。\n",
    "2. 反向传播：利用自动微分机制，自动求模型参数（parameters）对于loss的梯度（gradients）。\n",
    "3. 参数优化：将梯度更新到参数上。\n",
    "\n",
    "MindSpore使用函数式自动微分机制，因此针对上述步骤需要实现：\n",
    "1. 正向计算函数定义。\n",
    "2. 通过函数变换获得梯度计算函数。\n",
    "3. 训练函数定义，执行正向计算、反向传播和参数优化。\n",
    "\n",
    "为对比增加dropout层和普通定义模型，进行如下设置：\n",
    "1. 构建完全相同的梯度下降方法分别训练两个模型。\n",
    "2. 采用完全相同的数据集以及相同的分割比例。\n",
    "3. 采用相同的交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ac1ffeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化损失函数和优化器\n",
    "# 计算预测值和目标值之间的交叉熵损失\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "#构建一个Optimizer对象，能够保持当前参数状态并基于计算得到的梯度进行参数更新 此处使用随机梯度下降算法\n",
    "optimizer = nn.SGD(model.trainable_params(), learning_rate=LEARNING_RATE) \n",
    "optimizer2 = nn.SGD(model2.trainable_params(), learning_rate=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "70123538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, loss_fn, optimizer):\n",
    "    # 定义 forward 函数\n",
    "    def forward_fn(data, label):\n",
    "        # 将数据载入模型\n",
    "        logits = model(data)\n",
    "        # 根据模型训练获取损失函数值\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    # 调用梯度函数，value_and_grad()为生成求导函数，用于计算给定函数的正向计算结果和梯度\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "    # 定义一步训练的函数\n",
    "    def train_step(data, label):\n",
    "        # 计算梯度，记录变量是怎么来的\n",
    "        (loss, _), grads = grad_fn(data, label)\n",
    "        # 获得损失 depend用来处理操作间的依赖关系\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        # 批量训练获得损失值\n",
    "        loss = train_step(data, label)\n",
    "        # 当完成所有数据样本的训练\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7d8e9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model2, dataset, loss_fn, optimizer2):\n",
    "    # 定义 forward 函数\n",
    "    def forward_fn(data, label):\n",
    "        # 将数据载入模型\n",
    "        logits = model2(data)\n",
    "        # 根据模型训练获取损失函数值\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    # 调用梯度函数，value_and_grad()为生成求导函数，用于计算给定函数的正向计算结果和梯度\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer2.parameters, has_aux=True)\n",
    "    # 定义一步训练的函数\n",
    "    def train_step(data, label):\n",
    "        # 计算梯度，记录变量是怎么来的\n",
    "        (loss, _), grads = grad_fn(data, label)\n",
    "        # 获得损失 depend用来处理操作间的依赖关系\n",
    "        loss = ops.depend(loss, optimizer2(grads))\n",
    "        return loss\n",
    "    size = dataset.get_dataset_size()\n",
    "    model2.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        # 批量训练获得损失值\n",
    "        loss = train_step(data, label)\n",
    "        # 当完成所有数据样本的训练\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c19c031",
   "metadata": {},
   "source": [
    "定义完全相同的两个测试函数，通过调用mindspore中的相关函数，使用accuracy以及平均损失等指标评估预测模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3e35429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator(): # 遍历所有测试样本数据\n",
    "        pred = model(data)                              # 根据已训练模型获取预测值\n",
    "        total += len(data)                              # 统计样本数\n",
    "        test_loss += loss_fn(pred, label).asnumpy()     # 统计样本损失值\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()# 统计预测正确的样本个数\n",
    "    test_loss /= num_batches                              # 求得平均损失\n",
    "    correct /= total                                      # 计算accuracy\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "84443fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model2, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model2.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator(): # 遍历所有测试样本数据\n",
    "        pred = model2(data)                              # 根据已训练模型获取预测值\n",
    "        total += len(data)                              # 统计样本数\n",
    "        test_loss += loss_fn(pred, label).asnumpy()     # 统计样本损失值\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()# 统计预测正确的样本个数\n",
    "    test_loss /= num_batches                              # 求得平均损失\n",
    "    correct /= total                                      # 计算accuracy\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c6d5b",
   "metadata": {},
   "source": [
    "训练过程需多次迭代数据集，一次完整的迭代称为一轮（epoch）。在每一轮，遍历训练集进行训练，结束后使用测试集进行预测。打印每一轮的loss值和预测准确率（Accuracy），可以看到loss在不断下降，Accuracy在不断提高。\\\n",
    "相比较没有dropout层的模型，可以看到在测试集中，搭建了dropout层的模型准确率更高，损失函数更小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "51406e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303168  [  0/938]\n",
      "loss: 2.290722  [100/938]\n",
      "loss: 2.277732  [200/938]\n",
      "loss: 2.217930  [300/938]\n",
      "loss: 2.032400  [400/938]\n",
      "loss: 1.600762  [500/938]\n",
      "loss: 1.086087  [600/938]\n",
      "loss: 0.924158  [700/938]\n",
      "loss: 0.985418  [800/938]\n",
      "loss: 0.657237  [900/938]\n",
      "Test: \n",
      " Accuracy: 84.8%, Avg loss: 0.551248 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.485563  [  0/938]\n",
      "loss: 0.606317  [100/938]\n",
      "loss: 0.357362  [200/938]\n",
      "loss: 0.529909  [300/938]\n",
      "loss: 0.351772  [400/938]\n",
      "loss: 0.438352  [500/938]\n",
      "loss: 0.358124  [600/938]\n",
      "loss: 0.332275  [700/938]\n",
      "loss: 0.505310  [800/938]\n",
      "loss: 0.164845  [900/938]\n",
      "Test: \n",
      " Accuracy: 90.2%, Avg loss: 0.339299 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.287869  [  0/938]\n",
      "loss: 0.491125  [100/938]\n",
      "loss: 0.423553  [200/938]\n",
      "loss: 0.253829  [300/938]\n",
      "loss: 0.354354  [400/938]\n",
      "loss: 0.162713  [500/938]\n",
      "loss: 0.355667  [600/938]\n",
      "loss: 0.332413  [700/938]\n",
      "loss: 0.423598  [800/938]\n",
      "loss: 0.537284  [900/938]\n",
      "Test: \n",
      " Accuracy: 91.8%, Avg loss: 0.281351 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.242408  [  0/938]\n",
      "loss: 0.355052  [100/938]\n",
      "loss: 0.286729  [200/938]\n",
      "loss: 0.164357  [300/938]\n",
      "loss: 0.207063  [400/938]\n",
      "loss: 0.316899  [500/938]\n",
      "loss: 0.238396  [600/938]\n",
      "loss: 0.164506  [700/938]\n",
      "loss: 0.255135  [800/938]\n",
      "loss: 0.243977  [900/938]\n",
      "Test: \n",
      " Accuracy: 92.9%, Avg loss: 0.239173 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.224891  [  0/938]\n",
      "loss: 0.313441  [100/938]\n",
      "loss: 0.139947  [200/938]\n",
      "loss: 0.184205  [300/938]\n",
      "loss: 0.254970  [400/938]\n",
      "loss: 0.277337  [500/938]\n",
      "loss: 0.141728  [600/938]\n",
      "loss: 0.224305  [700/938]\n",
      "loss: 0.274766  [800/938]\n",
      "loss: 0.196430  [900/938]\n",
      "Test: \n",
      " Accuracy: 94.0%, Avg loss: 0.208303 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.241593  [  0/938]\n",
      "loss: 0.147544  [100/938]\n",
      "loss: 0.096911  [200/938]\n",
      "loss: 0.099522  [300/938]\n",
      "loss: 0.169584  [400/938]\n",
      "loss: 0.190933  [500/938]\n",
      "loss: 0.196691  [600/938]\n",
      "loss: 0.191450  [700/938]\n",
      "loss: 0.121528  [800/938]\n",
      "loss: 0.106476  [900/938]\n",
      "Test: \n",
      " Accuracy: 94.6%, Avg loss: 0.180516 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.090762  [  0/938]\n",
      "loss: 0.162024  [100/938]\n",
      "loss: 0.198217  [200/938]\n",
      "loss: 0.160088  [300/938]\n",
      "loss: 0.395504  [400/938]\n",
      "loss: 0.378135  [500/938]\n",
      "loss: 0.081436  [600/938]\n",
      "loss: 0.259485  [700/938]\n",
      "loss: 0.261677  [800/938]\n",
      "loss: 0.145557  [900/938]\n",
      "Test: \n",
      " Accuracy: 95.2%, Avg loss: 0.164220 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.081718  [  0/938]\n",
      "loss: 0.364681  [100/938]\n",
      "loss: 0.150890  [200/938]\n",
      "loss: 0.171302  [300/938]\n",
      "loss: 0.426347  [400/938]\n",
      "loss: 0.369621  [500/938]\n",
      "loss: 0.083390  [600/938]\n",
      "loss: 0.210494  [700/938]\n",
      "loss: 0.097979  [800/938]\n",
      "loss: 0.159017  [900/938]\n",
      "Test: \n",
      " Accuracy: 95.6%, Avg loss: 0.147843 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.422087  [  0/938]\n",
      "loss: 0.144157  [100/938]\n",
      "loss: 0.123082  [200/938]\n",
      "loss: 0.320264  [300/938]\n",
      "loss: 0.196234  [400/938]\n",
      "loss: 0.069769  [500/938]\n",
      "loss: 0.204628  [600/938]\n",
      "loss: 0.100911  [700/938]\n",
      "loss: 0.205976  [800/938]\n",
      "loss: 0.124337  [900/938]\n",
      "Test: \n",
      " Accuracy: 95.9%, Avg loss: 0.138929 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.091324  [  0/938]\n",
      "loss: 0.168941  [100/938]\n",
      "loss: 0.112003  [200/938]\n",
      "loss: 0.116664  [300/938]\n",
      "loss: 0.158222  [400/938]\n",
      "loss: 0.235219  [500/938]\n",
      "loss: 0.172064  [600/938]\n",
      "loss: 0.110199  [700/938]\n",
      "loss: 0.153175  [800/938]\n",
      "loss: 0.136838  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.1%, Avg loss: 0.124564 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.094827  [  0/938]\n",
      "loss: 0.162821  [100/938]\n",
      "loss: 0.065331  [200/938]\n",
      "loss: 0.079452  [300/938]\n",
      "loss: 0.139815  [400/938]\n",
      "loss: 0.155354  [500/938]\n",
      "loss: 0.062832  [600/938]\n",
      "loss: 0.044143  [700/938]\n",
      "loss: 0.063938  [800/938]\n",
      "loss: 0.114733  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.5%, Avg loss: 0.117798 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.055672  [  0/938]\n",
      "loss: 0.203748  [100/938]\n",
      "loss: 0.095442  [200/938]\n",
      "loss: 0.045572  [300/938]\n",
      "loss: 0.078935  [400/938]\n",
      "loss: 0.085250  [500/938]\n",
      "loss: 0.185142  [600/938]\n",
      "loss: 0.070061  [700/938]\n",
      "loss: 0.063070  [800/938]\n",
      "loss: 0.035732  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.7%, Avg loss: 0.110638 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.103456  [  0/938]\n",
      "loss: 0.080601  [100/938]\n",
      "loss: 0.062086  [200/938]\n",
      "loss: 0.062782  [300/938]\n",
      "loss: 0.057590  [400/938]\n",
      "loss: 0.050018  [500/938]\n",
      "loss: 0.305695  [600/938]\n",
      "loss: 0.151454  [700/938]\n",
      "loss: 0.096374  [800/938]\n",
      "loss: 0.103675  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.8%, Avg loss: 0.105685 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.090108  [  0/938]\n",
      "loss: 0.224643  [100/938]\n",
      "loss: 0.049577  [200/938]\n",
      "loss: 0.251388  [300/938]\n",
      "loss: 0.154614  [400/938]\n",
      "loss: 0.150493  [500/938]\n",
      "loss: 0.088376  [600/938]\n",
      "loss: 0.140739  [700/938]\n",
      "loss: 0.255311  [800/938]\n",
      "loss: 0.082954  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.9%, Avg loss: 0.099887 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.031630  [  0/938]\n",
      "loss: 0.036152  [100/938]\n",
      "loss: 0.108670  [200/938]\n",
      "loss: 0.066993  [300/938]\n",
      "loss: 0.039808  [400/938]\n",
      "loss: 0.086352  [500/938]\n",
      "loss: 0.049622  [600/938]\n",
      "loss: 0.029869  [700/938]\n",
      "loss: 0.093508  [800/938]\n",
      "loss: 0.220474  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.1%, Avg loss: 0.096308 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.066486  [  0/938]\n",
      "loss: 0.058713  [100/938]\n",
      "loss: 0.060895  [200/938]\n",
      "loss: 0.114230  [300/938]\n",
      "loss: 0.130206  [400/938]\n",
      "loss: 0.183726  [500/938]\n",
      "loss: 0.064209  [600/938]\n",
      "loss: 0.041117  [700/938]\n",
      "loss: 0.044342  [800/938]\n",
      "loss: 0.046538  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.2%, Avg loss: 0.089724 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.067785  [  0/938]\n",
      "loss: 0.070822  [100/938]\n",
      "loss: 0.131282  [200/938]\n",
      "loss: 0.047934  [300/938]\n",
      "loss: 0.032932  [400/938]\n",
      "loss: 0.111458  [500/938]\n",
      "loss: 0.084944  [600/938]\n",
      "loss: 0.101064  [700/938]\n",
      "loss: 0.181305  [800/938]\n",
      "loss: 0.063437  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.3%, Avg loss: 0.087405 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.066560  [  0/938]\n",
      "loss: 0.207371  [100/938]\n",
      "loss: 0.066067  [200/938]\n",
      "loss: 0.036261  [300/938]\n",
      "loss: 0.121379  [400/938]\n",
      "loss: 0.024086  [500/938]\n",
      "loss: 0.074772  [600/938]\n",
      "loss: 0.035890  [700/938]\n",
      "loss: 0.251140  [800/938]\n",
      "loss: 0.021061  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.5%, Avg loss: 0.083758 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.060909  [  0/938]\n",
      "loss: 0.102151  [100/938]\n",
      "loss: 0.073090  [200/938]\n",
      "loss: 0.037106  [300/938]\n",
      "loss: 0.059692  [400/938]\n",
      "loss: 0.051499  [500/938]\n",
      "loss: 0.039866  [600/938]\n",
      "loss: 0.067746  [700/938]\n",
      "loss: 0.069823  [800/938]\n",
      "loss: 0.049122  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.5%, Avg loss: 0.079779 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.074085  [  0/938]\n",
      "loss: 0.089888  [100/938]\n",
      "loss: 0.016946  [200/938]\n",
      "loss: 0.046123  [300/938]\n",
      "loss: 0.033487  [400/938]\n",
      "loss: 0.017377  [500/938]\n",
      "loss: 0.063826  [600/938]\n",
      "loss: 0.096470  [700/938]\n",
      "loss: 0.162130  [800/938]\n",
      "loss: 0.072804  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.5%, Avg loss: 0.078494 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.047423  [  0/938]\n",
      "loss: 0.033583  [100/938]\n",
      "loss: 0.025378  [200/938]\n",
      "loss: 0.017275  [300/938]\n",
      "loss: 0.073747  [400/938]\n",
      "loss: 0.095362  [500/938]\n",
      "loss: 0.077604  [600/938]\n",
      "loss: 0.032973  [700/938]\n",
      "loss: 0.048335  [800/938]\n",
      "loss: 0.186247  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.5%, Avg loss: 0.076042 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.019949  [  0/938]\n",
      "loss: 0.088000  [100/938]\n",
      "loss: 0.119155  [200/938]\n",
      "loss: 0.081894  [300/938]\n",
      "loss: 0.086747  [400/938]\n",
      "loss: 0.077904  [500/938]\n",
      "loss: 0.017965  [600/938]\n",
      "loss: 0.115195  [700/938]\n",
      "loss: 0.022435  [800/938]\n",
      "loss: 0.068498  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.7%, Avg loss: 0.073739 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.037308  [  0/938]\n",
      "loss: 0.120877  [100/938]\n",
      "loss: 0.012097  [200/938]\n",
      "loss: 0.083231  [300/938]\n",
      "loss: 0.016537  [400/938]\n",
      "loss: 0.031777  [500/938]\n",
      "loss: 0.090411  [600/938]\n",
      "loss: 0.076756  [700/938]\n",
      "loss: 0.137658  [800/938]\n",
      "loss: 0.016916  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.6%, Avg loss: 0.071537 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.055741  [  0/938]\n",
      "loss: 0.048170  [100/938]\n",
      "loss: 0.026535  [200/938]\n",
      "loss: 0.063622  [300/938]\n",
      "loss: 0.041132  [400/938]\n",
      "loss: 0.021897  [500/938]\n",
      "loss: 0.066274  [600/938]\n",
      "loss: 0.026751  [700/938]\n",
      "loss: 0.048829  [800/938]\n",
      "loss: 0.023129  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.7%, Avg loss: 0.072192 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.103402  [  0/938]\n",
      "loss: 0.063984  [100/938]\n",
      "loss: 0.023389  [200/938]\n",
      "loss: 0.114886  [300/938]\n",
      "loss: 0.037395  [400/938]\n",
      "loss: 0.153253  [500/938]\n",
      "loss: 0.023054  [600/938]\n",
      "loss: 0.074618  [700/938]\n",
      "loss: 0.033453  [800/938]\n",
      "loss: 0.077196  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.7%, Avg loss: 0.070981 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.008689  [  0/938]\n",
      "loss: 0.088547  [100/938]\n",
      "loss: 0.040009  [200/938]\n",
      "loss: 0.082978  [300/938]\n",
      "loss: 0.039561  [400/938]\n",
      "loss: 0.097298  [500/938]\n",
      "loss: 0.022515  [600/938]\n",
      "loss: 0.038187  [700/938]\n",
      "loss: 0.016796  [800/938]\n",
      "loss: 0.009010  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.7%, Avg loss: 0.072263 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.061383  [  0/938]\n",
      "loss: 0.026471  [100/938]\n",
      "loss: 0.043209  [200/938]\n",
      "loss: 0.049230  [300/938]\n",
      "loss: 0.046410  [400/938]\n",
      "loss: 0.055491  [500/938]\n",
      "loss: 0.078212  [600/938]\n",
      "loss: 0.013568  [700/938]\n",
      "loss: 0.055661  [800/938]\n",
      "loss: 0.040177  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.8%, Avg loss: 0.068646 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.080770  [  0/938]\n",
      "loss: 0.092116  [100/938]\n",
      "loss: 0.043769  [200/938]\n",
      "loss: 0.027540  [300/938]\n",
      "loss: 0.056865  [400/938]\n",
      "loss: 0.042521  [500/938]\n",
      "loss: 0.054653  [600/938]\n",
      "loss: 0.027678  [700/938]\n",
      "loss: 0.087239  [800/938]\n",
      "loss: 0.016354  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.068177 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.086869  [  0/938]\n",
      "loss: 0.027387  [100/938]\n",
      "loss: 0.050811  [200/938]\n",
      "loss: 0.007581  [300/938]\n",
      "loss: 0.024716  [400/938]\n",
      "loss: 0.007407  [500/938]\n",
      "loss: 0.019467  [600/938]\n",
      "loss: 0.088389  [700/938]\n",
      "loss: 0.017841  [800/938]\n",
      "loss: 0.055812  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.067494 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.009646  [  0/938]\n",
      "loss: 0.061879  [100/938]\n",
      "loss: 0.117854  [200/938]\n",
      "loss: 0.015397  [300/938]\n",
      "loss: 0.013277  [400/938]\n",
      "loss: 0.052895  [500/938]\n",
      "loss: 0.029015  [600/938]\n",
      "loss: 0.009898  [700/938]\n",
      "loss: 0.011122  [800/938]\n",
      "loss: 0.010331  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.8%, Avg loss: 0.065562 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.035111  [  0/938]\n",
      "loss: 0.039367  [100/938]\n",
      "loss: 0.045764  [200/938]\n",
      "loss: 0.047881  [300/938]\n",
      "loss: 0.020342  [400/938]\n",
      "loss: 0.072828  [500/938]\n",
      "loss: 0.023515  [600/938]\n",
      "loss: 0.072492  [700/938]\n",
      "loss: 0.027776  [800/938]\n",
      "loss: 0.026123  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.064891 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.058007  [  0/938]\n",
      "loss: 0.030355  [100/938]\n",
      "loss: 0.004635  [200/938]\n",
      "loss: 0.031788  [300/938]\n",
      "loss: 0.016166  [400/938]\n",
      "loss: 0.032376  [500/938]\n",
      "loss: 0.051375  [600/938]\n",
      "loss: 0.018581  [700/938]\n",
      "loss: 0.035625  [800/938]\n",
      "loss: 0.005287  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.065905 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.027702  [  0/938]\n",
      "loss: 0.033306  [100/938]\n",
      "loss: 0.048889  [200/938]\n",
      "loss: 0.039300  [300/938]\n",
      "loss: 0.011052  [400/938]\n",
      "loss: 0.004788  [500/938]\n",
      "loss: 0.029987  [600/938]\n",
      "loss: 0.061339  [700/938]\n",
      "loss: 0.008332  [800/938]\n",
      "loss: 0.023738  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.063945 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.009436  [  0/938]\n",
      "loss: 0.065247  [100/938]\n",
      "loss: 0.030991  [200/938]\n",
      "loss: 0.016731  [300/938]\n",
      "loss: 0.010605  [400/938]\n",
      "loss: 0.029055  [500/938]\n",
      "loss: 0.082037  [600/938]\n",
      "loss: 0.131269  [700/938]\n",
      "loss: 0.016111  [800/938]\n",
      "loss: 0.006600  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.062543 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.027450  [  0/938]\n",
      "loss: 0.009427  [100/938]\n",
      "loss: 0.007989  [200/938]\n",
      "loss: 0.076614  [300/938]\n",
      "loss: 0.007857  [400/938]\n",
      "loss: 0.060025  [500/938]\n",
      "loss: 0.026011  [600/938]\n",
      "loss: 0.016501  [700/938]\n",
      "loss: 0.006919  [800/938]\n",
      "loss: 0.020734  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.061875 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.007077  [  0/938]\n",
      "loss: 0.006441  [100/938]\n",
      "loss: 0.006918  [200/938]\n",
      "loss: 0.140772  [300/938]\n",
      "loss: 0.004980  [400/938]\n",
      "loss: 0.021391  [500/938]\n",
      "loss: 0.074048  [600/938]\n",
      "loss: 0.012407  [700/938]\n",
      "loss: 0.049719  [800/938]\n",
      "loss: 0.036428  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.061758 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.011394  [  0/938]\n",
      "loss: 0.036683  [100/938]\n",
      "loss: 0.027921  [200/938]\n",
      "loss: 0.028139  [300/938]\n",
      "loss: 0.015593  [400/938]\n",
      "loss: 0.016674  [500/938]\n",
      "loss: 0.037807  [600/938]\n",
      "loss: 0.003529  [700/938]\n",
      "loss: 0.009577  [800/938]\n",
      "loss: 0.046270  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.061483 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.005671  [  0/938]\n",
      "loss: 0.031611  [100/938]\n",
      "loss: 0.065525  [200/938]\n",
      "loss: 0.011045  [300/938]\n",
      "loss: 0.036532  [400/938]\n",
      "loss: 0.074100  [500/938]\n",
      "loss: 0.013369  [600/938]\n",
      "loss: 0.004194  [700/938]\n",
      "loss: 0.015596  [800/938]\n",
      "loss: 0.028598  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.061071 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.015334  [  0/938]\n",
      "loss: 0.028628  [100/938]\n",
      "loss: 0.019005  [200/938]\n",
      "loss: 0.010773  [300/938]\n",
      "loss: 0.013507  [400/938]\n",
      "loss: 0.059458  [500/938]\n",
      "loss: 0.014704  [600/938]\n",
      "loss: 0.029052  [700/938]\n",
      "loss: 0.021992  [800/938]\n",
      "loss: 0.022243  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.061540 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.013600  [  0/938]\n",
      "loss: 0.001908  [100/938]\n",
      "loss: 0.025279  [200/938]\n",
      "loss: 0.009800  [300/938]\n",
      "loss: 0.008979  [400/938]\n",
      "loss: 0.009123  [500/938]\n",
      "loss: 0.027885  [600/938]\n",
      "loss: 0.009677  [700/938]\n",
      "loss: 0.007729  [800/938]\n",
      "loss: 0.003837  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.060978 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.013132  [  0/938]\n",
      "loss: 0.009435  [100/938]\n",
      "loss: 0.033884  [200/938]\n",
      "loss: 0.096715  [300/938]\n",
      "loss: 0.064003  [400/938]\n",
      "loss: 0.059884  [500/938]\n",
      "loss: 0.017582  [600/938]\n",
      "loss: 0.011280  [700/938]\n",
      "loss: 0.011304  [800/938]\n",
      "loss: 0.014134  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.061716 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.024752  [  0/938]\n",
      "loss: 0.027329  [100/938]\n",
      "loss: 0.010226  [200/938]\n",
      "loss: 0.008025  [300/938]\n",
      "loss: 0.031034  [400/938]\n",
      "loss: 0.014873  [500/938]\n",
      "loss: 0.007442  [600/938]\n",
      "loss: 0.028734  [700/938]\n",
      "loss: 0.020703  [800/938]\n",
      "loss: 0.021180  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.061346 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.041851  [  0/938]\n",
      "loss: 0.020790  [100/938]\n",
      "loss: 0.004807  [200/938]\n",
      "loss: 0.029438  [300/938]\n",
      "loss: 0.015674  [400/938]\n",
      "loss: 0.009178  [500/938]\n",
      "loss: 0.012318  [600/938]\n",
      "loss: 0.009603  [700/938]\n",
      "loss: 0.022601  [800/938]\n",
      "loss: 0.028402  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.060952 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.009749  [  0/938]\n",
      "loss: 0.015426  [100/938]\n",
      "loss: 0.009358  [200/938]\n",
      "loss: 0.027647  [300/938]\n",
      "loss: 0.010077  [400/938]\n",
      "loss: 0.022621  [500/938]\n",
      "loss: 0.002329  [600/938]\n",
      "loss: 0.024402  [700/938]\n",
      "loss: 0.020746  [800/938]\n",
      "loss: 0.016461  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.060867 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.008769  [  0/938]\n",
      "loss: 0.003924  [100/938]\n",
      "loss: 0.036806  [200/938]\n",
      "loss: 0.015596  [300/938]\n",
      "loss: 0.011674  [400/938]\n",
      "loss: 0.007681  [500/938]\n",
      "loss: 0.053380  [600/938]\n",
      "loss: 0.027578  [700/938]\n",
      "loss: 0.051738  [800/938]\n",
      "loss: 0.005955  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.060211 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.008524  [  0/938]\n",
      "loss: 0.021686  [100/938]\n",
      "loss: 0.010485  [200/938]\n",
      "loss: 0.007604  [300/938]\n",
      "loss: 0.013142  [400/938]\n",
      "loss: 0.021692  [500/938]\n",
      "loss: 0.029400  [600/938]\n",
      "loss: 0.000722  [700/938]\n",
      "loss: 0.010938  [800/938]\n",
      "loss: 0.015405  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.060894 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.085661  [  0/938]\n",
      "loss: 0.010006  [100/938]\n",
      "loss: 0.005446  [200/938]\n",
      "loss: 0.002358  [300/938]\n",
      "loss: 0.068657  [400/938]\n",
      "loss: 0.022627  [500/938]\n",
      "loss: 0.003068  [600/938]\n",
      "loss: 0.006973  [700/938]\n",
      "loss: 0.003838  [800/938]\n",
      "loss: 0.033428  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.059834 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.006588  [  0/938]\n",
      "loss: 0.013181  [100/938]\n",
      "loss: 0.004020  [200/938]\n",
      "loss: 0.007075  [300/938]\n",
      "loss: 0.026743  [400/938]\n",
      "loss: 0.033834  [500/938]\n",
      "loss: 0.039972  [600/938]\n",
      "loss: 0.048242  [700/938]\n",
      "loss: 0.038924  [800/938]\n",
      "loss: 0.007246  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.059818 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.013203  [  0/938]\n",
      "loss: 0.031275  [100/938]\n",
      "loss: 0.009442  [200/938]\n",
      "loss: 0.011839  [300/938]\n",
      "loss: 0.005608  [400/938]\n",
      "loss: 0.003300  [500/938]\n",
      "loss: 0.008637  [600/938]\n",
      "loss: 0.028704  [700/938]\n",
      "loss: 0.015011  [800/938]\n",
      "loss: 0.032935  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.059464 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.003501  [  0/938]\n",
      "loss: 0.006850  [100/938]\n",
      "loss: 0.010490  [200/938]\n",
      "loss: 0.006687  [300/938]\n",
      "loss: 0.042609  [400/938]\n",
      "loss: 0.024131  [500/938]\n",
      "loss: 0.031666  [600/938]\n",
      "loss: 0.007737  [700/938]\n",
      "loss: 0.111903  [800/938]\n",
      "loss: 0.008097  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.060394 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.010749  [  0/938]\n",
      "loss: 0.007459  [100/938]\n",
      "loss: 0.030617  [200/938]\n",
      "loss: 0.030307  [300/938]\n",
      "loss: 0.024427  [400/938]\n",
      "loss: 0.006893  [500/938]\n",
      "loss: 0.003952  [600/938]\n",
      "loss: 0.017656  [700/938]\n",
      "loss: 0.012427  [800/938]\n",
      "loss: 0.036594  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.060475 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.006294  [  0/938]\n",
      "loss: 0.008445  [100/938]\n",
      "loss: 0.006573  [200/938]\n",
      "loss: 0.006454  [300/938]\n",
      "loss: 0.049092  [400/938]\n",
      "loss: 0.009249  [500/938]\n",
      "loss: 0.029191  [600/938]\n",
      "loss: 0.004104  [700/938]\n",
      "loss: 0.007737  [800/938]\n",
      "loss: 0.012129  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.059569 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.013401  [  0/938]\n",
      "loss: 0.006163  [100/938]\n",
      "loss: 0.002571  [200/938]\n",
      "loss: 0.067315  [300/938]\n",
      "loss: 0.008920  [400/938]\n",
      "loss: 0.004948  [500/938]\n",
      "loss: 0.048407  [600/938]\n",
      "loss: 0.011791  [700/938]\n",
      "loss: 0.006253  [800/938]\n",
      "loss: 0.011815  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.059602 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.002187  [  0/938]\n",
      "loss: 0.021921  [100/938]\n",
      "loss: 0.004559  [200/938]\n",
      "loss: 0.080197  [300/938]\n",
      "loss: 0.007794  [400/938]\n",
      "loss: 0.022696  [500/938]\n",
      "loss: 0.005365  [600/938]\n",
      "loss: 0.012374  [700/938]\n",
      "loss: 0.032237  [800/938]\n",
      "loss: 0.002704  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.058699 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.035723  [  0/938]\n",
      "loss: 0.019249  [100/938]\n",
      "loss: 0.041614  [200/938]\n",
      "loss: 0.008593  [300/938]\n",
      "loss: 0.034334  [400/938]\n",
      "loss: 0.009496  [500/938]\n",
      "loss: 0.009721  [600/938]\n",
      "loss: 0.007587  [700/938]\n",
      "loss: 0.011291  [800/938]\n",
      "loss: 0.006909  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.060144 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.018464  [  0/938]\n",
      "loss: 0.003980  [100/938]\n",
      "loss: 0.007106  [200/938]\n",
      "loss: 0.002707  [300/938]\n",
      "loss: 0.008878  [400/938]\n",
      "loss: 0.003352  [500/938]\n",
      "loss: 0.013719  [600/938]\n",
      "loss: 0.013909  [700/938]\n",
      "loss: 0.009044  [800/938]\n",
      "loss: 0.018748  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.059120 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.001154  [  0/938]\n",
      "loss: 0.015254  [100/938]\n",
      "loss: 0.006765  [200/938]\n",
      "loss: 0.017117  [300/938]\n",
      "loss: 0.014975  [400/938]\n",
      "loss: 0.008657  [500/938]\n",
      "loss: 0.010869  [600/938]\n",
      "loss: 0.009505  [700/938]\n",
      "loss: 0.017463  [800/938]\n",
      "loss: 0.003275  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.061265 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.021027  [  0/938]\n",
      "loss: 0.008683  [100/938]\n",
      "loss: 0.006740  [200/938]\n",
      "loss: 0.015248  [300/938]\n",
      "loss: 0.004516  [400/938]\n",
      "loss: 0.033029  [500/938]\n",
      "loss: 0.004166  [600/938]\n",
      "loss: 0.018710  [700/938]\n",
      "loss: 0.005515  [800/938]\n",
      "loss: 0.006488  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.059846 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.007067  [  0/938]\n",
      "loss: 0.017274  [100/938]\n",
      "loss: 0.010231  [200/938]\n",
      "loss: 0.003610  [300/938]\n",
      "loss: 0.027878  [400/938]\n",
      "loss: 0.003515  [500/938]\n",
      "loss: 0.000910  [600/938]\n",
      "loss: 0.002220  [700/938]\n",
      "loss: 0.017438  [800/938]\n",
      "loss: 0.002051  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.060249 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.009105  [  0/938]\n",
      "loss: 0.004027  [100/938]\n",
      "loss: 0.001612  [200/938]\n",
      "loss: 0.003657  [300/938]\n",
      "loss: 0.020996  [400/938]\n",
      "loss: 0.012077  [500/938]\n",
      "loss: 0.003995  [600/938]\n",
      "loss: 0.013513  [700/938]\n",
      "loss: 0.004382  [800/938]\n",
      "loss: 0.025793  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.059656 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.014062  [  0/938]\n",
      "loss: 0.016086  [100/938]\n",
      "loss: 0.011928  [200/938]\n",
      "loss: 0.005612  [300/938]\n",
      "loss: 0.008203  [400/938]\n",
      "loss: 0.043705  [500/938]\n",
      "loss: 0.003590  [600/938]\n",
      "loss: 0.011066  [700/938]\n",
      "loss: 0.003979  [800/938]\n",
      "loss: 0.014571  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.060066 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.002689  [  0/938]\n",
      "loss: 0.008197  [100/938]\n",
      "loss: 0.004614  [200/938]\n",
      "loss: 0.004780  [300/938]\n",
      "loss: 0.010876  [400/938]\n",
      "loss: 0.013923  [500/938]\n",
      "loss: 0.006529  [600/938]\n",
      "loss: 0.007725  [700/938]\n",
      "loss: 0.011085  [800/938]\n",
      "loss: 0.021832  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.060416 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.000919  [  0/938]\n",
      "loss: 0.020570  [100/938]\n",
      "loss: 0.001577  [200/938]\n",
      "loss: 0.006256  [300/938]\n",
      "loss: 0.017292  [400/938]\n",
      "loss: 0.007732  [500/938]\n",
      "loss: 0.000815  [600/938]\n",
      "loss: 0.003756  [700/938]\n",
      "loss: 0.004258  [800/938]\n",
      "loss: 0.004953  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.061873 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.015710  [  0/938]\n",
      "loss: 0.001753  [100/938]\n",
      "loss: 0.004326  [200/938]\n",
      "loss: 0.003338  [300/938]\n",
      "loss: 0.004446  [400/938]\n",
      "loss: 0.001821  [500/938]\n",
      "loss: 0.025082  [600/938]\n",
      "loss: 0.002541  [700/938]\n",
      "loss: 0.018520  [800/938]\n",
      "loss: 0.003739  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.061756 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.004501  [  0/938]\n",
      "loss: 0.001609  [100/938]\n",
      "loss: 0.013474  [200/938]\n",
      "loss: 0.035949  [300/938]\n",
      "loss: 0.008758  [400/938]\n",
      "loss: 0.007129  [500/938]\n",
      "loss: 0.005741  [600/938]\n",
      "loss: 0.005506  [700/938]\n",
      "loss: 0.005039  [800/938]\n",
      "loss: 0.013839  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.060453 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.002238  [  0/938]\n",
      "loss: 0.010846  [100/938]\n",
      "loss: 0.012434  [200/938]\n",
      "loss: 0.013991  [300/938]\n",
      "loss: 0.006733  [400/938]\n",
      "loss: 0.011933  [500/938]\n",
      "loss: 0.008481  [600/938]\n",
      "loss: 0.007374  [700/938]\n",
      "loss: 0.015567  [800/938]\n",
      "loss: 0.002416  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.061098 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.002806  [  0/938]\n",
      "loss: 0.001791  [100/938]\n",
      "loss: 0.001046  [200/938]\n",
      "loss: 0.011613  [300/938]\n",
      "loss: 0.007066  [400/938]\n",
      "loss: 0.009228  [500/938]\n",
      "loss: 0.001389  [600/938]\n",
      "loss: 0.009634  [700/938]\n",
      "loss: 0.015933  [800/938]\n",
      "loss: 0.002701  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.060599 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.000997  [  0/938]\n",
      "loss: 0.004677  [100/938]\n",
      "loss: 0.009448  [200/938]\n",
      "loss: 0.004288  [300/938]\n",
      "loss: 0.003352  [400/938]\n",
      "loss: 0.007233  [500/938]\n",
      "loss: 0.014071  [600/938]\n",
      "loss: 0.019670  [700/938]\n",
      "loss: 0.006682  [800/938]\n",
      "loss: 0.002073  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.061864 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.007022  [  0/938]\n",
      "loss: 0.003888  [100/938]\n",
      "loss: 0.009542  [200/938]\n",
      "loss: 0.004193  [300/938]\n",
      "loss: 0.002194  [400/938]\n",
      "loss: 0.005433  [500/938]\n",
      "loss: 0.028291  [600/938]\n",
      "loss: 0.003383  [700/938]\n",
      "loss: 0.003793  [800/938]\n",
      "loss: 0.002751  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.060837 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.004712  [  0/938]\n",
      "loss: 0.006008  [100/938]\n",
      "loss: 0.013113  [200/938]\n",
      "loss: 0.017306  [300/938]\n",
      "loss: 0.009594  [400/938]\n",
      "loss: 0.012501  [500/938]\n",
      "loss: 0.002410  [600/938]\n",
      "loss: 0.001228  [700/938]\n",
      "loss: 0.004224  [800/938]\n",
      "loss: 0.003384  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.060248 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.001394  [  0/938]\n",
      "loss: 0.007580  [100/938]\n",
      "loss: 0.006692  [200/938]\n",
      "loss: 0.005326  [300/938]\n",
      "loss: 0.011957  [400/938]\n",
      "loss: 0.009727  [500/938]\n",
      "loss: 0.002185  [600/938]\n",
      "loss: 0.001938  [700/938]\n",
      "loss: 0.002514  [800/938]\n",
      "loss: 0.003727  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.060509 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.003134  [  0/938]\n",
      "loss: 0.002496  [100/938]\n",
      "loss: 0.015840  [200/938]\n",
      "loss: 0.002965  [300/938]\n",
      "loss: 0.010524  [400/938]\n",
      "loss: 0.004645  [500/938]\n",
      "loss: 0.002387  [600/938]\n",
      "loss: 0.025841  [700/938]\n",
      "loss: 0.001371  [800/938]\n",
      "loss: 0.004338  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.061898 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.003058  [  0/938]\n",
      "loss: 0.003007  [100/938]\n",
      "loss: 0.008359  [200/938]\n",
      "loss: 0.003115  [300/938]\n",
      "loss: 0.027562  [400/938]\n",
      "loss: 0.009631  [500/938]\n",
      "loss: 0.008284  [600/938]\n",
      "loss: 0.011462  [700/938]\n",
      "loss: 0.009378  [800/938]\n",
      "loss: 0.008017  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.062422 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.010619  [  0/938]\n",
      "loss: 0.020129  [100/938]\n",
      "loss: 0.003007  [200/938]\n",
      "loss: 0.002771  [300/938]\n",
      "loss: 0.035263  [400/938]\n",
      "loss: 0.002251  [500/938]\n",
      "loss: 0.004505  [600/938]\n",
      "loss: 0.004907  [700/938]\n",
      "loss: 0.018529  [800/938]\n",
      "loss: 0.007931  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.061675 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.003561  [  0/938]\n",
      "loss: 0.004333  [100/938]\n",
      "loss: 0.018256  [200/938]\n",
      "loss: 0.002670  [300/938]\n",
      "loss: 0.003563  [400/938]\n",
      "loss: 0.003838  [500/938]\n",
      "loss: 0.002287  [600/938]\n",
      "loss: 0.000708  [700/938]\n",
      "loss: 0.010975  [800/938]\n",
      "loss: 0.004106  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.063764 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.008561  [  0/938]\n",
      "loss: 0.020301  [100/938]\n",
      "loss: 0.003586  [200/938]\n",
      "loss: 0.022933  [300/938]\n",
      "loss: 0.002008  [400/938]\n",
      "loss: 0.005358  [500/938]\n",
      "loss: 0.009389  [600/938]\n",
      "loss: 0.004836  [700/938]\n",
      "loss: 0.002337  [800/938]\n",
      "loss: 0.001326  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.062333 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.004161  [  0/938]\n",
      "loss: 0.003163  [100/938]\n",
      "loss: 0.007839  [200/938]\n",
      "loss: 0.002657  [300/938]\n",
      "loss: 0.002628  [400/938]\n",
      "loss: 0.000527  [500/938]\n",
      "loss: 0.003106  [600/938]\n",
      "loss: 0.002063  [700/938]\n",
      "loss: 0.011886  [800/938]\n",
      "loss: 0.006563  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.061260 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.007799  [  0/938]\n",
      "loss: 0.001245  [100/938]\n",
      "loss: 0.003804  [200/938]\n",
      "loss: 0.009596  [300/938]\n",
      "loss: 0.007209  [400/938]\n",
      "loss: 0.003099  [500/938]\n",
      "loss: 0.031332  [600/938]\n",
      "loss: 0.002098  [700/938]\n",
      "loss: 0.004664  [800/938]\n",
      "loss: 0.004204  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.061476 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.006540  [  0/938]\n",
      "loss: 0.000817  [100/938]\n",
      "loss: 0.001822  [200/938]\n",
      "loss: 0.010892  [300/938]\n",
      "loss: 0.000849  [400/938]\n",
      "loss: 0.013486  [500/938]\n",
      "loss: 0.007849  [600/938]\n",
      "loss: 0.002263  [700/938]\n",
      "loss: 0.019034  [800/938]\n",
      "loss: 0.004114  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.061642 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.001374  [  0/938]\n",
      "loss: 0.005066  [100/938]\n",
      "loss: 0.005477  [200/938]\n",
      "loss: 0.004836  [300/938]\n",
      "loss: 0.001761  [400/938]\n",
      "loss: 0.012855  [500/938]\n",
      "loss: 0.005978  [600/938]\n",
      "loss: 0.003576  [700/938]\n",
      "loss: 0.000778  [800/938]\n",
      "loss: 0.001346  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.061937 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.004214  [  0/938]\n",
      "loss: 0.005063  [100/938]\n",
      "loss: 0.002914  [200/938]\n",
      "loss: 0.001350  [300/938]\n",
      "loss: 0.001690  [400/938]\n",
      "loss: 0.002026  [500/938]\n",
      "loss: 0.003339  [600/938]\n",
      "loss: 0.001229  [700/938]\n",
      "loss: 0.005489  [800/938]\n",
      "loss: 0.012153  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.062793 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.001470  [  0/938]\n",
      "loss: 0.005171  [100/938]\n",
      "loss: 0.003800  [200/938]\n",
      "loss: 0.006335  [300/938]\n",
      "loss: 0.002435  [400/938]\n",
      "loss: 0.000694  [500/938]\n",
      "loss: 0.021198  [600/938]\n",
      "loss: 0.000918  [700/938]\n",
      "loss: 0.001247  [800/938]\n",
      "loss: 0.000780  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.062250 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.018983  [  0/938]\n",
      "loss: 0.004655  [100/938]\n",
      "loss: 0.000805  [200/938]\n",
      "loss: 0.001791  [300/938]\n",
      "loss: 0.004497  [400/938]\n",
      "loss: 0.000454  [500/938]\n",
      "loss: 0.002959  [600/938]\n",
      "loss: 0.000738  [700/938]\n",
      "loss: 0.001699  [800/938]\n",
      "loss: 0.002637  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.062896 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.000574  [  0/938]\n",
      "loss: 0.000867  [100/938]\n",
      "loss: 0.014918  [200/938]\n",
      "loss: 0.006727  [300/938]\n",
      "loss: 0.000944  [400/938]\n",
      "loss: 0.005676  [500/938]\n",
      "loss: 0.001212  [600/938]\n",
      "loss: 0.008371  [700/938]\n",
      "loss: 0.004419  [800/938]\n",
      "loss: 0.002378  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.063716 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.004587  [  0/938]\n",
      "loss: 0.002684  [100/938]\n",
      "loss: 0.002265  [200/938]\n",
      "loss: 0.001772  [300/938]\n",
      "loss: 0.011809  [400/938]\n",
      "loss: 0.000458  [500/938]\n",
      "loss: 0.000570  [600/938]\n",
      "loss: 0.000348  [700/938]\n",
      "loss: 0.010876  [800/938]\n",
      "loss: 0.003852  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.062126 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.005156  [  0/938]\n",
      "loss: 0.000664  [100/938]\n",
      "loss: 0.004408  [200/938]\n",
      "loss: 0.006701  [300/938]\n",
      "loss: 0.001464  [400/938]\n",
      "loss: 0.008193  [500/938]\n",
      "loss: 0.011331  [600/938]\n",
      "loss: 0.001489  [700/938]\n",
      "loss: 0.000602  [800/938]\n",
      "loss: 0.005144  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.063160 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.004449  [  0/938]\n",
      "loss: 0.009317  [100/938]\n",
      "loss: 0.005380  [200/938]\n",
      "loss: 0.001601  [300/938]\n",
      "loss: 0.031526  [400/938]\n",
      "loss: 0.011543  [500/938]\n",
      "loss: 0.001599  [600/938]\n",
      "loss: 0.015851  [700/938]\n",
      "loss: 0.006891  [800/938]\n",
      "loss: 0.009504  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.063185 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.007866  [  0/938]\n",
      "loss: 0.003162  [100/938]\n",
      "loss: 0.003280  [200/938]\n",
      "loss: 0.011131  [300/938]\n",
      "loss: 0.004658  [400/938]\n",
      "loss: 0.008177  [500/938]\n",
      "loss: 0.001085  [600/938]\n",
      "loss: 0.022247  [700/938]\n",
      "loss: 0.002698  [800/938]\n",
      "loss: 0.000488  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.062999 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.004874  [  0/938]\n",
      "loss: 0.002889  [100/938]\n",
      "loss: 0.002478  [200/938]\n",
      "loss: 0.002415  [300/938]\n",
      "loss: 0.003035  [400/938]\n",
      "loss: 0.000431  [500/938]\n",
      "loss: 0.034298  [600/938]\n",
      "loss: 0.019706  [700/938]\n",
      "loss: 0.002369  [800/938]\n",
      "loss: 0.001080  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.063099 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.002125  [  0/938]\n",
      "loss: 0.003277  [100/938]\n",
      "loss: 0.000975  [200/938]\n",
      "loss: 0.002560  [300/938]\n",
      "loss: 0.006158  [400/938]\n",
      "loss: 0.009291  [500/938]\n",
      "loss: 0.002274  [600/938]\n",
      "loss: 0.005044  [700/938]\n",
      "loss: 0.002206  [800/938]\n",
      "loss: 0.002253  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.063627 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.007107  [  0/938]\n",
      "loss: 0.003235  [100/938]\n",
      "loss: 0.017083  [200/938]\n",
      "loss: 0.003630  [300/938]\n",
      "loss: 0.001026  [400/938]\n",
      "loss: 0.008432  [500/938]\n",
      "loss: 0.003559  [600/938]\n",
      "loss: 0.001957  [700/938]\n",
      "loss: 0.004359  [800/938]\n",
      "loss: 0.003521  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.063749 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.004900  [  0/938]\n",
      "loss: 0.001234  [100/938]\n",
      "loss: 0.002532  [200/938]\n",
      "loss: 0.002277  [300/938]\n",
      "loss: 0.002457  [400/938]\n",
      "loss: 0.002084  [500/938]\n",
      "loss: 0.007381  [600/938]\n",
      "loss: 0.007402  [700/938]\n",
      "loss: 0.004404  [800/938]\n",
      "loss: 0.003098  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065717 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.001436  [  0/938]\n",
      "loss: 0.024653  [100/938]\n",
      "loss: 0.003357  [200/938]\n",
      "loss: 0.002663  [300/938]\n",
      "loss: 0.005281  [400/938]\n",
      "loss: 0.003257  [500/938]\n",
      "loss: 0.005071  [600/938]\n",
      "loss: 0.000784  [700/938]\n",
      "loss: 0.001357  [800/938]\n",
      "loss: 0.001870  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.064063 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.001324  [  0/938]\n",
      "loss: 0.001121  [100/938]\n",
      "loss: 0.004887  [200/938]\n",
      "loss: 0.001954  [300/938]\n",
      "loss: 0.004882  [400/938]\n",
      "loss: 0.001649  [500/938]\n",
      "loss: 0.003390  [600/938]\n",
      "loss: 0.001547  [700/938]\n",
      "loss: 0.005245  [800/938]\n",
      "loss: 0.001174  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.064305 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.001544  [  0/938]\n",
      "loss: 0.000556  [100/938]\n",
      "loss: 0.000422  [200/938]\n",
      "loss: 0.003963  [300/938]\n",
      "loss: 0.000412  [400/938]\n",
      "loss: 0.007137  [500/938]\n",
      "loss: 0.002728  [600/938]\n",
      "loss: 0.001942  [700/938]\n",
      "loss: 0.014869  [800/938]\n",
      "loss: 0.010774  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065194 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.005885  [  0/938]\n",
      "loss: 0.009858  [100/938]\n",
      "loss: 0.001285  [200/938]\n",
      "loss: 0.005218  [300/938]\n",
      "loss: 0.004317  [400/938]\n",
      "loss: 0.002029  [500/938]\n",
      "loss: 0.000642  [600/938]\n",
      "loss: 0.004088  [700/938]\n",
      "loss: 0.001840  [800/938]\n",
      "loss: 0.014649  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.064141 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.010227  [  0/938]\n",
      "loss: 0.000383  [100/938]\n",
      "loss: 0.002290  [200/938]\n",
      "loss: 0.004029  [300/938]\n",
      "loss: 0.002931  [400/938]\n",
      "loss: 0.006991  [500/938]\n",
      "loss: 0.000398  [600/938]\n",
      "loss: 0.001668  [700/938]\n",
      "loss: 0.003806  [800/938]\n",
      "loss: 0.004494  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.063629 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.004581  [  0/938]\n",
      "loss: 0.001050  [100/938]\n",
      "loss: 0.004893  [200/938]\n",
      "loss: 0.004214  [300/938]\n",
      "loss: 0.007303  [400/938]\n",
      "loss: 0.000774  [500/938]\n",
      "loss: 0.003568  [600/938]\n",
      "loss: 0.000692  [700/938]\n",
      "loss: 0.002326  [800/938]\n",
      "loss: 0.001950  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.064527 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.001017  [  0/938]\n",
      "loss: 0.003992  [100/938]\n",
      "loss: 0.002531  [200/938]\n",
      "loss: 0.004637  [300/938]\n",
      "loss: 0.000769  [400/938]\n",
      "loss: 0.010822  [500/938]\n",
      "loss: 0.001631  [600/938]\n",
      "loss: 0.002198  [700/938]\n",
      "loss: 0.001968  [800/938]\n",
      "loss: 0.000821  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065558 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.007627  [  0/938]\n",
      "loss: 0.001132  [100/938]\n",
      "loss: 0.000197  [200/938]\n",
      "loss: 0.005992  [300/938]\n",
      "loss: 0.001812  [400/938]\n",
      "loss: 0.004981  [500/938]\n",
      "loss: 0.003288  [600/938]\n",
      "loss: 0.000897  [700/938]\n",
      "loss: 0.001381  [800/938]\n",
      "loss: 0.000917  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.065121 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.001270  [  0/938]\n",
      "loss: 0.000782  [100/938]\n",
      "loss: 0.001563  [200/938]\n",
      "loss: 0.003475  [300/938]\n",
      "loss: 0.010827  [400/938]\n",
      "loss: 0.000841  [500/938]\n",
      "loss: 0.001570  [600/938]\n",
      "loss: 0.003159  [700/938]\n",
      "loss: 0.000929  [800/938]\n",
      "loss: 0.002367  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065109 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.000957  [  0/938]\n",
      "loss: 0.002225  [100/938]\n",
      "loss: 0.001972  [200/938]\n",
      "loss: 0.000578  [300/938]\n",
      "loss: 0.002475  [400/938]\n",
      "loss: 0.003128  [500/938]\n",
      "loss: 0.006989  [600/938]\n",
      "loss: 0.005368  [700/938]\n",
      "loss: 0.005659  [800/938]\n",
      "loss: 0.000761  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.067358 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.001551  [  0/938]\n",
      "loss: 0.000702  [100/938]\n",
      "loss: 0.002089  [200/938]\n",
      "loss: 0.002068  [300/938]\n",
      "loss: 0.000794  [400/938]\n",
      "loss: 0.005335  [500/938]\n",
      "loss: 0.001715  [600/938]\n",
      "loss: 0.001467  [700/938]\n",
      "loss: 0.000816  [800/938]\n",
      "loss: 0.001757  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065726 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.006530  [  0/938]\n",
      "loss: 0.001853  [100/938]\n",
      "loss: 0.001655  [200/938]\n",
      "loss: 0.003538  [300/938]\n",
      "loss: 0.002929  [400/938]\n",
      "loss: 0.002704  [500/938]\n",
      "loss: 0.000283  [600/938]\n",
      "loss: 0.003646  [700/938]\n",
      "loss: 0.003393  [800/938]\n",
      "loss: 0.001010  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065990 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.001205  [  0/938]\n",
      "loss: 0.000681  [100/938]\n",
      "loss: 0.002230  [200/938]\n",
      "loss: 0.000469  [300/938]\n",
      "loss: 0.002494  [400/938]\n",
      "loss: 0.003769  [500/938]\n",
      "loss: 0.007985  [600/938]\n",
      "loss: 0.001166  [700/938]\n",
      "loss: 0.001420  [800/938]\n",
      "loss: 0.004027  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.066833 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.002209  [  0/938]\n",
      "loss: 0.001604  [100/938]\n",
      "loss: 0.002813  [200/938]\n",
      "loss: 0.003900  [300/938]\n",
      "loss: 0.011316  [400/938]\n",
      "loss: 0.001703  [500/938]\n",
      "loss: 0.001388  [600/938]\n",
      "loss: 0.007080  [700/938]\n",
      "loss: 0.028508  [800/938]\n",
      "loss: 0.000232  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.064182 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.011411  [  0/938]\n",
      "loss: 0.004908  [100/938]\n",
      "loss: 0.000895  [200/938]\n",
      "loss: 0.001794  [300/938]\n",
      "loss: 0.006970  [400/938]\n",
      "loss: 0.002278  [500/938]\n",
      "loss: 0.000570  [600/938]\n",
      "loss: 0.003100  [700/938]\n",
      "loss: 0.002118  [800/938]\n",
      "loss: 0.001620  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065402 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.003054  [  0/938]\n",
      "loss: 0.002794  [100/938]\n",
      "loss: 0.002074  [200/938]\n",
      "loss: 0.004980  [300/938]\n",
      "loss: 0.001673  [400/938]\n",
      "loss: 0.004713  [500/938]\n",
      "loss: 0.002683  [600/938]\n",
      "loss: 0.002752  [700/938]\n",
      "loss: 0.001348  [800/938]\n",
      "loss: 0.003430  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.068443 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.000699  [  0/938]\n",
      "loss: 0.002345  [100/938]\n",
      "loss: 0.000604  [200/938]\n",
      "loss: 0.000870  [300/938]\n",
      "loss: 0.000850  [400/938]\n",
      "loss: 0.004508  [500/938]\n",
      "loss: 0.001307  [600/938]\n",
      "loss: 0.003135  [700/938]\n",
      "loss: 0.000778  [800/938]\n",
      "loss: 0.002239  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.064681 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.000887  [  0/938]\n",
      "loss: 0.000310  [100/938]\n",
      "loss: 0.003383  [200/938]\n",
      "loss: 0.001421  [300/938]\n",
      "loss: 0.000630  [400/938]\n",
      "loss: 0.004473  [500/938]\n",
      "loss: 0.000825  [600/938]\n",
      "loss: 0.000410  [700/938]\n",
      "loss: 0.002134  [800/938]\n",
      "loss: 0.001475  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.064959 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.001495  [  0/938]\n",
      "loss: 0.000765  [100/938]\n",
      "loss: 0.000528  [200/938]\n",
      "loss: 0.003834  [300/938]\n",
      "loss: 0.001100  [400/938]\n",
      "loss: 0.000482  [500/938]\n",
      "loss: 0.003715  [600/938]\n",
      "loss: 0.001432  [700/938]\n",
      "loss: 0.002441  [800/938]\n",
      "loss: 0.006352  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065949 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.001359  [  0/938]\n",
      "loss: 0.001869  [100/938]\n",
      "loss: 0.002001  [200/938]\n",
      "loss: 0.001137  [300/938]\n",
      "loss: 0.002359  [400/938]\n",
      "loss: 0.001085  [500/938]\n",
      "loss: 0.000631  [600/938]\n",
      "loss: 0.002535  [700/938]\n",
      "loss: 0.003936  [800/938]\n",
      "loss: 0.001564  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.067362 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.001383  [  0/938]\n",
      "loss: 0.003886  [100/938]\n",
      "loss: 0.003721  [200/938]\n",
      "loss: 0.000289  [300/938]\n",
      "loss: 0.001262  [400/938]\n",
      "loss: 0.002524  [500/938]\n",
      "loss: 0.001787  [600/938]\n",
      "loss: 0.008661  [700/938]\n",
      "loss: 0.001692  [800/938]\n",
      "loss: 0.000847  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066533 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.000930  [  0/938]\n",
      "loss: 0.001301  [100/938]\n",
      "loss: 0.007835  [200/938]\n",
      "loss: 0.000956  [300/938]\n",
      "loss: 0.003789  [400/938]\n",
      "loss: 0.005133  [500/938]\n",
      "loss: 0.002150  [600/938]\n",
      "loss: 0.001763  [700/938]\n",
      "loss: 0.001422  [800/938]\n",
      "loss: 0.001588  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.067685 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.002600  [  0/938]\n",
      "loss: 0.001309  [100/938]\n",
      "loss: 0.002106  [200/938]\n",
      "loss: 0.003707  [300/938]\n",
      "loss: 0.002465  [400/938]\n",
      "loss: 0.000802  [500/938]\n",
      "loss: 0.000713  [600/938]\n",
      "loss: 0.000879  [700/938]\n",
      "loss: 0.006915  [800/938]\n",
      "loss: 0.005112  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.067183 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.000480  [  0/938]\n",
      "loss: 0.003185  [100/938]\n",
      "loss: 0.013767  [200/938]\n",
      "loss: 0.005064  [300/938]\n",
      "loss: 0.001207  [400/938]\n",
      "loss: 0.004830  [500/938]\n",
      "loss: 0.000569  [600/938]\n",
      "loss: 0.001593  [700/938]\n",
      "loss: 0.000637  [800/938]\n",
      "loss: 0.001678  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066177 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.001064  [  0/938]\n",
      "loss: 0.002711  [100/938]\n",
      "loss: 0.004520  [200/938]\n",
      "loss: 0.001776  [300/938]\n",
      "loss: 0.000458  [400/938]\n",
      "loss: 0.000514  [500/938]\n",
      "loss: 0.000723  [600/938]\n",
      "loss: 0.001171  [700/938]\n",
      "loss: 0.001458  [800/938]\n",
      "loss: 0.009434  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066631 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.001759  [  0/938]\n",
      "loss: 0.003634  [100/938]\n",
      "loss: 0.001088  [200/938]\n",
      "loss: 0.000469  [300/938]\n",
      "loss: 0.002919  [400/938]\n",
      "loss: 0.031166  [500/938]\n",
      "loss: 0.009459  [600/938]\n",
      "loss: 0.005238  [700/938]\n",
      "loss: 0.002209  [800/938]\n",
      "loss: 0.002970  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.067038 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.002489  [  0/938]\n",
      "loss: 0.000648  [100/938]\n",
      "loss: 0.000363  [200/938]\n",
      "loss: 0.001308  [300/938]\n",
      "loss: 0.000424  [400/938]\n",
      "loss: 0.000697  [500/938]\n",
      "loss: 0.004402  [600/938]\n",
      "loss: 0.005977  [700/938]\n",
      "loss: 0.002078  [800/938]\n",
      "loss: 0.002240  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066447 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.004363  [  0/938]\n",
      "loss: 0.000339  [100/938]\n",
      "loss: 0.004667  [200/938]\n",
      "loss: 0.003127  [300/938]\n",
      "loss: 0.003551  [400/938]\n",
      "loss: 0.000593  [500/938]\n",
      "loss: 0.001079  [600/938]\n",
      "loss: 0.002378  [700/938]\n",
      "loss: 0.000404  [800/938]\n",
      "loss: 0.000823  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066796 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.000549  [  0/938]\n",
      "loss: 0.000167  [100/938]\n",
      "loss: 0.001050  [200/938]\n",
      "loss: 0.001615  [300/938]\n",
      "loss: 0.006350  [400/938]\n",
      "loss: 0.004557  [500/938]\n",
      "loss: 0.000221  [600/938]\n",
      "loss: 0.000544  [700/938]\n",
      "loss: 0.002581  [800/938]\n",
      "loss: 0.003558  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065923 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.000486  [  0/938]\n",
      "loss: 0.001298  [100/938]\n",
      "loss: 0.004863  [200/938]\n",
      "loss: 0.001266  [300/938]\n",
      "loss: 0.002400  [400/938]\n",
      "loss: 0.003138  [500/938]\n",
      "loss: 0.001260  [600/938]\n",
      "loss: 0.003098  [700/938]\n",
      "loss: 0.001657  [800/938]\n",
      "loss: 0.000254  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066299 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.001756  [  0/938]\n",
      "loss: 0.002584  [100/938]\n",
      "loss: 0.001036  [200/938]\n",
      "loss: 0.001687  [300/938]\n",
      "loss: 0.003452  [400/938]\n",
      "loss: 0.000537  [500/938]\n",
      "loss: 0.000478  [600/938]\n",
      "loss: 0.002446  [700/938]\n",
      "loss: 0.000298  [800/938]\n",
      "loss: 0.002117  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.065733 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.000563  [  0/938]\n",
      "loss: 0.000720  [100/938]\n",
      "loss: 0.000116  [200/938]\n",
      "loss: 0.001713  [300/938]\n",
      "loss: 0.000857  [400/938]\n",
      "loss: 0.030155  [500/938]\n",
      "loss: 0.000841  [600/938]\n",
      "loss: 0.000768  [700/938]\n",
      "loss: 0.005828  [800/938]\n",
      "loss: 0.000594  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066817 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.001335  [  0/938]\n",
      "loss: 0.000469  [100/938]\n",
      "loss: 0.001050  [200/938]\n",
      "loss: 0.001701  [300/938]\n",
      "loss: 0.000766  [400/938]\n",
      "loss: 0.001644  [500/938]\n",
      "loss: 0.003651  [600/938]\n",
      "loss: 0.001314  [700/938]\n",
      "loss: 0.000677  [800/938]\n",
      "loss: 0.001161  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066553 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.001706  [  0/938]\n",
      "loss: 0.001138  [100/938]\n",
      "loss: 0.000682  [200/938]\n",
      "loss: 0.000830  [300/938]\n",
      "loss: 0.000641  [400/938]\n",
      "loss: 0.001562  [500/938]\n",
      "loss: 0.000787  [600/938]\n",
      "loss: 0.001064  [700/938]\n",
      "loss: 0.001700  [800/938]\n",
      "loss: 0.000862  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066734 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.001178  [  0/938]\n",
      "loss: 0.000577  [100/938]\n",
      "loss: 0.000767  [200/938]\n",
      "loss: 0.000635  [300/938]\n",
      "loss: 0.001126  [400/938]\n",
      "loss: 0.005578  [500/938]\n",
      "loss: 0.000328  [600/938]\n",
      "loss: 0.002315  [700/938]\n",
      "loss: 0.000429  [800/938]\n",
      "loss: 0.001777  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066387 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.003593  [  0/938]\n",
      "loss: 0.003408  [100/938]\n",
      "loss: 0.003796  [200/938]\n",
      "loss: 0.002641  [300/938]\n",
      "loss: 0.000824  [400/938]\n",
      "loss: 0.005321  [500/938]\n",
      "loss: 0.000712  [600/938]\n",
      "loss: 0.008177  [700/938]\n",
      "loss: 0.001603  [800/938]\n",
      "loss: 0.006631  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.068549 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.000275  [  0/938]\n",
      "loss: 0.000535  [100/938]\n",
      "loss: 0.000962  [200/938]\n",
      "loss: 0.004057  [300/938]\n",
      "loss: 0.006569  [400/938]\n",
      "loss: 0.002608  [500/938]\n",
      "loss: 0.003773  [600/938]\n",
      "loss: 0.002018  [700/938]\n",
      "loss: 0.001225  [800/938]\n",
      "loss: 0.000362  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.067729 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.002727  [  0/938]\n",
      "loss: 0.001001  [100/938]\n",
      "loss: 0.000819  [200/938]\n",
      "loss: 0.004578  [300/938]\n",
      "loss: 0.000347  [400/938]\n",
      "loss: 0.010024  [500/938]\n",
      "loss: 0.000233  [600/938]\n",
      "loss: 0.004951  [700/938]\n",
      "loss: 0.001540  [800/938]\n",
      "loss: 0.001715  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.066945 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.001298  [  0/938]\n",
      "loss: 0.002758  [100/938]\n",
      "loss: 0.001126  [200/938]\n",
      "loss: 0.000203  [300/938]\n",
      "loss: 0.001452  [400/938]\n",
      "loss: 0.000803  [500/938]\n",
      "loss: 0.003667  [600/938]\n",
      "loss: 0.001718  [700/938]\n",
      "loss: 0.003441  [800/938]\n",
      "loss: 0.003310  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.068196 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.000141  [  0/938]\n",
      "loss: 0.009269  [100/938]\n",
      "loss: 0.000212  [200/938]\n",
      "loss: 0.004899  [300/938]\n",
      "loss: 0.000249  [400/938]\n",
      "loss: 0.001640  [500/938]\n",
      "loss: 0.001938  [600/938]\n",
      "loss: 0.000568  [700/938]\n",
      "loss: 0.003413  [800/938]\n",
      "loss: 0.000090  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070664 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.009250  [  0/938]\n",
      "loss: 0.001903  [100/938]\n",
      "loss: 0.000435  [200/938]\n",
      "loss: 0.000236  [300/938]\n",
      "loss: 0.000941  [400/938]\n",
      "loss: 0.001019  [500/938]\n",
      "loss: 0.000809  [600/938]\n",
      "loss: 0.000983  [700/938]\n",
      "loss: 0.005321  [800/938]\n",
      "loss: 0.001627  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.067912 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.003506  [  0/938]\n",
      "loss: 0.001731  [100/938]\n",
      "loss: 0.001578  [200/938]\n",
      "loss: 0.009988  [300/938]\n",
      "loss: 0.003120  [400/938]\n",
      "loss: 0.003275  [500/938]\n",
      "loss: 0.002249  [600/938]\n",
      "loss: 0.000128  [700/938]\n",
      "loss: 0.000642  [800/938]\n",
      "loss: 0.002217  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.068528 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.001165  [  0/938]\n",
      "loss: 0.001007  [100/938]\n",
      "loss: 0.001605  [200/938]\n",
      "loss: 0.000846  [300/938]\n",
      "loss: 0.000867  [400/938]\n",
      "loss: 0.001636  [500/938]\n",
      "loss: 0.001916  [600/938]\n",
      "loss: 0.003795  [700/938]\n",
      "loss: 0.000697  [800/938]\n",
      "loss: 0.003501  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.068232 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.003158  [  0/938]\n",
      "loss: 0.000459  [100/938]\n",
      "loss: 0.001247  [200/938]\n",
      "loss: 0.006542  [300/938]\n",
      "loss: 0.000400  [400/938]\n",
      "loss: 0.000875  [500/938]\n",
      "loss: 0.000350  [600/938]\n",
      "loss: 0.000235  [700/938]\n",
      "loss: 0.000314  [800/938]\n",
      "loss: 0.001448  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070235 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.002141  [  0/938]\n",
      "loss: 0.001805  [100/938]\n",
      "loss: 0.002777  [200/938]\n",
      "loss: 0.001182  [300/938]\n",
      "loss: 0.003574  [400/938]\n",
      "loss: 0.002314  [500/938]\n",
      "loss: 0.000345  [600/938]\n",
      "loss: 0.002098  [700/938]\n",
      "loss: 0.000568  [800/938]\n",
      "loss: 0.001177  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069739 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.002169  [  0/938]\n",
      "loss: 0.001605  [100/938]\n",
      "loss: 0.001554  [200/938]\n",
      "loss: 0.004086  [300/938]\n",
      "loss: 0.002524  [400/938]\n",
      "loss: 0.001024  [500/938]\n",
      "loss: 0.001340  [600/938]\n",
      "loss: 0.000304  [700/938]\n",
      "loss: 0.000171  [800/938]\n",
      "loss: 0.000767  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069109 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.000639  [  0/938]\n",
      "loss: 0.003025  [100/938]\n",
      "loss: 0.003177  [200/938]\n",
      "loss: 0.000277  [300/938]\n",
      "loss: 0.000618  [400/938]\n",
      "loss: 0.000321  [500/938]\n",
      "loss: 0.001120  [600/938]\n",
      "loss: 0.002011  [700/938]\n",
      "loss: 0.007642  [800/938]\n",
      "loss: 0.001124  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.067940 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.004277  [  0/938]\n",
      "loss: 0.000246  [100/938]\n",
      "loss: 0.008890  [200/938]\n",
      "loss: 0.002013  [300/938]\n",
      "loss: 0.000395  [400/938]\n",
      "loss: 0.000485  [500/938]\n",
      "loss: 0.001679  [600/938]\n",
      "loss: 0.001043  [700/938]\n",
      "loss: 0.001167  [800/938]\n",
      "loss: 0.000625  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.068183 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.003304  [  0/938]\n",
      "loss: 0.000111  [100/938]\n",
      "loss: 0.008799  [200/938]\n",
      "loss: 0.000923  [300/938]\n",
      "loss: 0.000583  [400/938]\n",
      "loss: 0.001252  [500/938]\n",
      "loss: 0.001005  [600/938]\n",
      "loss: 0.001859  [700/938]\n",
      "loss: 0.001135  [800/938]\n",
      "loss: 0.002752  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.068252 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.000889  [  0/938]\n",
      "loss: 0.003401  [100/938]\n",
      "loss: 0.001042  [200/938]\n",
      "loss: 0.000754  [300/938]\n",
      "loss: 0.001260  [400/938]\n",
      "loss: 0.000758  [500/938]\n",
      "loss: 0.000813  [600/938]\n",
      "loss: 0.001758  [700/938]\n",
      "loss: 0.003858  [800/938]\n",
      "loss: 0.001427  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069518 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.000297  [  0/938]\n",
      "loss: 0.001066  [100/938]\n",
      "loss: 0.003679  [200/938]\n",
      "loss: 0.003282  [300/938]\n",
      "loss: 0.000404  [400/938]\n",
      "loss: 0.005594  [500/938]\n",
      "loss: 0.005183  [600/938]\n",
      "loss: 0.000667  [700/938]\n",
      "loss: 0.001639  [800/938]\n",
      "loss: 0.000444  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.068067 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.000593  [  0/938]\n",
      "loss: 0.000355  [100/938]\n",
      "loss: 0.001332  [200/938]\n",
      "loss: 0.001562  [300/938]\n",
      "loss: 0.000244  [400/938]\n",
      "loss: 0.000640  [500/938]\n",
      "loss: 0.000611  [600/938]\n",
      "loss: 0.002542  [700/938]\n",
      "loss: 0.003639  [800/938]\n",
      "loss: 0.000876  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.067591 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.005162  [  0/938]\n",
      "loss: 0.001488  [100/938]\n",
      "loss: 0.000756  [200/938]\n",
      "loss: 0.000471  [300/938]\n",
      "loss: 0.010193  [400/938]\n",
      "loss: 0.001155  [500/938]\n",
      "loss: 0.001419  [600/938]\n",
      "loss: 0.000575  [700/938]\n",
      "loss: 0.001403  [800/938]\n",
      "loss: 0.000158  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.067805 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.002142  [  0/938]\n",
      "loss: 0.001046  [100/938]\n",
      "loss: 0.001140  [200/938]\n",
      "loss: 0.001072  [300/938]\n",
      "loss: 0.001749  [400/938]\n",
      "loss: 0.000876  [500/938]\n",
      "loss: 0.002646  [600/938]\n",
      "loss: 0.003755  [700/938]\n",
      "loss: 0.002129  [800/938]\n",
      "loss: 0.000226  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.068373 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.000635  [  0/938]\n",
      "loss: 0.001226  [100/938]\n",
      "loss: 0.000760  [200/938]\n",
      "loss: 0.002110  [300/938]\n",
      "loss: 0.005253  [400/938]\n",
      "loss: 0.002251  [500/938]\n",
      "loss: 0.005002  [600/938]\n",
      "loss: 0.000414  [700/938]\n",
      "loss: 0.008363  [800/938]\n",
      "loss: 0.003816  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070869 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.000212  [  0/938]\n",
      "loss: 0.000274  [100/938]\n",
      "loss: 0.005400  [200/938]\n",
      "loss: 0.000590  [300/938]\n",
      "loss: 0.000833  [400/938]\n",
      "loss: 0.000175  [500/938]\n",
      "loss: 0.006211  [600/938]\n",
      "loss: 0.001356  [700/938]\n",
      "loss: 0.000508  [800/938]\n",
      "loss: 0.009482  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069043 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.000333  [  0/938]\n",
      "loss: 0.004799  [100/938]\n",
      "loss: 0.001094  [200/938]\n",
      "loss: 0.002918  [300/938]\n",
      "loss: 0.000460  [400/938]\n",
      "loss: 0.000652  [500/938]\n",
      "loss: 0.001715  [600/938]\n",
      "loss: 0.005561  [700/938]\n",
      "loss: 0.002549  [800/938]\n",
      "loss: 0.002657  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.069682 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.001089  [  0/938]\n",
      "loss: 0.002247  [100/938]\n",
      "loss: 0.002324  [200/938]\n",
      "loss: 0.004374  [300/938]\n",
      "loss: 0.001419  [400/938]\n",
      "loss: 0.000173  [500/938]\n",
      "loss: 0.000324  [600/938]\n",
      "loss: 0.000939  [700/938]\n",
      "loss: 0.000376  [800/938]\n",
      "loss: 0.000936  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069235 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.000445  [  0/938]\n",
      "loss: 0.001908  [100/938]\n",
      "loss: 0.000721  [200/938]\n",
      "loss: 0.001309  [300/938]\n",
      "loss: 0.002030  [400/938]\n",
      "loss: 0.000525  [500/938]\n",
      "loss: 0.005170  [600/938]\n",
      "loss: 0.000766  [700/938]\n",
      "loss: 0.000549  [800/938]\n",
      "loss: 0.000927  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070445 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.006186  [  0/938]\n",
      "loss: 0.003225  [100/938]\n",
      "loss: 0.002908  [200/938]\n",
      "loss: 0.003021  [300/938]\n",
      "loss: 0.002402  [400/938]\n",
      "loss: 0.000470  [500/938]\n",
      "loss: 0.001110  [600/938]\n",
      "loss: 0.000479  [700/938]\n",
      "loss: 0.003269  [800/938]\n",
      "loss: 0.001972  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069796 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.000099  [  0/938]\n",
      "loss: 0.000442  [100/938]\n",
      "loss: 0.001093  [200/938]\n",
      "loss: 0.000353  [300/938]\n",
      "loss: 0.004053  [400/938]\n",
      "loss: 0.000294  [500/938]\n",
      "loss: 0.001934  [600/938]\n",
      "loss: 0.001500  [700/938]\n",
      "loss: 0.001015  [800/938]\n",
      "loss: 0.000404  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069947 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.001973  [  0/938]\n",
      "loss: 0.000597  [100/938]\n",
      "loss: 0.000867  [200/938]\n",
      "loss: 0.000842  [300/938]\n",
      "loss: 0.002750  [400/938]\n",
      "loss: 0.000756  [500/938]\n",
      "loss: 0.002281  [600/938]\n",
      "loss: 0.004072  [700/938]\n",
      "loss: 0.003108  [800/938]\n",
      "loss: 0.005563  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.3%, Avg loss: 0.068807 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.000314  [  0/938]\n",
      "loss: 0.002794  [100/938]\n",
      "loss: 0.001539  [200/938]\n",
      "loss: 0.001254  [300/938]\n",
      "loss: 0.000647  [400/938]\n",
      "loss: 0.001518  [500/938]\n",
      "loss: 0.000516  [600/938]\n",
      "loss: 0.003806  [700/938]\n",
      "loss: 0.001861  [800/938]\n",
      "loss: 0.000231  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069508 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.003108  [  0/938]\n",
      "loss: 0.003467  [100/938]\n",
      "loss: 0.002952  [200/938]\n",
      "loss: 0.002049  [300/938]\n",
      "loss: 0.002046  [400/938]\n",
      "loss: 0.002824  [500/938]\n",
      "loss: 0.000451  [600/938]\n",
      "loss: 0.000905  [700/938]\n",
      "loss: 0.000228  [800/938]\n",
      "loss: 0.000492  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071776 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.000224  [  0/938]\n",
      "loss: 0.002546  [100/938]\n",
      "loss: 0.005601  [200/938]\n",
      "loss: 0.000760  [300/938]\n",
      "loss: 0.001400  [400/938]\n",
      "loss: 0.002564  [500/938]\n",
      "loss: 0.001862  [600/938]\n",
      "loss: 0.000276  [700/938]\n",
      "loss: 0.000169  [800/938]\n",
      "loss: 0.000994  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069587 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.000646  [  0/938]\n",
      "loss: 0.000245  [100/938]\n",
      "loss: 0.002035  [200/938]\n",
      "loss: 0.000212  [300/938]\n",
      "loss: 0.001912  [400/938]\n",
      "loss: 0.000390  [500/938]\n",
      "loss: 0.000121  [600/938]\n",
      "loss: 0.000897  [700/938]\n",
      "loss: 0.003157  [800/938]\n",
      "loss: 0.001619  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069826 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.000776  [  0/938]\n",
      "loss: 0.001110  [100/938]\n",
      "loss: 0.001332  [200/938]\n",
      "loss: 0.001176  [300/938]\n",
      "loss: 0.003074  [400/938]\n",
      "loss: 0.001651  [500/938]\n",
      "loss: 0.001304  [600/938]\n",
      "loss: 0.000955  [700/938]\n",
      "loss: 0.000906  [800/938]\n",
      "loss: 0.001130  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069512 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.000168  [  0/938]\n",
      "loss: 0.000189  [100/938]\n",
      "loss: 0.001820  [200/938]\n",
      "loss: 0.000091  [300/938]\n",
      "loss: 0.002272  [400/938]\n",
      "loss: 0.000426  [500/938]\n",
      "loss: 0.001002  [600/938]\n",
      "loss: 0.000304  [700/938]\n",
      "loss: 0.000794  [800/938]\n",
      "loss: 0.000886  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069377 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.000293  [  0/938]\n",
      "loss: 0.000471  [100/938]\n",
      "loss: 0.000438  [200/938]\n",
      "loss: 0.001650  [300/938]\n",
      "loss: 0.000235  [400/938]\n",
      "loss: 0.001400  [500/938]\n",
      "loss: 0.001535  [600/938]\n",
      "loss: 0.000549  [700/938]\n",
      "loss: 0.002039  [800/938]\n",
      "loss: 0.002990  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069068 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.000311  [  0/938]\n",
      "loss: 0.000261  [100/938]\n",
      "loss: 0.000158  [200/938]\n",
      "loss: 0.001397  [300/938]\n",
      "loss: 0.000682  [400/938]\n",
      "loss: 0.001278  [500/938]\n",
      "loss: 0.000267  [600/938]\n",
      "loss: 0.003164  [700/938]\n",
      "loss: 0.002241  [800/938]\n",
      "loss: 0.001609  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070001 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.000918  [  0/938]\n",
      "loss: 0.002983  [100/938]\n",
      "loss: 0.004503  [200/938]\n",
      "loss: 0.000464  [300/938]\n",
      "loss: 0.001325  [400/938]\n",
      "loss: 0.000670  [500/938]\n",
      "loss: 0.000439  [600/938]\n",
      "loss: 0.002384  [700/938]\n",
      "loss: 0.000568  [800/938]\n",
      "loss: 0.001948  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069583 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.000090  [  0/938]\n",
      "loss: 0.000549  [100/938]\n",
      "loss: 0.002225  [200/938]\n",
      "loss: 0.000920  [300/938]\n",
      "loss: 0.000335  [400/938]\n",
      "loss: 0.000452  [500/938]\n",
      "loss: 0.000390  [600/938]\n",
      "loss: 0.000687  [700/938]\n",
      "loss: 0.000422  [800/938]\n",
      "loss: 0.001964  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069425 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.003008  [  0/938]\n",
      "loss: 0.000584  [100/938]\n",
      "loss: 0.000736  [200/938]\n",
      "loss: 0.000355  [300/938]\n",
      "loss: 0.000972  [400/938]\n",
      "loss: 0.002251  [500/938]\n",
      "loss: 0.001207  [600/938]\n",
      "loss: 0.000318  [700/938]\n",
      "loss: 0.000599  [800/938]\n",
      "loss: 0.000896  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069941 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.000511  [  0/938]\n",
      "loss: 0.000606  [100/938]\n",
      "loss: 0.001304  [200/938]\n",
      "loss: 0.000424  [300/938]\n",
      "loss: 0.000729  [400/938]\n",
      "loss: 0.002966  [500/938]\n",
      "loss: 0.000556  [600/938]\n",
      "loss: 0.001087  [700/938]\n",
      "loss: 0.001188  [800/938]\n",
      "loss: 0.000234  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069975 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.003951  [  0/938]\n",
      "loss: 0.000230  [100/938]\n",
      "loss: 0.000631  [200/938]\n",
      "loss: 0.001508  [300/938]\n",
      "loss: 0.002442  [400/938]\n",
      "loss: 0.000595  [500/938]\n",
      "loss: 0.000233  [600/938]\n",
      "loss: 0.000660  [700/938]\n",
      "loss: 0.000566  [800/938]\n",
      "loss: 0.000672  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070945 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.000154  [  0/938]\n",
      "loss: 0.001446  [100/938]\n",
      "loss: 0.001536  [200/938]\n",
      "loss: 0.000119  [300/938]\n",
      "loss: 0.000201  [400/938]\n",
      "loss: 0.003557  [500/938]\n",
      "loss: 0.001287  [600/938]\n",
      "loss: 0.000212  [700/938]\n",
      "loss: 0.000475  [800/938]\n",
      "loss: 0.000505  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070992 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.001333  [  0/938]\n",
      "loss: 0.000882  [100/938]\n",
      "loss: 0.000409  [200/938]\n",
      "loss: 0.000086  [300/938]\n",
      "loss: 0.000859  [400/938]\n",
      "loss: 0.001390  [500/938]\n",
      "loss: 0.000531  [600/938]\n",
      "loss: 0.000290  [700/938]\n",
      "loss: 0.000832  [800/938]\n",
      "loss: 0.000255  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070609 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.001006  [  0/938]\n",
      "loss: 0.000257  [100/938]\n",
      "loss: 0.000200  [200/938]\n",
      "loss: 0.000484  [300/938]\n",
      "loss: 0.000162  [400/938]\n",
      "loss: 0.001493  [500/938]\n",
      "loss: 0.000615  [600/938]\n",
      "loss: 0.000508  [700/938]\n",
      "loss: 0.000495  [800/938]\n",
      "loss: 0.001244  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071531 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.000628  [  0/938]\n",
      "loss: 0.000712  [100/938]\n",
      "loss: 0.000234  [200/938]\n",
      "loss: 0.001163  [300/938]\n",
      "loss: 0.006785  [400/938]\n",
      "loss: 0.003363  [500/938]\n",
      "loss: 0.000522  [600/938]\n",
      "loss: 0.000268  [700/938]\n",
      "loss: 0.000123  [800/938]\n",
      "loss: 0.001601  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070792 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.000488  [  0/938]\n",
      "loss: 0.000291  [100/938]\n",
      "loss: 0.002075  [200/938]\n",
      "loss: 0.000263  [300/938]\n",
      "loss: 0.000369  [400/938]\n",
      "loss: 0.001203  [500/938]\n",
      "loss: 0.001241  [600/938]\n",
      "loss: 0.000222  [700/938]\n",
      "loss: 0.000837  [800/938]\n",
      "loss: 0.000053  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070663 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.000952  [  0/938]\n",
      "loss: 0.000721  [100/938]\n",
      "loss: 0.001521  [200/938]\n",
      "loss: 0.006952  [300/938]\n",
      "loss: 0.000328  [400/938]\n",
      "loss: 0.000462  [500/938]\n",
      "loss: 0.004212  [600/938]\n",
      "loss: 0.001224  [700/938]\n",
      "loss: 0.000412  [800/938]\n",
      "loss: 0.000470  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071004 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.001800  [  0/938]\n",
      "loss: 0.000263  [100/938]\n",
      "loss: 0.000528  [200/938]\n",
      "loss: 0.001442  [300/938]\n",
      "loss: 0.002560  [400/938]\n",
      "loss: 0.000299  [500/938]\n",
      "loss: 0.000735  [600/938]\n",
      "loss: 0.000478  [700/938]\n",
      "loss: 0.000917  [800/938]\n",
      "loss: 0.001278  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070184 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.000476  [  0/938]\n",
      "loss: 0.000363  [100/938]\n",
      "loss: 0.003422  [200/938]\n",
      "loss: 0.000260  [300/938]\n",
      "loss: 0.000138  [400/938]\n",
      "loss: 0.000260  [500/938]\n",
      "loss: 0.000244  [600/938]\n",
      "loss: 0.000062  [700/938]\n",
      "loss: 0.000296  [800/938]\n",
      "loss: 0.007502  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.074008 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.001903  [  0/938]\n",
      "loss: 0.001046  [100/938]\n",
      "loss: 0.000797  [200/938]\n",
      "loss: 0.000348  [300/938]\n",
      "loss: 0.000630  [400/938]\n",
      "loss: 0.000220  [500/938]\n",
      "loss: 0.000823  [600/938]\n",
      "loss: 0.001036  [700/938]\n",
      "loss: 0.000432  [800/938]\n",
      "loss: 0.000845  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070228 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.002197  [  0/938]\n",
      "loss: 0.001166  [100/938]\n",
      "loss: 0.000153  [200/938]\n",
      "loss: 0.000110  [300/938]\n",
      "loss: 0.000359  [400/938]\n",
      "loss: 0.000623  [500/938]\n",
      "loss: 0.000704  [600/938]\n",
      "loss: 0.000024  [700/938]\n",
      "loss: 0.000316  [800/938]\n",
      "loss: 0.001262  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069946 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.001188  [  0/938]\n",
      "loss: 0.000499  [100/938]\n",
      "loss: 0.000687  [200/938]\n",
      "loss: 0.000665  [300/938]\n",
      "loss: 0.000182  [400/938]\n",
      "loss: 0.000238  [500/938]\n",
      "loss: 0.001300  [600/938]\n",
      "loss: 0.001858  [700/938]\n",
      "loss: 0.000214  [800/938]\n",
      "loss: 0.000382  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.069955 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.000503  [  0/938]\n",
      "loss: 0.001303  [100/938]\n",
      "loss: 0.000978  [200/938]\n",
      "loss: 0.000448  [300/938]\n",
      "loss: 0.000294  [400/938]\n",
      "loss: 0.002979  [500/938]\n",
      "loss: 0.000988  [600/938]\n",
      "loss: 0.000676  [700/938]\n",
      "loss: 0.001277  [800/938]\n",
      "loss: 0.002695  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071229 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.006128  [  0/938]\n",
      "loss: 0.001251  [100/938]\n",
      "loss: 0.000493  [200/938]\n",
      "loss: 0.000679  [300/938]\n",
      "loss: 0.001711  [400/938]\n",
      "loss: 0.000233  [500/938]\n",
      "loss: 0.000514  [600/938]\n",
      "loss: 0.001686  [700/938]\n",
      "loss: 0.000515  [800/938]\n",
      "loss: 0.001230  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071182 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.000273  [  0/938]\n",
      "loss: 0.001416  [100/938]\n",
      "loss: 0.002877  [200/938]\n",
      "loss: 0.007098  [300/938]\n",
      "loss: 0.000730  [400/938]\n",
      "loss: 0.002534  [500/938]\n",
      "loss: 0.009796  [600/938]\n",
      "loss: 0.000807  [700/938]\n",
      "loss: 0.000305  [800/938]\n",
      "loss: 0.001197  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.3%, Avg loss: 0.070773 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.002067  [  0/938]\n",
      "loss: 0.004003  [100/938]\n",
      "loss: 0.000134  [200/938]\n",
      "loss: 0.000376  [300/938]\n",
      "loss: 0.001625  [400/938]\n",
      "loss: 0.000594  [500/938]\n",
      "loss: 0.000221  [600/938]\n",
      "loss: 0.001137  [700/938]\n",
      "loss: 0.002438  [800/938]\n",
      "loss: 0.003584  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.3%, Avg loss: 0.070883 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.002977  [  0/938]\n",
      "loss: 0.000511  [100/938]\n",
      "loss: 0.000345  [200/938]\n",
      "loss: 0.001749  [300/938]\n",
      "loss: 0.000131  [400/938]\n",
      "loss: 0.003215  [500/938]\n",
      "loss: 0.000538  [600/938]\n",
      "loss: 0.003214  [700/938]\n",
      "loss: 0.000472  [800/938]\n",
      "loss: 0.000077  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070855 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.001541  [  0/938]\n",
      "loss: 0.001419  [100/938]\n",
      "loss: 0.000290  [200/938]\n",
      "loss: 0.000740  [300/938]\n",
      "loss: 0.000271  [400/938]\n",
      "loss: 0.000298  [500/938]\n",
      "loss: 0.001758  [600/938]\n",
      "loss: 0.001201  [700/938]\n",
      "loss: 0.000591  [800/938]\n",
      "loss: 0.000238  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071069 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.001647  [  0/938]\n",
      "loss: 0.000575  [100/938]\n",
      "loss: 0.001523  [200/938]\n",
      "loss: 0.000667  [300/938]\n",
      "loss: 0.001997  [400/938]\n",
      "loss: 0.000567  [500/938]\n",
      "loss: 0.000342  [600/938]\n",
      "loss: 0.000436  [700/938]\n",
      "loss: 0.000286  [800/938]\n",
      "loss: 0.000360  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071810 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.000670  [  0/938]\n",
      "loss: 0.003063  [100/938]\n",
      "loss: 0.000358  [200/938]\n",
      "loss: 0.001207  [300/938]\n",
      "loss: 0.000265  [400/938]\n",
      "loss: 0.000228  [500/938]\n",
      "loss: 0.000907  [600/938]\n",
      "loss: 0.001299  [700/938]\n",
      "loss: 0.001507  [800/938]\n",
      "loss: 0.000926  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.3%, Avg loss: 0.070632 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.001386  [  0/938]\n",
      "loss: 0.001457  [100/938]\n",
      "loss: 0.000522  [200/938]\n",
      "loss: 0.001454  [300/938]\n",
      "loss: 0.002056  [400/938]\n",
      "loss: 0.000239  [500/938]\n",
      "loss: 0.002301  [600/938]\n",
      "loss: 0.000879  [700/938]\n",
      "loss: 0.000181  [800/938]\n",
      "loss: 0.000627  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071833 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.000310  [  0/938]\n",
      "loss: 0.000985  [100/938]\n",
      "loss: 0.000498  [200/938]\n",
      "loss: 0.001777  [300/938]\n",
      "loss: 0.001937  [400/938]\n",
      "loss: 0.000734  [500/938]\n",
      "loss: 0.000062  [600/938]\n",
      "loss: 0.001954  [700/938]\n",
      "loss: 0.001210  [800/938]\n",
      "loss: 0.000592  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.073626 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.000716  [  0/938]\n",
      "loss: 0.000465  [100/938]\n",
      "loss: 0.000688  [200/938]\n",
      "loss: 0.009288  [300/938]\n",
      "loss: 0.000761  [400/938]\n",
      "loss: 0.000598  [500/938]\n",
      "loss: 0.003738  [600/938]\n",
      "loss: 0.000155  [700/938]\n",
      "loss: 0.000729  [800/938]\n",
      "loss: 0.000988  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070907 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.000375  [  0/938]\n",
      "loss: 0.000592  [100/938]\n",
      "loss: 0.000471  [200/938]\n",
      "loss: 0.000429  [300/938]\n",
      "loss: 0.000429  [400/938]\n",
      "loss: 0.000054  [500/938]\n",
      "loss: 0.000543  [600/938]\n",
      "loss: 0.000321  [700/938]\n",
      "loss: 0.000885  [800/938]\n",
      "loss: 0.000841  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.074101 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.003817  [  0/938]\n",
      "loss: 0.003612  [100/938]\n",
      "loss: 0.000220  [200/938]\n",
      "loss: 0.000511  [300/938]\n",
      "loss: 0.000233  [400/938]\n",
      "loss: 0.001568  [500/938]\n",
      "loss: 0.004549  [600/938]\n",
      "loss: 0.001757  [700/938]\n",
      "loss: 0.000155  [800/938]\n",
      "loss: 0.001037  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070730 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.000659  [  0/938]\n",
      "loss: 0.000380  [100/938]\n",
      "loss: 0.003024  [200/938]\n",
      "loss: 0.000201  [300/938]\n",
      "loss: 0.002933  [400/938]\n",
      "loss: 0.000515  [500/938]\n",
      "loss: 0.001416  [600/938]\n",
      "loss: 0.000781  [700/938]\n",
      "loss: 0.001030  [800/938]\n",
      "loss: 0.002411  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070975 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.000295  [  0/938]\n",
      "loss: 0.000257  [100/938]\n",
      "loss: 0.000138  [200/938]\n",
      "loss: 0.002016  [300/938]\n",
      "loss: 0.002474  [400/938]\n",
      "loss: 0.002690  [500/938]\n",
      "loss: 0.000197  [600/938]\n",
      "loss: 0.000424  [700/938]\n",
      "loss: 0.001143  [800/938]\n",
      "loss: 0.001096  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.070108 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.002314  [  0/938]\n",
      "loss: 0.000458  [100/938]\n",
      "loss: 0.000161  [200/938]\n",
      "loss: 0.001664  [300/938]\n",
      "loss: 0.002726  [400/938]\n",
      "loss: 0.001378  [500/938]\n",
      "loss: 0.001444  [600/938]\n",
      "loss: 0.000173  [700/938]\n",
      "loss: 0.001390  [800/938]\n",
      "loss: 0.000546  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.3%, Avg loss: 0.071023 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.002201  [  0/938]\n",
      "loss: 0.003326  [100/938]\n",
      "loss: 0.001256  [200/938]\n",
      "loss: 0.000120  [300/938]\n",
      "loss: 0.000219  [400/938]\n",
      "loss: 0.000434  [500/938]\n",
      "loss: 0.001210  [600/938]\n",
      "loss: 0.001127  [700/938]\n",
      "loss: 0.000395  [800/938]\n",
      "loss: 0.003576  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.072289 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.001817  [  0/938]\n",
      "loss: 0.000615  [100/938]\n",
      "loss: 0.000996  [200/938]\n",
      "loss: 0.001929  [300/938]\n",
      "loss: 0.001918  [400/938]\n",
      "loss: 0.001235  [500/938]\n",
      "loss: 0.000448  [600/938]\n",
      "loss: 0.001071  [700/938]\n",
      "loss: 0.000824  [800/938]\n",
      "loss: 0.000880  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.072535 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.001765  [  0/938]\n",
      "loss: 0.000268  [100/938]\n",
      "loss: 0.000907  [200/938]\n",
      "loss: 0.000163  [300/938]\n",
      "loss: 0.000134  [400/938]\n",
      "loss: 0.000797  [500/938]\n",
      "loss: 0.007798  [600/938]\n",
      "loss: 0.000207  [700/938]\n",
      "loss: 0.000056  [800/938]\n",
      "loss: 0.000625  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071330 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.000951  [  0/938]\n",
      "loss: 0.000860  [100/938]\n",
      "loss: 0.000038  [200/938]\n",
      "loss: 0.000347  [300/938]\n",
      "loss: 0.000905  [400/938]\n",
      "loss: 0.000935  [500/938]\n",
      "loss: 0.000645  [600/938]\n",
      "loss: 0.000259  [700/938]\n",
      "loss: 0.000580  [800/938]\n",
      "loss: 0.000198  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071465 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.000110  [  0/938]\n",
      "loss: 0.000826  [100/938]\n",
      "loss: 0.000625  [200/938]\n",
      "loss: 0.000701  [300/938]\n",
      "loss: 0.000995  [400/938]\n",
      "loss: 0.000611  [500/938]\n",
      "loss: 0.000639  [600/938]\n",
      "loss: 0.000303  [700/938]\n",
      "loss: 0.000746  [800/938]\n",
      "loss: 0.003386  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.071978 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.000748  [  0/938]\n",
      "loss: 0.000344  [100/938]\n",
      "loss: 0.000290  [200/938]\n",
      "loss: 0.000113  [300/938]\n",
      "loss: 0.000388  [400/938]\n",
      "loss: 0.000127  [500/938]\n",
      "loss: 0.000336  [600/938]\n",
      "loss: 0.000153  [700/938]\n",
      "loss: 0.000580  [800/938]\n",
      "loss: 0.000527  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.072326 \n",
      "\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "for t in range(EPOCH):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, train_dataset, loss_fn, optimizer)      # 训练模型\n",
    "    test(model, test_dataset, loss_fn)                   # 测试模型\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5ead568f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303347  [  0/938]\n",
      "loss: 2.286153  [100/938]\n",
      "loss: 2.254628  [200/938]\n",
      "loss: 2.194401  [300/938]\n",
      "loss: 1.928651  [400/938]\n",
      "loss: 1.374302  [500/938]\n",
      "loss: 0.980530  [600/938]\n",
      "loss: 0.810860  [700/938]\n",
      "loss: 0.499896  [800/938]\n",
      "loss: 0.525144  [900/938]\n",
      "Test: \n",
      " Accuracy: 85.1%, Avg loss: 0.521591 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.639420  [  0/938]\n",
      "loss: 0.377849  [100/938]\n",
      "loss: 0.252687  [200/938]\n",
      "loss: 0.383685  [300/938]\n",
      "loss: 0.326278  [400/938]\n",
      "loss: 0.389681  [500/938]\n",
      "loss: 0.321718  [600/938]\n",
      "loss: 0.241322  [700/938]\n",
      "loss: 0.410330  [800/938]\n",
      "loss: 0.226820  [900/938]\n",
      "Test: \n",
      " Accuracy: 90.1%, Avg loss: 0.338876 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.291637  [  0/938]\n",
      "loss: 0.255920  [100/938]\n",
      "loss: 0.336062  [200/938]\n",
      "loss: 0.433291  [300/938]\n",
      "loss: 0.375580  [400/938]\n",
      "loss: 0.225151  [500/938]\n",
      "loss: 0.601096  [600/938]\n",
      "loss: 0.500888  [700/938]\n",
      "loss: 0.097533  [800/938]\n",
      "loss: 0.405210  [900/938]\n",
      "Test: \n",
      " Accuracy: 92.0%, Avg loss: 0.279810 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.124329  [  0/938]\n",
      "loss: 0.416703  [100/938]\n",
      "loss: 0.291061  [200/938]\n",
      "loss: 0.376560  [300/938]\n",
      "loss: 0.143028  [400/938]\n",
      "loss: 0.237703  [500/938]\n",
      "loss: 0.144146  [600/938]\n",
      "loss: 0.222031  [700/938]\n",
      "loss: 0.188706  [800/938]\n",
      "loss: 0.144347  [900/938]\n",
      "Test: \n",
      " Accuracy: 93.0%, Avg loss: 0.241944 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.198851  [  0/938]\n",
      "loss: 0.109238  [100/938]\n",
      "loss: 0.196466  [200/938]\n",
      "loss: 0.201066  [300/938]\n",
      "loss: 0.317773  [400/938]\n",
      "loss: 0.094153  [500/938]\n",
      "loss: 0.273691  [600/938]\n",
      "loss: 0.107782  [700/938]\n",
      "loss: 0.312877  [800/938]\n",
      "loss: 0.342339  [900/938]\n",
      "Test: \n",
      " Accuracy: 93.8%, Avg loss: 0.212018 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.158175  [  0/938]\n",
      "loss: 0.174224  [100/938]\n",
      "loss: 0.153344  [200/938]\n",
      "loss: 0.272831  [300/938]\n",
      "loss: 0.169949  [400/938]\n",
      "loss: 0.337728  [500/938]\n",
      "loss: 0.239442  [600/938]\n",
      "loss: 0.160750  [700/938]\n",
      "loss: 0.276937  [800/938]\n",
      "loss: 0.229787  [900/938]\n",
      "Test: \n",
      " Accuracy: 94.6%, Avg loss: 0.184984 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.365212  [  0/938]\n",
      "loss: 0.240964  [100/938]\n",
      "loss: 0.166320  [200/938]\n",
      "loss: 0.178638  [300/938]\n",
      "loss: 0.184801  [400/938]\n",
      "loss: 0.105031  [500/938]\n",
      "loss: 0.162622  [600/938]\n",
      "loss: 0.178407  [700/938]\n",
      "loss: 0.174821  [800/938]\n",
      "loss: 0.165282  [900/938]\n",
      "Test: \n",
      " Accuracy: 95.2%, Avg loss: 0.164481 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.210283  [  0/938]\n",
      "loss: 0.185968  [100/938]\n",
      "loss: 0.085995  [200/938]\n",
      "loss: 0.262325  [300/938]\n",
      "loss: 0.137198  [400/938]\n",
      "loss: 0.089031  [500/938]\n",
      "loss: 0.101315  [600/938]\n",
      "loss: 0.149987  [700/938]\n",
      "loss: 0.145133  [800/938]\n",
      "loss: 0.175085  [900/938]\n",
      "Test: \n",
      " Accuracy: 95.6%, Avg loss: 0.152181 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.180238  [  0/938]\n",
      "loss: 0.106106  [100/938]\n",
      "loss: 0.180945  [200/938]\n",
      "loss: 0.185316  [300/938]\n",
      "loss: 0.107773  [400/938]\n",
      "loss: 0.205060  [500/938]\n",
      "loss: 0.185806  [600/938]\n",
      "loss: 0.065885  [700/938]\n",
      "loss: 0.198879  [800/938]\n",
      "loss: 0.074117  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.0%, Avg loss: 0.138133 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.159352  [  0/938]\n",
      "loss: 0.306474  [100/938]\n",
      "loss: 0.249038  [200/938]\n",
      "loss: 0.122430  [300/938]\n",
      "loss: 0.053226  [400/938]\n",
      "loss: 0.177846  [500/938]\n",
      "loss: 0.183644  [600/938]\n",
      "loss: 0.088115  [700/938]\n",
      "loss: 0.072044  [800/938]\n",
      "loss: 0.327599  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.1%, Avg loss: 0.130801 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.100446  [  0/938]\n",
      "loss: 0.106256  [100/938]\n",
      "loss: 0.063339  [200/938]\n",
      "loss: 0.111422  [300/938]\n",
      "loss: 0.170285  [400/938]\n",
      "loss: 0.123918  [500/938]\n",
      "loss: 0.138338  [600/938]\n",
      "loss: 0.074528  [700/938]\n",
      "loss: 0.075452  [800/938]\n",
      "loss: 0.134286  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.4%, Avg loss: 0.118466 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.036506  [  0/938]\n",
      "loss: 0.050580  [100/938]\n",
      "loss: 0.042348  [200/938]\n",
      "loss: 0.185150  [300/938]\n",
      "loss: 0.254976  [400/938]\n",
      "loss: 0.039829  [500/938]\n",
      "loss: 0.164018  [600/938]\n",
      "loss: 0.098020  [700/938]\n",
      "loss: 0.174404  [800/938]\n",
      "loss: 0.088278  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.8%, Avg loss: 0.113385 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.091989  [  0/938]\n",
      "loss: 0.048641  [100/938]\n",
      "loss: 0.058694  [200/938]\n",
      "loss: 0.046961  [300/938]\n",
      "loss: 0.146384  [400/938]\n",
      "loss: 0.031619  [500/938]\n",
      "loss: 0.108682  [600/938]\n",
      "loss: 0.039613  [700/938]\n",
      "loss: 0.136310  [800/938]\n",
      "loss: 0.020127  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.9%, Avg loss: 0.104008 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.029642  [  0/938]\n",
      "loss: 0.127623  [100/938]\n",
      "loss: 0.071740  [200/938]\n",
      "loss: 0.252594  [300/938]\n",
      "loss: 0.134806  [400/938]\n",
      "loss: 0.135620  [500/938]\n",
      "loss: 0.028739  [600/938]\n",
      "loss: 0.035207  [700/938]\n",
      "loss: 0.080960  [800/938]\n",
      "loss: 0.144258  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.9%, Avg loss: 0.103442 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.188537  [  0/938]\n",
      "loss: 0.042417  [100/938]\n",
      "loss: 0.094757  [200/938]\n",
      "loss: 0.073960  [300/938]\n",
      "loss: 0.077559  [400/938]\n",
      "loss: 0.048583  [500/938]\n",
      "loss: 0.028099  [600/938]\n",
      "loss: 0.120999  [700/938]\n",
      "loss: 0.112383  [800/938]\n",
      "loss: 0.189310  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.1%, Avg loss: 0.096427 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.052800  [  0/938]\n",
      "loss: 0.077240  [100/938]\n",
      "loss: 0.136345  [200/938]\n",
      "loss: 0.085098  [300/938]\n",
      "loss: 0.047062  [400/938]\n",
      "loss: 0.062462  [500/938]\n",
      "loss: 0.115889  [600/938]\n",
      "loss: 0.039044  [700/938]\n",
      "loss: 0.111309  [800/938]\n",
      "loss: 0.174299  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.1%, Avg loss: 0.091441 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.017122  [  0/938]\n",
      "loss: 0.105155  [100/938]\n",
      "loss: 0.141865  [200/938]\n",
      "loss: 0.110249  [300/938]\n",
      "loss: 0.059492  [400/938]\n",
      "loss: 0.048190  [500/938]\n",
      "loss: 0.018972  [600/938]\n",
      "loss: 0.046995  [700/938]\n",
      "loss: 0.025144  [800/938]\n",
      "loss: 0.163964  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.2%, Avg loss: 0.089323 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.040872  [  0/938]\n",
      "loss: 0.060653  [100/938]\n",
      "loss: 0.026161  [200/938]\n",
      "loss: 0.128635  [300/938]\n",
      "loss: 0.064396  [400/938]\n",
      "loss: 0.052167  [500/938]\n",
      "loss: 0.019044  [600/938]\n",
      "loss: 0.037059  [700/938]\n",
      "loss: 0.071152  [800/938]\n",
      "loss: 0.023194  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.4%, Avg loss: 0.085035 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.104991  [  0/938]\n",
      "loss: 0.073220  [100/938]\n",
      "loss: 0.019138  [200/938]\n",
      "loss: 0.158533  [300/938]\n",
      "loss: 0.069039  [400/938]\n",
      "loss: 0.009212  [500/938]\n",
      "loss: 0.134930  [600/938]\n",
      "loss: 0.016518  [700/938]\n",
      "loss: 0.040416  [800/938]\n",
      "loss: 0.033464  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.5%, Avg loss: 0.083359 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.086065  [  0/938]\n",
      "loss: 0.026619  [100/938]\n",
      "loss: 0.063933  [200/938]\n",
      "loss: 0.018131  [300/938]\n",
      "loss: 0.029320  [400/938]\n",
      "loss: 0.052844  [500/938]\n",
      "loss: 0.058364  [600/938]\n",
      "loss: 0.026496  [700/938]\n",
      "loss: 0.086363  [800/938]\n",
      "loss: 0.032994  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.6%, Avg loss: 0.081559 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.034790  [  0/938]\n",
      "loss: 0.011713  [100/938]\n",
      "loss: 0.030159  [200/938]\n",
      "loss: 0.051266  [300/938]\n",
      "loss: 0.037848  [400/938]\n",
      "loss: 0.058825  [500/938]\n",
      "loss: 0.049443  [600/938]\n",
      "loss: 0.042654  [700/938]\n",
      "loss: 0.047277  [800/938]\n",
      "loss: 0.058934  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.4%, Avg loss: 0.079901 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.034483  [  0/938]\n",
      "loss: 0.009293  [100/938]\n",
      "loss: 0.023503  [200/938]\n",
      "loss: 0.074312  [300/938]\n",
      "loss: 0.088426  [400/938]\n",
      "loss: 0.101141  [500/938]\n",
      "loss: 0.061947  [600/938]\n",
      "loss: 0.028861  [700/938]\n",
      "loss: 0.030731  [800/938]\n",
      "loss: 0.103920  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.6%, Avg loss: 0.080740 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.031317  [  0/938]\n",
      "loss: 0.017629  [100/938]\n",
      "loss: 0.030885  [200/938]\n",
      "loss: 0.036385  [300/938]\n",
      "loss: 0.057531  [400/938]\n",
      "loss: 0.037211  [500/938]\n",
      "loss: 0.046479  [600/938]\n",
      "loss: 0.020872  [700/938]\n",
      "loss: 0.055358  [800/938]\n",
      "loss: 0.155197  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.5%, Avg loss: 0.076671 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.047227  [  0/938]\n",
      "loss: 0.027403  [100/938]\n",
      "loss: 0.015167  [200/938]\n",
      "loss: 0.011682  [300/938]\n",
      "loss: 0.025933  [400/938]\n",
      "loss: 0.031538  [500/938]\n",
      "loss: 0.009765  [600/938]\n",
      "loss: 0.030051  [700/938]\n",
      "loss: 0.043023  [800/938]\n",
      "loss: 0.048299  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.6%, Avg loss: 0.075681 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.014951  [  0/938]\n",
      "loss: 0.018447  [100/938]\n",
      "loss: 0.011575  [200/938]\n",
      "loss: 0.054633  [300/938]\n",
      "loss: 0.033794  [400/938]\n",
      "loss: 0.011497  [500/938]\n",
      "loss: 0.050309  [600/938]\n",
      "loss: 0.041915  [700/938]\n",
      "loss: 0.014158  [800/938]\n",
      "loss: 0.071837  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.7%, Avg loss: 0.072982 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.016125  [  0/938]\n",
      "loss: 0.064324  [100/938]\n",
      "loss: 0.060675  [200/938]\n",
      "loss: 0.012290  [300/938]\n",
      "loss: 0.018381  [400/938]\n",
      "loss: 0.035206  [500/938]\n",
      "loss: 0.036292  [600/938]\n",
      "loss: 0.073450  [700/938]\n",
      "loss: 0.074910  [800/938]\n",
      "loss: 0.057736  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.7%, Avg loss: 0.072115 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.084189  [  0/938]\n",
      "loss: 0.022683  [100/938]\n",
      "loss: 0.008434  [200/938]\n",
      "loss: 0.203970  [300/938]\n",
      "loss: 0.020618  [400/938]\n",
      "loss: 0.029544  [500/938]\n",
      "loss: 0.034474  [600/938]\n",
      "loss: 0.057905  [700/938]\n",
      "loss: 0.014458  [800/938]\n",
      "loss: 0.032846  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.069775 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.019351  [  0/938]\n",
      "loss: 0.017195  [100/938]\n",
      "loss: 0.027008  [200/938]\n",
      "loss: 0.057626  [300/938]\n",
      "loss: 0.011140  [400/938]\n",
      "loss: 0.020573  [500/938]\n",
      "loss: 0.015256  [600/938]\n",
      "loss: 0.050539  [700/938]\n",
      "loss: 0.021565  [800/938]\n",
      "loss: 0.033037  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.071059 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.043673  [  0/938]\n",
      "loss: 0.023919  [100/938]\n",
      "loss: 0.011247  [200/938]\n",
      "loss: 0.095530  [300/938]\n",
      "loss: 0.016064  [400/938]\n",
      "loss: 0.035990  [500/938]\n",
      "loss: 0.019828  [600/938]\n",
      "loss: 0.052471  [700/938]\n",
      "loss: 0.023493  [800/938]\n",
      "loss: 0.066739  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.068432 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.019528  [  0/938]\n",
      "loss: 0.010910  [100/938]\n",
      "loss: 0.015973  [200/938]\n",
      "loss: 0.033041  [300/938]\n",
      "loss: 0.077944  [400/938]\n",
      "loss: 0.046425  [500/938]\n",
      "loss: 0.020006  [600/938]\n",
      "loss: 0.011709  [700/938]\n",
      "loss: 0.051072  [800/938]\n",
      "loss: 0.037833  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.8%, Avg loss: 0.068716 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.012415  [  0/938]\n",
      "loss: 0.113651  [100/938]\n",
      "loss: 0.009597  [200/938]\n",
      "loss: 0.005577  [300/938]\n",
      "loss: 0.006665  [400/938]\n",
      "loss: 0.038980  [500/938]\n",
      "loss: 0.021141  [600/938]\n",
      "loss: 0.044144  [700/938]\n",
      "loss: 0.038446  [800/938]\n",
      "loss: 0.064688  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.066683 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.005676  [  0/938]\n",
      "loss: 0.011944  [100/938]\n",
      "loss: 0.056429  [200/938]\n",
      "loss: 0.029200  [300/938]\n",
      "loss: 0.012646  [400/938]\n",
      "loss: 0.010511  [500/938]\n",
      "loss: 0.011859  [600/938]\n",
      "loss: 0.066311  [700/938]\n",
      "loss: 0.036219  [800/938]\n",
      "loss: 0.036846  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.068229 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.016504  [  0/938]\n",
      "loss: 0.026050  [100/938]\n",
      "loss: 0.010111  [200/938]\n",
      "loss: 0.054069  [300/938]\n",
      "loss: 0.024209  [400/938]\n",
      "loss: 0.030140  [500/938]\n",
      "loss: 0.008665  [600/938]\n",
      "loss: 0.017870  [700/938]\n",
      "loss: 0.014256  [800/938]\n",
      "loss: 0.010632  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.8%, Avg loss: 0.067525 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.021232  [  0/938]\n",
      "loss: 0.006463  [100/938]\n",
      "loss: 0.091442  [200/938]\n",
      "loss: 0.016398  [300/938]\n",
      "loss: 0.023702  [400/938]\n",
      "loss: 0.054060  [500/938]\n",
      "loss: 0.038938  [600/938]\n",
      "loss: 0.050950  [700/938]\n",
      "loss: 0.025183  [800/938]\n",
      "loss: 0.015825  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.065486 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.025335  [  0/938]\n",
      "loss: 0.004653  [100/938]\n",
      "loss: 0.023087  [200/938]\n",
      "loss: 0.015902  [300/938]\n",
      "loss: 0.013929  [400/938]\n",
      "loss: 0.005415  [500/938]\n",
      "loss: 0.022468  [600/938]\n",
      "loss: 0.029730  [700/938]\n",
      "loss: 0.062340  [800/938]\n",
      "loss: 0.024904  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.067499 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.006521  [  0/938]\n",
      "loss: 0.023015  [100/938]\n",
      "loss: 0.033737  [200/938]\n",
      "loss: 0.011973  [300/938]\n",
      "loss: 0.107800  [400/938]\n",
      "loss: 0.003285  [500/938]\n",
      "loss: 0.009870  [600/938]\n",
      "loss: 0.015840  [700/938]\n",
      "loss: 0.006136  [800/938]\n",
      "loss: 0.050886  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.8%, Avg loss: 0.067234 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.122256  [  0/938]\n",
      "loss: 0.025230  [100/938]\n",
      "loss: 0.026127  [200/938]\n",
      "loss: 0.018837  [300/938]\n",
      "loss: 0.006945  [400/938]\n",
      "loss: 0.035016  [500/938]\n",
      "loss: 0.011614  [600/938]\n",
      "loss: 0.012677  [700/938]\n",
      "loss: 0.005090  [800/938]\n",
      "loss: 0.009570  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.8%, Avg loss: 0.065350 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.005760  [  0/938]\n",
      "loss: 0.011911  [100/938]\n",
      "loss: 0.011208  [200/938]\n",
      "loss: 0.007175  [300/938]\n",
      "loss: 0.011687  [400/938]\n",
      "loss: 0.007007  [500/938]\n",
      "loss: 0.031261  [600/938]\n",
      "loss: 0.024487  [700/938]\n",
      "loss: 0.003333  [800/938]\n",
      "loss: 0.035529  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.8%, Avg loss: 0.069720 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.008872  [  0/938]\n",
      "loss: 0.028846  [100/938]\n",
      "loss: 0.006361  [200/938]\n",
      "loss: 0.005532  [300/938]\n",
      "loss: 0.006927  [400/938]\n",
      "loss: 0.034451  [500/938]\n",
      "loss: 0.007025  [600/938]\n",
      "loss: 0.021757  [700/938]\n",
      "loss: 0.003805  [800/938]\n",
      "loss: 0.017648  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.065747 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.007561  [  0/938]\n",
      "loss: 0.004015  [100/938]\n",
      "loss: 0.054567  [200/938]\n",
      "loss: 0.004236  [300/938]\n",
      "loss: 0.002362  [400/938]\n",
      "loss: 0.008216  [500/938]\n",
      "loss: 0.011107  [600/938]\n",
      "loss: 0.027451  [700/938]\n",
      "loss: 0.008038  [800/938]\n",
      "loss: 0.023877  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.066172 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.008271  [  0/938]\n",
      "loss: 0.030668  [100/938]\n",
      "loss: 0.034387  [200/938]\n",
      "loss: 0.007573  [300/938]\n",
      "loss: 0.012941  [400/938]\n",
      "loss: 0.025051  [500/938]\n",
      "loss: 0.004430  [600/938]\n",
      "loss: 0.007942  [700/938]\n",
      "loss: 0.004752  [800/938]\n",
      "loss: 0.011546  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.064613 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.011053  [  0/938]\n",
      "loss: 0.008146  [100/938]\n",
      "loss: 0.007665  [200/938]\n",
      "loss: 0.006426  [300/938]\n",
      "loss: 0.016377  [400/938]\n",
      "loss: 0.012033  [500/938]\n",
      "loss: 0.012889  [600/938]\n",
      "loss: 0.008494  [700/938]\n",
      "loss: 0.027229  [800/938]\n",
      "loss: 0.014726  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.065425 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.024393  [  0/938]\n",
      "loss: 0.008399  [100/938]\n",
      "loss: 0.003355  [200/938]\n",
      "loss: 0.023770  [300/938]\n",
      "loss: 0.010171  [400/938]\n",
      "loss: 0.001779  [500/938]\n",
      "loss: 0.007074  [600/938]\n",
      "loss: 0.006666  [700/938]\n",
      "loss: 0.011997  [800/938]\n",
      "loss: 0.012143  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.065089 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.003726  [  0/938]\n",
      "loss: 0.002862  [100/938]\n",
      "loss: 0.009264  [200/938]\n",
      "loss: 0.004385  [300/938]\n",
      "loss: 0.023762  [400/938]\n",
      "loss: 0.004696  [500/938]\n",
      "loss: 0.009465  [600/938]\n",
      "loss: 0.011428  [700/938]\n",
      "loss: 0.022308  [800/938]\n",
      "loss: 0.012900  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.8%, Avg loss: 0.066065 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.013598  [  0/938]\n",
      "loss: 0.003993  [100/938]\n",
      "loss: 0.018264  [200/938]\n",
      "loss: 0.015274  [300/938]\n",
      "loss: 0.004821  [400/938]\n",
      "loss: 0.004074  [500/938]\n",
      "loss: 0.009812  [600/938]\n",
      "loss: 0.007736  [700/938]\n",
      "loss: 0.009392  [800/938]\n",
      "loss: 0.005547  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.065981 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.004409  [  0/938]\n",
      "loss: 0.004635  [100/938]\n",
      "loss: 0.010423  [200/938]\n",
      "loss: 0.006024  [300/938]\n",
      "loss: 0.001688  [400/938]\n",
      "loss: 0.013881  [500/938]\n",
      "loss: 0.006666  [600/938]\n",
      "loss: 0.011720  [700/938]\n",
      "loss: 0.011453  [800/938]\n",
      "loss: 0.005354  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.066436 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.009490  [  0/938]\n",
      "loss: 0.010155  [100/938]\n",
      "loss: 0.004563  [200/938]\n",
      "loss: 0.021174  [300/938]\n",
      "loss: 0.009454  [400/938]\n",
      "loss: 0.006625  [500/938]\n",
      "loss: 0.033221  [600/938]\n",
      "loss: 0.003839  [700/938]\n",
      "loss: 0.005295  [800/938]\n",
      "loss: 0.007811  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.069854 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.009982  [  0/938]\n",
      "loss: 0.005294  [100/938]\n",
      "loss: 0.008927  [200/938]\n",
      "loss: 0.006674  [300/938]\n",
      "loss: 0.008929  [400/938]\n",
      "loss: 0.011461  [500/938]\n",
      "loss: 0.008685  [600/938]\n",
      "loss: 0.011382  [700/938]\n",
      "loss: 0.007618  [800/938]\n",
      "loss: 0.004298  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.066272 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.006821  [  0/938]\n",
      "loss: 0.006870  [100/938]\n",
      "loss: 0.013947  [200/938]\n",
      "loss: 0.005999  [300/938]\n",
      "loss: 0.022972  [400/938]\n",
      "loss: 0.001075  [500/938]\n",
      "loss: 0.027235  [600/938]\n",
      "loss: 0.006166  [700/938]\n",
      "loss: 0.006899  [800/938]\n",
      "loss: 0.004081  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.065562 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.015141  [  0/938]\n",
      "loss: 0.007488  [100/938]\n",
      "loss: 0.000704  [200/938]\n",
      "loss: 0.015098  [300/938]\n",
      "loss: 0.003478  [400/938]\n",
      "loss: 0.002989  [500/938]\n",
      "loss: 0.003647  [600/938]\n",
      "loss: 0.004515  [700/938]\n",
      "loss: 0.027109  [800/938]\n",
      "loss: 0.009754  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.066202 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.002952  [  0/938]\n",
      "loss: 0.021216  [100/938]\n",
      "loss: 0.039597  [200/938]\n",
      "loss: 0.007024  [300/938]\n",
      "loss: 0.003798  [400/938]\n",
      "loss: 0.007030  [500/938]\n",
      "loss: 0.004211  [600/938]\n",
      "loss: 0.009173  [700/938]\n",
      "loss: 0.004944  [800/938]\n",
      "loss: 0.006866  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.067759 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.012567  [  0/938]\n",
      "loss: 0.002976  [100/938]\n",
      "loss: 0.007509  [200/938]\n",
      "loss: 0.009126  [300/938]\n",
      "loss: 0.084098  [400/938]\n",
      "loss: 0.002696  [500/938]\n",
      "loss: 0.006859  [600/938]\n",
      "loss: 0.011540  [700/938]\n",
      "loss: 0.009091  [800/938]\n",
      "loss: 0.009160  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.066587 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.008064  [  0/938]\n",
      "loss: 0.003164  [100/938]\n",
      "loss: 0.002387  [200/938]\n",
      "loss: 0.007322  [300/938]\n",
      "loss: 0.009565  [400/938]\n",
      "loss: 0.005934  [500/938]\n",
      "loss: 0.019350  [600/938]\n",
      "loss: 0.009646  [700/938]\n",
      "loss: 0.006803  [800/938]\n",
      "loss: 0.015923  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.066810 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.006772  [  0/938]\n",
      "loss: 0.031421  [100/938]\n",
      "loss: 0.005309  [200/938]\n",
      "loss: 0.009477  [300/938]\n",
      "loss: 0.004682  [400/938]\n",
      "loss: 0.013004  [500/938]\n",
      "loss: 0.002591  [600/938]\n",
      "loss: 0.002047  [700/938]\n",
      "loss: 0.002527  [800/938]\n",
      "loss: 0.005962  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.067559 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.005868  [  0/938]\n",
      "loss: 0.006626  [100/938]\n",
      "loss: 0.001601  [200/938]\n",
      "loss: 0.002488  [300/938]\n",
      "loss: 0.004339  [400/938]\n",
      "loss: 0.018294  [500/938]\n",
      "loss: 0.001406  [600/938]\n",
      "loss: 0.007293  [700/938]\n",
      "loss: 0.003656  [800/938]\n",
      "loss: 0.009129  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.067067 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.004418  [  0/938]\n",
      "loss: 0.004209  [100/938]\n",
      "loss: 0.005379  [200/938]\n",
      "loss: 0.007084  [300/938]\n",
      "loss: 0.008150  [400/938]\n",
      "loss: 0.004833  [500/938]\n",
      "loss: 0.004499  [600/938]\n",
      "loss: 0.011269  [700/938]\n",
      "loss: 0.010655  [800/938]\n",
      "loss: 0.005783  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.068736 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.006518  [  0/938]\n",
      "loss: 0.002473  [100/938]\n",
      "loss: 0.003963  [200/938]\n",
      "loss: 0.001029  [300/938]\n",
      "loss: 0.010297  [400/938]\n",
      "loss: 0.004775  [500/938]\n",
      "loss: 0.011701  [600/938]\n",
      "loss: 0.011640  [700/938]\n",
      "loss: 0.006807  [800/938]\n",
      "loss: 0.003386  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.067465 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.011711  [  0/938]\n",
      "loss: 0.003382  [100/938]\n",
      "loss: 0.006871  [200/938]\n",
      "loss: 0.008393  [300/938]\n",
      "loss: 0.005837  [400/938]\n",
      "loss: 0.008605  [500/938]\n",
      "loss: 0.009702  [600/938]\n",
      "loss: 0.072101  [700/938]\n",
      "loss: 0.002153  [800/938]\n",
      "loss: 0.003144  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.070116 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.002926  [  0/938]\n",
      "loss: 0.006776  [100/938]\n",
      "loss: 0.000833  [200/938]\n",
      "loss: 0.006912  [300/938]\n",
      "loss: 0.003727  [400/938]\n",
      "loss: 0.007526  [500/938]\n",
      "loss: 0.010559  [600/938]\n",
      "loss: 0.005597  [700/938]\n",
      "loss: 0.005559  [800/938]\n",
      "loss: 0.003485  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.067407 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.008512  [  0/938]\n",
      "loss: 0.001605  [100/938]\n",
      "loss: 0.004067  [200/938]\n",
      "loss: 0.002623  [300/938]\n",
      "loss: 0.001889  [400/938]\n",
      "loss: 0.002203  [500/938]\n",
      "loss: 0.004798  [600/938]\n",
      "loss: 0.005200  [700/938]\n",
      "loss: 0.004213  [800/938]\n",
      "loss: 0.009393  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.068514 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.001810  [  0/938]\n",
      "loss: 0.003077  [100/938]\n",
      "loss: 0.002358  [200/938]\n",
      "loss: 0.001033  [300/938]\n",
      "loss: 0.005544  [400/938]\n",
      "loss: 0.001281  [500/938]\n",
      "loss: 0.007306  [600/938]\n",
      "loss: 0.007429  [700/938]\n",
      "loss: 0.007789  [800/938]\n",
      "loss: 0.004497  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.068550 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.006025  [  0/938]\n",
      "loss: 0.006045  [100/938]\n",
      "loss: 0.007728  [200/938]\n",
      "loss: 0.005023  [300/938]\n",
      "loss: 0.003721  [400/938]\n",
      "loss: 0.000880  [500/938]\n",
      "loss: 0.005080  [600/938]\n",
      "loss: 0.006521  [700/938]\n",
      "loss: 0.003180  [800/938]\n",
      "loss: 0.009626  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.068514 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.004113  [  0/938]\n",
      "loss: 0.001310  [100/938]\n",
      "loss: 0.004374  [200/938]\n",
      "loss: 0.002489  [300/938]\n",
      "loss: 0.001628  [400/938]\n",
      "loss: 0.001392  [500/938]\n",
      "loss: 0.000369  [600/938]\n",
      "loss: 0.004108  [700/938]\n",
      "loss: 0.010139  [800/938]\n",
      "loss: 0.007925  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.070006 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.004594  [  0/938]\n",
      "loss: 0.006305  [100/938]\n",
      "loss: 0.005061  [200/938]\n",
      "loss: 0.008756  [300/938]\n",
      "loss: 0.001153  [400/938]\n",
      "loss: 0.006466  [500/938]\n",
      "loss: 0.006130  [600/938]\n",
      "loss: 0.004927  [700/938]\n",
      "loss: 0.007470  [800/938]\n",
      "loss: 0.003493  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.068778 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.002473  [  0/938]\n",
      "loss: 0.002562  [100/938]\n",
      "loss: 0.007760  [200/938]\n",
      "loss: 0.002027  [300/938]\n",
      "loss: 0.004015  [400/938]\n",
      "loss: 0.004151  [500/938]\n",
      "loss: 0.005725  [600/938]\n",
      "loss: 0.002941  [700/938]\n",
      "loss: 0.001722  [800/938]\n",
      "loss: 0.001336  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.068773 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.003158  [  0/938]\n",
      "loss: 0.006576  [100/938]\n",
      "loss: 0.002812  [200/938]\n",
      "loss: 0.005129  [300/938]\n",
      "loss: 0.005342  [400/938]\n",
      "loss: 0.005315  [500/938]\n",
      "loss: 0.003092  [600/938]\n",
      "loss: 0.002349  [700/938]\n",
      "loss: 0.007196  [800/938]\n",
      "loss: 0.002642  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.069581 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.000546  [  0/938]\n",
      "loss: 0.003931  [100/938]\n",
      "loss: 0.003027  [200/938]\n",
      "loss: 0.002735  [300/938]\n",
      "loss: 0.003248  [400/938]\n",
      "loss: 0.004161  [500/938]\n",
      "loss: 0.001941  [600/938]\n",
      "loss: 0.002665  [700/938]\n",
      "loss: 0.001307  [800/938]\n",
      "loss: 0.002078  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.069969 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.001695  [  0/938]\n",
      "loss: 0.004360  [100/938]\n",
      "loss: 0.003426  [200/938]\n",
      "loss: 0.003519  [300/938]\n",
      "loss: 0.001282  [400/938]\n",
      "loss: 0.004390  [500/938]\n",
      "loss: 0.006934  [600/938]\n",
      "loss: 0.002603  [700/938]\n",
      "loss: 0.001990  [800/938]\n",
      "loss: 0.002906  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.071526 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.003104  [  0/938]\n",
      "loss: 0.003968  [100/938]\n",
      "loss: 0.004585  [200/938]\n",
      "loss: 0.001235  [300/938]\n",
      "loss: 0.001272  [400/938]\n",
      "loss: 0.003165  [500/938]\n",
      "loss: 0.002414  [600/938]\n",
      "loss: 0.005554  [700/938]\n",
      "loss: 0.006176  [800/938]\n",
      "loss: 0.004744  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.070073 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.003496  [  0/938]\n",
      "loss: 0.002770  [100/938]\n",
      "loss: 0.002754  [200/938]\n",
      "loss: 0.000393  [300/938]\n",
      "loss: 0.001162  [400/938]\n",
      "loss: 0.001299  [500/938]\n",
      "loss: 0.002149  [600/938]\n",
      "loss: 0.003077  [700/938]\n",
      "loss: 0.004134  [800/938]\n",
      "loss: 0.005980  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.072420 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.000808  [  0/938]\n",
      "loss: 0.002982  [100/938]\n",
      "loss: 0.001269  [200/938]\n",
      "loss: 0.001800  [300/938]\n",
      "loss: 0.003777  [400/938]\n",
      "loss: 0.003789  [500/938]\n",
      "loss: 0.002209  [600/938]\n",
      "loss: 0.002783  [700/938]\n",
      "loss: 0.002365  [800/938]\n",
      "loss: 0.002935  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.070204 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.001182  [  0/938]\n",
      "loss: 0.002928  [100/938]\n",
      "loss: 0.002103  [200/938]\n",
      "loss: 0.003321  [300/938]\n",
      "loss: 0.003979  [400/938]\n",
      "loss: 0.005019  [500/938]\n",
      "loss: 0.004964  [600/938]\n",
      "loss: 0.000892  [700/938]\n",
      "loss: 0.002754  [800/938]\n",
      "loss: 0.003484  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.070023 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.003401  [  0/938]\n",
      "loss: 0.004256  [100/938]\n",
      "loss: 0.001143  [200/938]\n",
      "loss: 0.002884  [300/938]\n",
      "loss: 0.001919  [400/938]\n",
      "loss: 0.003431  [500/938]\n",
      "loss: 0.002806  [600/938]\n",
      "loss: 0.002647  [700/938]\n",
      "loss: 0.001915  [800/938]\n",
      "loss: 0.000648  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.071494 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.009031  [  0/938]\n",
      "loss: 0.005442  [100/938]\n",
      "loss: 0.002550  [200/938]\n",
      "loss: 0.001326  [300/938]\n",
      "loss: 0.003592  [400/938]\n",
      "loss: 0.001826  [500/938]\n",
      "loss: 0.003515  [600/938]\n",
      "loss: 0.005613  [700/938]\n",
      "loss: 0.004151  [800/938]\n",
      "loss: 0.002730  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.070976 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.003796  [  0/938]\n",
      "loss: 0.006296  [100/938]\n",
      "loss: 0.002722  [200/938]\n",
      "loss: 0.003652  [300/938]\n",
      "loss: 0.004181  [400/938]\n",
      "loss: 0.003706  [500/938]\n",
      "loss: 0.001888  [600/938]\n",
      "loss: 0.001807  [700/938]\n",
      "loss: 0.001293  [800/938]\n",
      "loss: 0.001422  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.072035 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.002609  [  0/938]\n",
      "loss: 0.001459  [100/938]\n",
      "loss: 0.004903  [200/938]\n",
      "loss: 0.002005  [300/938]\n",
      "loss: 0.001844  [400/938]\n",
      "loss: 0.005249  [500/938]\n",
      "loss: 0.006901  [600/938]\n",
      "loss: 0.003303  [700/938]\n",
      "loss: 0.003085  [800/938]\n",
      "loss: 0.001141  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.070892 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.000291  [  0/938]\n",
      "loss: 0.005478  [100/938]\n",
      "loss: 0.002794  [200/938]\n",
      "loss: 0.003600  [300/938]\n",
      "loss: 0.000757  [400/938]\n",
      "loss: 0.001422  [500/938]\n",
      "loss: 0.007277  [600/938]\n",
      "loss: 0.003661  [700/938]\n",
      "loss: 0.003628  [800/938]\n",
      "loss: 0.001579  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.072819 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.004001  [  0/938]\n",
      "loss: 0.003994  [100/938]\n",
      "loss: 0.004626  [200/938]\n",
      "loss: 0.000917  [300/938]\n",
      "loss: 0.005185  [400/938]\n",
      "loss: 0.003582  [500/938]\n",
      "loss: 0.001048  [600/938]\n",
      "loss: 0.001148  [700/938]\n",
      "loss: 0.002086  [800/938]\n",
      "loss: 0.004937  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.072745 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.002930  [  0/938]\n",
      "loss: 0.001887  [100/938]\n",
      "loss: 0.005164  [200/938]\n",
      "loss: 0.001660  [300/938]\n",
      "loss: 0.000658  [400/938]\n",
      "loss: 0.003815  [500/938]\n",
      "loss: 0.001324  [600/938]\n",
      "loss: 0.001834  [700/938]\n",
      "loss: 0.001544  [800/938]\n",
      "loss: 0.005239  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.072561 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.002682  [  0/938]\n",
      "loss: 0.002968  [100/938]\n",
      "loss: 0.003827  [200/938]\n",
      "loss: 0.008734  [300/938]\n",
      "loss: 0.001670  [400/938]\n",
      "loss: 0.001612  [500/938]\n",
      "loss: 0.003212  [600/938]\n",
      "loss: 0.000537  [700/938]\n",
      "loss: 0.003554  [800/938]\n",
      "loss: 0.003071  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.072027 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.005017  [  0/938]\n",
      "loss: 0.009803  [100/938]\n",
      "loss: 0.002575  [200/938]\n",
      "loss: 0.000861  [300/938]\n",
      "loss: 0.002908  [400/938]\n",
      "loss: 0.001515  [500/938]\n",
      "loss: 0.002384  [600/938]\n",
      "loss: 0.002514  [700/938]\n",
      "loss: 0.004940  [800/938]\n",
      "loss: 0.001891  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.072536 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.005089  [  0/938]\n",
      "loss: 0.002298  [100/938]\n",
      "loss: 0.002009  [200/938]\n",
      "loss: 0.004535  [300/938]\n",
      "loss: 0.001438  [400/938]\n",
      "loss: 0.003660  [500/938]\n",
      "loss: 0.003007  [600/938]\n",
      "loss: 0.002958  [700/938]\n",
      "loss: 0.003552  [800/938]\n",
      "loss: 0.001630  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.072430 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.003550  [  0/938]\n",
      "loss: 0.000160  [100/938]\n",
      "loss: 0.002113  [200/938]\n",
      "loss: 0.000787  [300/938]\n",
      "loss: 0.002905  [400/938]\n",
      "loss: 0.002639  [500/938]\n",
      "loss: 0.002295  [600/938]\n",
      "loss: 0.000509  [700/938]\n",
      "loss: 0.001088  [800/938]\n",
      "loss: 0.005700  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.072201 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.002853  [  0/938]\n",
      "loss: 0.003939  [100/938]\n",
      "loss: 0.003321  [200/938]\n",
      "loss: 0.000730  [300/938]\n",
      "loss: 0.001524  [400/938]\n",
      "loss: 0.002399  [500/938]\n",
      "loss: 0.001883  [600/938]\n",
      "loss: 0.004174  [700/938]\n",
      "loss: 0.003997  [800/938]\n",
      "loss: 0.004786  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.072958 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.000703  [  0/938]\n",
      "loss: 0.000835  [100/938]\n",
      "loss: 0.005199  [200/938]\n",
      "loss: 0.002669  [300/938]\n",
      "loss: 0.003907  [400/938]\n",
      "loss: 0.004478  [500/938]\n",
      "loss: 0.000279  [600/938]\n",
      "loss: 0.002391  [700/938]\n",
      "loss: 0.003970  [800/938]\n",
      "loss: 0.000782  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.072627 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.001756  [  0/938]\n",
      "loss: 0.001153  [100/938]\n",
      "loss: 0.000816  [200/938]\n",
      "loss: 0.000704  [300/938]\n",
      "loss: 0.001674  [400/938]\n",
      "loss: 0.005743  [500/938]\n",
      "loss: 0.005353  [600/938]\n",
      "loss: 0.001027  [700/938]\n",
      "loss: 0.002560  [800/938]\n",
      "loss: 0.001140  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.072969 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.000513  [  0/938]\n",
      "loss: 0.000927  [100/938]\n",
      "loss: 0.000864  [200/938]\n",
      "loss: 0.002414  [300/938]\n",
      "loss: 0.004365  [400/938]\n",
      "loss: 0.004029  [500/938]\n",
      "loss: 0.001463  [600/938]\n",
      "loss: 0.002686  [700/938]\n",
      "loss: 0.002566  [800/938]\n",
      "loss: 0.002475  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.073534 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.001589  [  0/938]\n",
      "loss: 0.004646  [100/938]\n",
      "loss: 0.002455  [200/938]\n",
      "loss: 0.004080  [300/938]\n",
      "loss: 0.005552  [400/938]\n",
      "loss: 0.008910  [500/938]\n",
      "loss: 0.001092  [600/938]\n",
      "loss: 0.000939  [700/938]\n",
      "loss: 0.000598  [800/938]\n",
      "loss: 0.002396  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.073840 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.003545  [  0/938]\n",
      "loss: 0.004132  [100/938]\n",
      "loss: 0.000393  [200/938]\n",
      "loss: 0.005214  [300/938]\n",
      "loss: 0.000533  [400/938]\n",
      "loss: 0.000444  [500/938]\n",
      "loss: 0.003578  [600/938]\n",
      "loss: 0.002146  [700/938]\n",
      "loss: 0.002572  [800/938]\n",
      "loss: 0.001425  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.073619 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.001268  [  0/938]\n",
      "loss: 0.001023  [100/938]\n",
      "loss: 0.006777  [200/938]\n",
      "loss: 0.002380  [300/938]\n",
      "loss: 0.000403  [400/938]\n",
      "loss: 0.001357  [500/938]\n",
      "loss: 0.003463  [600/938]\n",
      "loss: 0.005219  [700/938]\n",
      "loss: 0.002195  [800/938]\n",
      "loss: 0.003182  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.073478 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.000544  [  0/938]\n",
      "loss: 0.001720  [100/938]\n",
      "loss: 0.001796  [200/938]\n",
      "loss: 0.000911  [300/938]\n",
      "loss: 0.001515  [400/938]\n",
      "loss: 0.002397  [500/938]\n",
      "loss: 0.000354  [600/938]\n",
      "loss: 0.000924  [700/938]\n",
      "loss: 0.000178  [800/938]\n",
      "loss: 0.002237  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.073904 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.002653  [  0/938]\n",
      "loss: 0.000886  [100/938]\n",
      "loss: 0.002982  [200/938]\n",
      "loss: 0.002198  [300/938]\n",
      "loss: 0.003504  [400/938]\n",
      "loss: 0.003785  [500/938]\n",
      "loss: 0.001401  [600/938]\n",
      "loss: 0.003394  [700/938]\n",
      "loss: 0.000605  [800/938]\n",
      "loss: 0.002566  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.073900 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.001587  [  0/938]\n",
      "loss: 0.001237  [100/938]\n",
      "loss: 0.001102  [200/938]\n",
      "loss: 0.004581  [300/938]\n",
      "loss: 0.000801  [400/938]\n",
      "loss: 0.003155  [500/938]\n",
      "loss: 0.000505  [600/938]\n",
      "loss: 0.001991  [700/938]\n",
      "loss: 0.003258  [800/938]\n",
      "loss: 0.001210  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.073614 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.000885  [  0/938]\n",
      "loss: 0.002544  [100/938]\n",
      "loss: 0.001384  [200/938]\n",
      "loss: 0.001804  [300/938]\n",
      "loss: 0.000925  [400/938]\n",
      "loss: 0.005194  [500/938]\n",
      "loss: 0.002254  [600/938]\n",
      "loss: 0.001598  [700/938]\n",
      "loss: 0.002039  [800/938]\n",
      "loss: 0.001399  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.074474 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.002608  [  0/938]\n",
      "loss: 0.002635  [100/938]\n",
      "loss: 0.001536  [200/938]\n",
      "loss: 0.000591  [300/938]\n",
      "loss: 0.001933  [400/938]\n",
      "loss: 0.001723  [500/938]\n",
      "loss: 0.001579  [600/938]\n",
      "loss: 0.002799  [700/938]\n",
      "loss: 0.004666  [800/938]\n",
      "loss: 0.001460  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.074722 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.001049  [  0/938]\n",
      "loss: 0.001226  [100/938]\n",
      "loss: 0.002278  [200/938]\n",
      "loss: 0.001866  [300/938]\n",
      "loss: 0.000404  [400/938]\n",
      "loss: 0.000526  [500/938]\n",
      "loss: 0.000539  [600/938]\n",
      "loss: 0.001822  [700/938]\n",
      "loss: 0.001322  [800/938]\n",
      "loss: 0.001526  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.074571 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.003047  [  0/938]\n",
      "loss: 0.002266  [100/938]\n",
      "loss: 0.000965  [200/938]\n",
      "loss: 0.001395  [300/938]\n",
      "loss: 0.006457  [400/938]\n",
      "loss: 0.001991  [500/938]\n",
      "loss: 0.000374  [600/938]\n",
      "loss: 0.001967  [700/938]\n",
      "loss: 0.002161  [800/938]\n",
      "loss: 0.000890  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.074926 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.002425  [  0/938]\n",
      "loss: 0.000799  [100/938]\n",
      "loss: 0.000843  [200/938]\n",
      "loss: 0.001676  [300/938]\n",
      "loss: 0.001331  [400/938]\n",
      "loss: 0.000627  [500/938]\n",
      "loss: 0.001164  [600/938]\n",
      "loss: 0.000776  [700/938]\n",
      "loss: 0.002453  [800/938]\n",
      "loss: 0.001943  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.074416 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.002448  [  0/938]\n",
      "loss: 0.002515  [100/938]\n",
      "loss: 0.000785  [200/938]\n",
      "loss: 0.002741  [300/938]\n",
      "loss: 0.001973  [400/938]\n",
      "loss: 0.001492  [500/938]\n",
      "loss: 0.001336  [600/938]\n",
      "loss: 0.001269  [700/938]\n",
      "loss: 0.001155  [800/938]\n",
      "loss: 0.003062  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.075565 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.001411  [  0/938]\n",
      "loss: 0.002695  [100/938]\n",
      "loss: 0.001661  [200/938]\n",
      "loss: 0.005115  [300/938]\n",
      "loss: 0.002313  [400/938]\n",
      "loss: 0.003690  [500/938]\n",
      "loss: 0.001209  [600/938]\n",
      "loss: 0.000730  [700/938]\n",
      "loss: 0.001056  [800/938]\n",
      "loss: 0.001533  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.075549 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.001067  [  0/938]\n",
      "loss: 0.002270  [100/938]\n",
      "loss: 0.001456  [200/938]\n",
      "loss: 0.000765  [300/938]\n",
      "loss: 0.000313  [400/938]\n",
      "loss: 0.002213  [500/938]\n",
      "loss: 0.000416  [600/938]\n",
      "loss: 0.000687  [700/938]\n",
      "loss: 0.001764  [800/938]\n",
      "loss: 0.003528  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.075490 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.000758  [  0/938]\n",
      "loss: 0.000969  [100/938]\n",
      "loss: 0.000363  [200/938]\n",
      "loss: 0.000475  [300/938]\n",
      "loss: 0.001030  [400/938]\n",
      "loss: 0.000634  [500/938]\n",
      "loss: 0.001074  [600/938]\n",
      "loss: 0.001361  [700/938]\n",
      "loss: 0.001619  [800/938]\n",
      "loss: 0.002395  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.076837 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.000867  [  0/938]\n",
      "loss: 0.001281  [100/938]\n",
      "loss: 0.001044  [200/938]\n",
      "loss: 0.001514  [300/938]\n",
      "loss: 0.000282  [400/938]\n",
      "loss: 0.001454  [500/938]\n",
      "loss: 0.000246  [600/938]\n",
      "loss: 0.001110  [700/938]\n",
      "loss: 0.000623  [800/938]\n",
      "loss: 0.000954  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.075459 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.002492  [  0/938]\n",
      "loss: 0.000563  [100/938]\n",
      "loss: 0.002647  [200/938]\n",
      "loss: 0.001607  [300/938]\n",
      "loss: 0.000896  [400/938]\n",
      "loss: 0.001006  [500/938]\n",
      "loss: 0.001308  [600/938]\n",
      "loss: 0.001210  [700/938]\n",
      "loss: 0.002916  [800/938]\n",
      "loss: 0.001584  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.075630 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.001272  [  0/938]\n",
      "loss: 0.000492  [100/938]\n",
      "loss: 0.001941  [200/938]\n",
      "loss: 0.001561  [300/938]\n",
      "loss: 0.002339  [400/938]\n",
      "loss: 0.000858  [500/938]\n",
      "loss: 0.008248  [600/938]\n",
      "loss: 0.003620  [700/938]\n",
      "loss: 0.000694  [800/938]\n",
      "loss: 0.001064  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.076801 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.001744  [  0/938]\n",
      "loss: 0.001097  [100/938]\n",
      "loss: 0.000840  [200/938]\n",
      "loss: 0.002612  [300/938]\n",
      "loss: 0.000893  [400/938]\n",
      "loss: 0.000787  [500/938]\n",
      "loss: 0.002969  [600/938]\n",
      "loss: 0.001206  [700/938]\n",
      "loss: 0.000347  [800/938]\n",
      "loss: 0.001567  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.075634 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.001409  [  0/938]\n",
      "loss: 0.000529  [100/938]\n",
      "loss: 0.000336  [200/938]\n",
      "loss: 0.003869  [300/938]\n",
      "loss: 0.000484  [400/938]\n",
      "loss: 0.003033  [500/938]\n",
      "loss: 0.001695  [600/938]\n",
      "loss: 0.002003  [700/938]\n",
      "loss: 0.001556  [800/938]\n",
      "loss: 0.001031  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.078291 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.002053  [  0/938]\n",
      "loss: 0.001853  [100/938]\n",
      "loss: 0.000537  [200/938]\n",
      "loss: 0.001972  [300/938]\n",
      "loss: 0.002453  [400/938]\n",
      "loss: 0.000903  [500/938]\n",
      "loss: 0.001921  [600/938]\n",
      "loss: 0.001600  [700/938]\n",
      "loss: 0.001517  [800/938]\n",
      "loss: 0.001344  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.078315 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.000601  [  0/938]\n",
      "loss: 0.000225  [100/938]\n",
      "loss: 0.000834  [200/938]\n",
      "loss: 0.002815  [300/938]\n",
      "loss: 0.000476  [400/938]\n",
      "loss: 0.000619  [500/938]\n",
      "loss: 0.000639  [600/938]\n",
      "loss: 0.000729  [700/938]\n",
      "loss: 0.001532  [800/938]\n",
      "loss: 0.001599  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.076212 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.000301  [  0/938]\n",
      "loss: 0.000765  [100/938]\n",
      "loss: 0.001005  [200/938]\n",
      "loss: 0.000657  [300/938]\n",
      "loss: 0.001662  [400/938]\n",
      "loss: 0.001521  [500/938]\n",
      "loss: 0.002261  [600/938]\n",
      "loss: 0.000699  [700/938]\n",
      "loss: 0.000658  [800/938]\n",
      "loss: 0.001087  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.076423 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.001038  [  0/938]\n",
      "loss: 0.001927  [100/938]\n",
      "loss: 0.002003  [200/938]\n",
      "loss: 0.000289  [300/938]\n",
      "loss: 0.003268  [400/938]\n",
      "loss: 0.001550  [500/938]\n",
      "loss: 0.001395  [600/938]\n",
      "loss: 0.001103  [700/938]\n",
      "loss: 0.002932  [800/938]\n",
      "loss: 0.001132  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.076856 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.001493  [  0/938]\n",
      "loss: 0.002150  [100/938]\n",
      "loss: 0.002538  [200/938]\n",
      "loss: 0.003186  [300/938]\n",
      "loss: 0.001634  [400/938]\n",
      "loss: 0.000968  [500/938]\n",
      "loss: 0.002020  [600/938]\n",
      "loss: 0.002023  [700/938]\n",
      "loss: 0.001634  [800/938]\n",
      "loss: 0.001510  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.076694 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.001292  [  0/938]\n",
      "loss: 0.003644  [100/938]\n",
      "loss: 0.000969  [200/938]\n",
      "loss: 0.002446  [300/938]\n",
      "loss: 0.001148  [400/938]\n",
      "loss: 0.001573  [500/938]\n",
      "loss: 0.002429  [600/938]\n",
      "loss: 0.000614  [700/938]\n",
      "loss: 0.001987  [800/938]\n",
      "loss: 0.000721  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.076962 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.000367  [  0/938]\n",
      "loss: 0.001538  [100/938]\n",
      "loss: 0.002456  [200/938]\n",
      "loss: 0.000368  [300/938]\n",
      "loss: 0.001215  [400/938]\n",
      "loss: 0.000340  [500/938]\n",
      "loss: 0.000662  [600/938]\n",
      "loss: 0.000691  [700/938]\n",
      "loss: 0.002509  [800/938]\n",
      "loss: 0.002853  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.076766 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.002842  [  0/938]\n",
      "loss: 0.003579  [100/938]\n",
      "loss: 0.001870  [200/938]\n",
      "loss: 0.001208  [300/938]\n",
      "loss: 0.001491  [400/938]\n",
      "loss: 0.000668  [500/938]\n",
      "loss: 0.001134  [600/938]\n",
      "loss: 0.002404  [700/938]\n",
      "loss: 0.002142  [800/938]\n",
      "loss: 0.000366  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.076934 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.000254  [  0/938]\n",
      "loss: 0.002061  [100/938]\n",
      "loss: 0.001236  [200/938]\n",
      "loss: 0.001513  [300/938]\n",
      "loss: 0.001691  [400/938]\n",
      "loss: 0.001472  [500/938]\n",
      "loss: 0.002095  [600/938]\n",
      "loss: 0.001081  [700/938]\n",
      "loss: 0.001412  [800/938]\n",
      "loss: 0.001266  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.077337 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.003316  [  0/938]\n",
      "loss: 0.001415  [100/938]\n",
      "loss: 0.000635  [200/938]\n",
      "loss: 0.001420  [300/938]\n",
      "loss: 0.001476  [400/938]\n",
      "loss: 0.001717  [500/938]\n",
      "loss: 0.001342  [600/938]\n",
      "loss: 0.000286  [700/938]\n",
      "loss: 0.001012  [800/938]\n",
      "loss: 0.002165  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.077325 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.001330  [  0/938]\n",
      "loss: 0.000766  [100/938]\n",
      "loss: 0.000857  [200/938]\n",
      "loss: 0.002007  [300/938]\n",
      "loss: 0.001220  [400/938]\n",
      "loss: 0.001076  [500/938]\n",
      "loss: 0.000774  [600/938]\n",
      "loss: 0.001526  [700/938]\n",
      "loss: 0.002795  [800/938]\n",
      "loss: 0.001188  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.077571 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.000118  [  0/938]\n",
      "loss: 0.000455  [100/938]\n",
      "loss: 0.001235  [200/938]\n",
      "loss: 0.000492  [300/938]\n",
      "loss: 0.002641  [400/938]\n",
      "loss: 0.000869  [500/938]\n",
      "loss: 0.000819  [600/938]\n",
      "loss: 0.001091  [700/938]\n",
      "loss: 0.001441  [800/938]\n",
      "loss: 0.001098  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.077498 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.000907  [  0/938]\n",
      "loss: 0.001707  [100/938]\n",
      "loss: 0.000842  [200/938]\n",
      "loss: 0.000793  [300/938]\n",
      "loss: 0.000766  [400/938]\n",
      "loss: 0.001744  [500/938]\n",
      "loss: 0.000373  [600/938]\n",
      "loss: 0.001188  [700/938]\n",
      "loss: 0.000484  [800/938]\n",
      "loss: 0.001155  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.077827 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.000562  [  0/938]\n",
      "loss: 0.001016  [100/938]\n",
      "loss: 0.002327  [200/938]\n",
      "loss: 0.001104  [300/938]\n",
      "loss: 0.001584  [400/938]\n",
      "loss: 0.002039  [500/938]\n",
      "loss: 0.001468  [600/938]\n",
      "loss: 0.001765  [700/938]\n",
      "loss: 0.000903  [800/938]\n",
      "loss: 0.000311  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.079878 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.001481  [  0/938]\n",
      "loss: 0.001446  [100/938]\n",
      "loss: 0.001399  [200/938]\n",
      "loss: 0.000778  [300/938]\n",
      "loss: 0.000778  [400/938]\n",
      "loss: 0.001120  [500/938]\n",
      "loss: 0.000446  [600/938]\n",
      "loss: 0.000229  [700/938]\n",
      "loss: 0.001597  [800/938]\n",
      "loss: 0.001329  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.077882 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.000704  [  0/938]\n",
      "loss: 0.001618  [100/938]\n",
      "loss: 0.001555  [200/938]\n",
      "loss: 0.000916  [300/938]\n",
      "loss: 0.000984  [400/938]\n",
      "loss: 0.000568  [500/938]\n",
      "loss: 0.000807  [600/938]\n",
      "loss: 0.000630  [700/938]\n",
      "loss: 0.001654  [800/938]\n",
      "loss: 0.001334  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.078315 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.000383  [  0/938]\n",
      "loss: 0.000670  [100/938]\n",
      "loss: 0.002502  [200/938]\n",
      "loss: 0.001073  [300/938]\n",
      "loss: 0.000246  [400/938]\n",
      "loss: 0.000770  [500/938]\n",
      "loss: 0.001390  [600/938]\n",
      "loss: 0.000990  [700/938]\n",
      "loss: 0.001496  [800/938]\n",
      "loss: 0.001519  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.081197 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.000690  [  0/938]\n",
      "loss: 0.000225  [100/938]\n",
      "loss: 0.001158  [200/938]\n",
      "loss: 0.001065  [300/938]\n",
      "loss: 0.001084  [400/938]\n",
      "loss: 0.000701  [500/938]\n",
      "loss: 0.002351  [600/938]\n",
      "loss: 0.001404  [700/938]\n",
      "loss: 0.001549  [800/938]\n",
      "loss: 0.000922  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.078197 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.000660  [  0/938]\n",
      "loss: 0.001952  [100/938]\n",
      "loss: 0.000687  [200/938]\n",
      "loss: 0.000604  [300/938]\n",
      "loss: 0.000506  [400/938]\n",
      "loss: 0.001234  [500/938]\n",
      "loss: 0.003367  [600/938]\n",
      "loss: 0.001093  [700/938]\n",
      "loss: 0.000338  [800/938]\n",
      "loss: 0.000877  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.078187 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.000196  [  0/938]\n",
      "loss: 0.001337  [100/938]\n",
      "loss: 0.001167  [200/938]\n",
      "loss: 0.000951  [300/938]\n",
      "loss: 0.001082  [400/938]\n",
      "loss: 0.000035  [500/938]\n",
      "loss: 0.000655  [600/938]\n",
      "loss: 0.001830  [700/938]\n",
      "loss: 0.000296  [800/938]\n",
      "loss: 0.001553  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.078766 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.000272  [  0/938]\n",
      "loss: 0.001350  [100/938]\n",
      "loss: 0.001242  [200/938]\n",
      "loss: 0.001269  [300/938]\n",
      "loss: 0.001588  [400/938]\n",
      "loss: 0.001082  [500/938]\n",
      "loss: 0.000872  [600/938]\n",
      "loss: 0.000943  [700/938]\n",
      "loss: 0.001492  [800/938]\n",
      "loss: 0.000611  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.078668 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.001743  [  0/938]\n",
      "loss: 0.001356  [100/938]\n",
      "loss: 0.001469  [200/938]\n",
      "loss: 0.001735  [300/938]\n",
      "loss: 0.000461  [400/938]\n",
      "loss: 0.000627  [500/938]\n",
      "loss: 0.001386  [600/938]\n",
      "loss: 0.000296  [700/938]\n",
      "loss: 0.000728  [800/938]\n",
      "loss: 0.000529  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.078720 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.001448  [  0/938]\n",
      "loss: 0.000349  [100/938]\n",
      "loss: 0.000310  [200/938]\n",
      "loss: 0.000958  [300/938]\n",
      "loss: 0.000407  [400/938]\n",
      "loss: 0.000514  [500/938]\n",
      "loss: 0.000359  [600/938]\n",
      "loss: 0.000327  [700/938]\n",
      "loss: 0.000937  [800/938]\n",
      "loss: 0.000491  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.079350 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.000411  [  0/938]\n",
      "loss: 0.000720  [100/938]\n",
      "loss: 0.000902  [200/938]\n",
      "loss: 0.000689  [300/938]\n",
      "loss: 0.002439  [400/938]\n",
      "loss: 0.001598  [500/938]\n",
      "loss: 0.000711  [600/938]\n",
      "loss: 0.000985  [700/938]\n",
      "loss: 0.000951  [800/938]\n",
      "loss: 0.002091  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.078896 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.001183  [  0/938]\n",
      "loss: 0.001266  [100/938]\n",
      "loss: 0.000224  [200/938]\n",
      "loss: 0.000911  [300/938]\n",
      "loss: 0.001051  [400/938]\n",
      "loss: 0.000617  [500/938]\n",
      "loss: 0.000536  [600/938]\n",
      "loss: 0.001602  [700/938]\n",
      "loss: 0.000771  [800/938]\n",
      "loss: 0.001512  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.078916 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.001096  [  0/938]\n",
      "loss: 0.000287  [100/938]\n",
      "loss: 0.000952  [200/938]\n",
      "loss: 0.000681  [300/938]\n",
      "loss: 0.000611  [400/938]\n",
      "loss: 0.000245  [500/938]\n",
      "loss: 0.000590  [600/938]\n",
      "loss: 0.000363  [700/938]\n",
      "loss: 0.000788  [800/938]\n",
      "loss: 0.001862  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.080747 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.000626  [  0/938]\n",
      "loss: 0.001259  [100/938]\n",
      "loss: 0.000713  [200/938]\n",
      "loss: 0.000568  [300/938]\n",
      "loss: 0.001297  [400/938]\n",
      "loss: 0.000457  [500/938]\n",
      "loss: 0.000177  [600/938]\n",
      "loss: 0.000925  [700/938]\n",
      "loss: 0.000943  [800/938]\n",
      "loss: 0.001696  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.079963 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.001119  [  0/938]\n",
      "loss: 0.000143  [100/938]\n",
      "loss: 0.000687  [200/938]\n",
      "loss: 0.000766  [300/938]\n",
      "loss: 0.000983  [400/938]\n",
      "loss: 0.000043  [500/938]\n",
      "loss: 0.000949  [600/938]\n",
      "loss: 0.001507  [700/938]\n",
      "loss: 0.002413  [800/938]\n",
      "loss: 0.001218  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.079360 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.001151  [  0/938]\n",
      "loss: 0.000710  [100/938]\n",
      "loss: 0.000928  [200/938]\n",
      "loss: 0.001818  [300/938]\n",
      "loss: 0.001214  [400/938]\n",
      "loss: 0.000756  [500/938]\n",
      "loss: 0.001264  [600/938]\n",
      "loss: 0.001241  [700/938]\n",
      "loss: 0.000796  [800/938]\n",
      "loss: 0.001186  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.080603 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.000943  [  0/938]\n",
      "loss: 0.000684  [100/938]\n",
      "loss: 0.000510  [200/938]\n",
      "loss: 0.000521  [300/938]\n",
      "loss: 0.001531  [400/938]\n",
      "loss: 0.000878  [500/938]\n",
      "loss: 0.000278  [600/938]\n",
      "loss: 0.000812  [700/938]\n",
      "loss: 0.000274  [800/938]\n",
      "loss: 0.000428  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.079421 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.000947  [  0/938]\n",
      "loss: 0.000570  [100/938]\n",
      "loss: 0.000306  [200/938]\n",
      "loss: 0.000389  [300/938]\n",
      "loss: 0.000814  [400/938]\n",
      "loss: 0.000946  [500/938]\n",
      "loss: 0.001665  [600/938]\n",
      "loss: 0.000102  [700/938]\n",
      "loss: 0.000596  [800/938]\n",
      "loss: 0.000366  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.079402 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.001056  [  0/938]\n",
      "loss: 0.000669  [100/938]\n",
      "loss: 0.000597  [200/938]\n",
      "loss: 0.001723  [300/938]\n",
      "loss: 0.001934  [400/938]\n",
      "loss: 0.000429  [500/938]\n",
      "loss: 0.000219  [600/938]\n",
      "loss: 0.000944  [700/938]\n",
      "loss: 0.001507  [800/938]\n",
      "loss: 0.000348  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.079717 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.000455  [  0/938]\n",
      "loss: 0.001307  [100/938]\n",
      "loss: 0.001299  [200/938]\n",
      "loss: 0.000729  [300/938]\n",
      "loss: 0.000796  [400/938]\n",
      "loss: 0.001546  [500/938]\n",
      "loss: 0.000121  [600/938]\n",
      "loss: 0.000638  [700/938]\n",
      "loss: 0.000157  [800/938]\n",
      "loss: 0.001220  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.079956 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.000205  [  0/938]\n",
      "loss: 0.001156  [100/938]\n",
      "loss: 0.000811  [200/938]\n",
      "loss: 0.000803  [300/938]\n",
      "loss: 0.000916  [400/938]\n",
      "loss: 0.000466  [500/938]\n",
      "loss: 0.000362  [600/938]\n",
      "loss: 0.001898  [700/938]\n",
      "loss: 0.001820  [800/938]\n",
      "loss: 0.000769  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.080884 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.001074  [  0/938]\n",
      "loss: 0.001470  [100/938]\n",
      "loss: 0.000599  [200/938]\n",
      "loss: 0.000675  [300/938]\n",
      "loss: 0.000924  [400/938]\n",
      "loss: 0.000806  [500/938]\n",
      "loss: 0.000270  [600/938]\n",
      "loss: 0.002298  [700/938]\n",
      "loss: 0.000502  [800/938]\n",
      "loss: 0.001239  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.080180 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.001677  [  0/938]\n",
      "loss: 0.003447  [100/938]\n",
      "loss: 0.000334  [200/938]\n",
      "loss: 0.001321  [300/938]\n",
      "loss: 0.000443  [400/938]\n",
      "loss: 0.001005  [500/938]\n",
      "loss: 0.000957  [600/938]\n",
      "loss: 0.000646  [700/938]\n",
      "loss: 0.000301  [800/938]\n",
      "loss: 0.001017  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.080351 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.001388  [  0/938]\n",
      "loss: 0.000340  [100/938]\n",
      "loss: 0.000750  [200/938]\n",
      "loss: 0.001199  [300/938]\n",
      "loss: 0.000610  [400/938]\n",
      "loss: 0.000508  [500/938]\n",
      "loss: 0.000459  [600/938]\n",
      "loss: 0.000489  [700/938]\n",
      "loss: 0.001218  [800/938]\n",
      "loss: 0.000777  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.079991 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.000981  [  0/938]\n",
      "loss: 0.000273  [100/938]\n",
      "loss: 0.001529  [200/938]\n",
      "loss: 0.000613  [300/938]\n",
      "loss: 0.001373  [400/938]\n",
      "loss: 0.000783  [500/938]\n",
      "loss: 0.000964  [600/938]\n",
      "loss: 0.001137  [700/938]\n",
      "loss: 0.000168  [800/938]\n",
      "loss: 0.000356  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.081097 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.000399  [  0/938]\n",
      "loss: 0.001064  [100/938]\n",
      "loss: 0.001193  [200/938]\n",
      "loss: 0.000996  [300/938]\n",
      "loss: 0.000863  [400/938]\n",
      "loss: 0.001368  [500/938]\n",
      "loss: 0.000500  [600/938]\n",
      "loss: 0.001550  [700/938]\n",
      "loss: 0.000475  [800/938]\n",
      "loss: 0.000640  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.080518 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.000930  [  0/938]\n",
      "loss: 0.000586  [100/938]\n",
      "loss: 0.000406  [200/938]\n",
      "loss: 0.000531  [300/938]\n",
      "loss: 0.001209  [400/938]\n",
      "loss: 0.001112  [500/938]\n",
      "loss: 0.000416  [600/938]\n",
      "loss: 0.000515  [700/938]\n",
      "loss: 0.000364  [800/938]\n",
      "loss: 0.000991  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.081416 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.000760  [  0/938]\n",
      "loss: 0.000492  [100/938]\n",
      "loss: 0.000490  [200/938]\n",
      "loss: 0.000150  [300/938]\n",
      "loss: 0.000558  [400/938]\n",
      "loss: 0.000606  [500/938]\n",
      "loss: 0.000715  [600/938]\n",
      "loss: 0.000206  [700/938]\n",
      "loss: 0.000311  [800/938]\n",
      "loss: 0.000486  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.080459 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.000585  [  0/938]\n",
      "loss: 0.001582  [100/938]\n",
      "loss: 0.000283  [200/938]\n",
      "loss: 0.000289  [300/938]\n",
      "loss: 0.001380  [400/938]\n",
      "loss: 0.000387  [500/938]\n",
      "loss: 0.000802  [600/938]\n",
      "loss: 0.000820  [700/938]\n",
      "loss: 0.000925  [800/938]\n",
      "loss: 0.000740  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.080633 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.001012  [  0/938]\n",
      "loss: 0.000265  [100/938]\n",
      "loss: 0.000501  [200/938]\n",
      "loss: 0.000654  [300/938]\n",
      "loss: 0.000304  [400/938]\n",
      "loss: 0.001849  [500/938]\n",
      "loss: 0.001261  [600/938]\n",
      "loss: 0.000978  [700/938]\n",
      "loss: 0.000452  [800/938]\n",
      "loss: 0.000791  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.080714 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.000813  [  0/938]\n",
      "loss: 0.000229  [100/938]\n",
      "loss: 0.000331  [200/938]\n",
      "loss: 0.001481  [300/938]\n",
      "loss: 0.000857  [400/938]\n",
      "loss: 0.001352  [500/938]\n",
      "loss: 0.000912  [600/938]\n",
      "loss: 0.001075  [700/938]\n",
      "loss: 0.000857  [800/938]\n",
      "loss: 0.000427  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.080887 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.001325  [  0/938]\n",
      "loss: 0.000817  [100/938]\n",
      "loss: 0.001351  [200/938]\n",
      "loss: 0.001023  [300/938]\n",
      "loss: 0.001395  [400/938]\n",
      "loss: 0.000693  [500/938]\n",
      "loss: 0.001235  [600/938]\n",
      "loss: 0.001399  [700/938]\n",
      "loss: 0.000265  [800/938]\n",
      "loss: 0.002408  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.081924 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.000816  [  0/938]\n",
      "loss: 0.000146  [100/938]\n",
      "loss: 0.000228  [200/938]\n",
      "loss: 0.000868  [300/938]\n",
      "loss: 0.001393  [400/938]\n",
      "loss: 0.000839  [500/938]\n",
      "loss: 0.000681  [600/938]\n",
      "loss: 0.001442  [700/938]\n",
      "loss: 0.000616  [800/938]\n",
      "loss: 0.000565  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.080911 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.000615  [  0/938]\n",
      "loss: 0.000489  [100/938]\n",
      "loss: 0.000485  [200/938]\n",
      "loss: 0.000263  [300/938]\n",
      "loss: 0.001200  [400/938]\n",
      "loss: 0.000506  [500/938]\n",
      "loss: 0.000155  [600/938]\n",
      "loss: 0.000753  [700/938]\n",
      "loss: 0.000891  [800/938]\n",
      "loss: 0.001462  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.081409 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.000215  [  0/938]\n",
      "loss: 0.001025  [100/938]\n",
      "loss: 0.000633  [200/938]\n",
      "loss: 0.001155  [300/938]\n",
      "loss: 0.001276  [400/938]\n",
      "loss: 0.000377  [500/938]\n",
      "loss: 0.000352  [600/938]\n",
      "loss: 0.002169  [700/938]\n",
      "loss: 0.000062  [800/938]\n",
      "loss: 0.000309  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.085691 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.000155  [  0/938]\n",
      "loss: 0.000697  [100/938]\n",
      "loss: 0.000459  [200/938]\n",
      "loss: 0.001250  [300/938]\n",
      "loss: 0.001021  [400/938]\n",
      "loss: 0.001214  [500/938]\n",
      "loss: 0.000077  [600/938]\n",
      "loss: 0.000799  [700/938]\n",
      "loss: 0.000702  [800/938]\n",
      "loss: 0.000223  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.082191 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.000383  [  0/938]\n",
      "loss: 0.000439  [100/938]\n",
      "loss: 0.000574  [200/938]\n",
      "loss: 0.000548  [300/938]\n",
      "loss: 0.000659  [400/938]\n",
      "loss: 0.000213  [500/938]\n",
      "loss: 0.000415  [600/938]\n",
      "loss: 0.000495  [700/938]\n",
      "loss: 0.000958  [800/938]\n",
      "loss: 0.001095  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.081774 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.000755  [  0/938]\n",
      "loss: 0.001657  [100/938]\n",
      "loss: 0.000844  [200/938]\n",
      "loss: 0.000295  [300/938]\n",
      "loss: 0.000152  [400/938]\n",
      "loss: 0.000569  [500/938]\n",
      "loss: 0.000776  [600/938]\n",
      "loss: 0.000759  [700/938]\n",
      "loss: 0.000729  [800/938]\n",
      "loss: 0.000160  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.081681 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.000055  [  0/938]\n",
      "loss: 0.000506  [100/938]\n",
      "loss: 0.000405  [200/938]\n",
      "loss: 0.000814  [300/938]\n",
      "loss: 0.000443  [400/938]\n",
      "loss: 0.000704  [500/938]\n",
      "loss: 0.000493  [600/938]\n",
      "loss: 0.001072  [700/938]\n",
      "loss: 0.000657  [800/938]\n",
      "loss: 0.001295  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.081617 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.000217  [  0/938]\n",
      "loss: 0.001559  [100/938]\n",
      "loss: 0.000525  [200/938]\n",
      "loss: 0.000261  [300/938]\n",
      "loss: 0.000746  [400/938]\n",
      "loss: 0.000632  [500/938]\n",
      "loss: 0.000598  [600/938]\n",
      "loss: 0.000418  [700/938]\n",
      "loss: 0.000609  [800/938]\n",
      "loss: 0.000414  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.081852 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.001172  [  0/938]\n",
      "loss: 0.000699  [100/938]\n",
      "loss: 0.001083  [200/938]\n",
      "loss: 0.001368  [300/938]\n",
      "loss: 0.000787  [400/938]\n",
      "loss: 0.000493  [500/938]\n",
      "loss: 0.001149  [600/938]\n",
      "loss: 0.000584  [700/938]\n",
      "loss: 0.001980  [800/938]\n",
      "loss: 0.000560  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.082529 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.000297  [  0/938]\n",
      "loss: 0.000606  [100/938]\n",
      "loss: 0.001010  [200/938]\n",
      "loss: 0.000779  [300/938]\n",
      "loss: 0.000237  [400/938]\n",
      "loss: 0.000313  [500/938]\n",
      "loss: 0.000452  [600/938]\n",
      "loss: 0.001028  [700/938]\n",
      "loss: 0.000670  [800/938]\n",
      "loss: 0.000380  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.081820 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.000528  [  0/938]\n",
      "loss: 0.001036  [100/938]\n",
      "loss: 0.000377  [200/938]\n",
      "loss: 0.000185  [300/938]\n",
      "loss: 0.000399  [400/938]\n",
      "loss: 0.000953  [500/938]\n",
      "loss: 0.000549  [600/938]\n",
      "loss: 0.000626  [700/938]\n",
      "loss: 0.000677  [800/938]\n",
      "loss: 0.000388  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.082636 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.001841  [  0/938]\n",
      "loss: 0.001204  [100/938]\n",
      "loss: 0.001205  [200/938]\n",
      "loss: 0.000251  [300/938]\n",
      "loss: 0.000335  [400/938]\n",
      "loss: 0.000598  [500/938]\n",
      "loss: 0.000324  [600/938]\n",
      "loss: 0.001090  [700/938]\n",
      "loss: 0.000187  [800/938]\n",
      "loss: 0.000470  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.082039 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.000292  [  0/938]\n",
      "loss: 0.000635  [100/938]\n",
      "loss: 0.000484  [200/938]\n",
      "loss: 0.000314  [300/938]\n",
      "loss: 0.000808  [400/938]\n",
      "loss: 0.001472  [500/938]\n",
      "loss: 0.000301  [600/938]\n",
      "loss: 0.000371  [700/938]\n",
      "loss: 0.000748  [800/938]\n",
      "loss: 0.000772  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.082245 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.000694  [  0/938]\n",
      "loss: 0.000558  [100/938]\n",
      "loss: 0.000873  [200/938]\n",
      "loss: 0.000431  [300/938]\n",
      "loss: 0.000268  [400/938]\n",
      "loss: 0.000146  [500/938]\n",
      "loss: 0.000541  [600/938]\n",
      "loss: 0.000533  [700/938]\n",
      "loss: 0.000085  [800/938]\n",
      "loss: 0.000368  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.082413 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.000202  [  0/938]\n",
      "loss: 0.000764  [100/938]\n",
      "loss: 0.000663  [200/938]\n",
      "loss: 0.000772  [300/938]\n",
      "loss: 0.000883  [400/938]\n",
      "loss: 0.001158  [500/938]\n",
      "loss: 0.000737  [600/938]\n",
      "loss: 0.000744  [700/938]\n",
      "loss: 0.001050  [800/938]\n",
      "loss: 0.000733  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.082304 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.000560  [  0/938]\n",
      "loss: 0.000991  [100/938]\n",
      "loss: 0.000675  [200/938]\n",
      "loss: 0.000270  [300/938]\n",
      "loss: 0.000861  [400/938]\n",
      "loss: 0.000181  [500/938]\n",
      "loss: 0.000468  [600/938]\n",
      "loss: 0.000301  [700/938]\n",
      "loss: 0.000483  [800/938]\n",
      "loss: 0.000916  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.082500 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.000270  [  0/938]\n",
      "loss: 0.000566  [100/938]\n",
      "loss: 0.000533  [200/938]\n",
      "loss: 0.000770  [300/938]\n",
      "loss: 0.000388  [400/938]\n",
      "loss: 0.000589  [500/938]\n",
      "loss: 0.000452  [600/938]\n",
      "loss: 0.000563  [700/938]\n",
      "loss: 0.000423  [800/938]\n",
      "loss: 0.000106  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.082368 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.000495  [  0/938]\n",
      "loss: 0.000529  [100/938]\n",
      "loss: 0.001462  [200/938]\n",
      "loss: 0.000150  [300/938]\n",
      "loss: 0.000166  [400/938]\n",
      "loss: 0.000435  [500/938]\n",
      "loss: 0.000401  [600/938]\n",
      "loss: 0.000837  [700/938]\n",
      "loss: 0.000053  [800/938]\n",
      "loss: 0.000591  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.082542 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.000401  [  0/938]\n",
      "loss: 0.000314  [100/938]\n",
      "loss: 0.000273  [200/938]\n",
      "loss: 0.000577  [300/938]\n",
      "loss: 0.000098  [400/938]\n",
      "loss: 0.000523  [500/938]\n",
      "loss: 0.001183  [600/938]\n",
      "loss: 0.002000  [700/938]\n",
      "loss: 0.000066  [800/938]\n",
      "loss: 0.000390  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.082678 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.000355  [  0/938]\n",
      "loss: 0.000428  [100/938]\n",
      "loss: 0.000846  [200/938]\n",
      "loss: 0.001861  [300/938]\n",
      "loss: 0.001487  [400/938]\n",
      "loss: 0.001296  [500/938]\n",
      "loss: 0.000740  [600/938]\n",
      "loss: 0.000693  [700/938]\n",
      "loss: 0.000225  [800/938]\n",
      "loss: 0.000279  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.082647 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.000656  [  0/938]\n",
      "loss: 0.000401  [100/938]\n",
      "loss: 0.001614  [200/938]\n",
      "loss: 0.000085  [300/938]\n",
      "loss: 0.001032  [400/938]\n",
      "loss: 0.000755  [500/938]\n",
      "loss: 0.000853  [600/938]\n",
      "loss: 0.000484  [700/938]\n",
      "loss: 0.000328  [800/938]\n",
      "loss: 0.000236  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.084018 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.000700  [  0/938]\n",
      "loss: 0.000269  [100/938]\n",
      "loss: 0.000122  [200/938]\n",
      "loss: 0.000448  [300/938]\n",
      "loss: 0.000837  [400/938]\n",
      "loss: 0.000950  [500/938]\n",
      "loss: 0.000232  [600/938]\n",
      "loss: 0.000111  [700/938]\n",
      "loss: 0.001308  [800/938]\n",
      "loss: 0.000456  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.082833 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.000404  [  0/938]\n",
      "loss: 0.000618  [100/938]\n",
      "loss: 0.001073  [200/938]\n",
      "loss: 0.001293  [300/938]\n",
      "loss: 0.000285  [400/938]\n",
      "loss: 0.000750  [500/938]\n",
      "loss: 0.000424  [600/938]\n",
      "loss: 0.000362  [700/938]\n",
      "loss: 0.000679  [800/938]\n",
      "loss: 0.000387  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.083054 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.000392  [  0/938]\n",
      "loss: 0.000725  [100/938]\n",
      "loss: 0.000515  [200/938]\n",
      "loss: 0.000116  [300/938]\n",
      "loss: 0.000493  [400/938]\n",
      "loss: 0.000066  [500/938]\n",
      "loss: 0.001214  [600/938]\n",
      "loss: 0.000847  [700/938]\n",
      "loss: 0.000602  [800/938]\n",
      "loss: 0.000356  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.083348 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.000265  [  0/938]\n",
      "loss: 0.000434  [100/938]\n",
      "loss: 0.000528  [200/938]\n",
      "loss: 0.000763  [300/938]\n",
      "loss: 0.001070  [400/938]\n",
      "loss: 0.000372  [500/938]\n",
      "loss: 0.000504  [600/938]\n",
      "loss: 0.000107  [700/938]\n",
      "loss: 0.000358  [800/938]\n",
      "loss: 0.000564  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.083183 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.000424  [  0/938]\n",
      "loss: 0.000876  [100/938]\n",
      "loss: 0.000345  [200/938]\n",
      "loss: 0.000398  [300/938]\n",
      "loss: 0.001791  [400/938]\n",
      "loss: 0.000306  [500/938]\n",
      "loss: 0.000663  [600/938]\n",
      "loss: 0.000332  [700/938]\n",
      "loss: 0.000587  [800/938]\n",
      "loss: 0.000452  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.083100 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.000753  [  0/938]\n",
      "loss: 0.000605  [100/938]\n",
      "loss: 0.001204  [200/938]\n",
      "loss: 0.000806  [300/938]\n",
      "loss: 0.000747  [400/938]\n",
      "loss: 0.000322  [500/938]\n",
      "loss: 0.001149  [600/938]\n",
      "loss: 0.000388  [700/938]\n",
      "loss: 0.000877  [800/938]\n",
      "loss: 0.000631  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.082990 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.000898  [  0/938]\n",
      "loss: 0.000077  [100/938]\n",
      "loss: 0.000596  [200/938]\n",
      "loss: 0.000084  [300/938]\n",
      "loss: 0.000590  [400/938]\n",
      "loss: 0.000098  [500/938]\n",
      "loss: 0.001465  [600/938]\n",
      "loss: 0.000091  [700/938]\n",
      "loss: 0.000409  [800/938]\n",
      "loss: 0.000774  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.083360 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.000677  [  0/938]\n",
      "loss: 0.000022  [100/938]\n",
      "loss: 0.000987  [200/938]\n",
      "loss: 0.001981  [300/938]\n",
      "loss: 0.000777  [400/938]\n",
      "loss: 0.000348  [500/938]\n",
      "loss: 0.000097  [600/938]\n",
      "loss: 0.000304  [700/938]\n",
      "loss: 0.000125  [800/938]\n",
      "loss: 0.000861  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.083379 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.000620  [  0/938]\n",
      "loss: 0.000564  [100/938]\n",
      "loss: 0.000218  [200/938]\n",
      "loss: 0.001005  [300/938]\n",
      "loss: 0.000768  [400/938]\n",
      "loss: 0.000597  [500/938]\n",
      "loss: 0.000745  [600/938]\n",
      "loss: 0.000654  [700/938]\n",
      "loss: 0.000388  [800/938]\n",
      "loss: 0.000643  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.083445 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.000520  [  0/938]\n",
      "loss: 0.000108  [100/938]\n",
      "loss: 0.000420  [200/938]\n",
      "loss: 0.001564  [300/938]\n",
      "loss: 0.000809  [400/938]\n",
      "loss: 0.000531  [500/938]\n",
      "loss: 0.000033  [600/938]\n",
      "loss: 0.000599  [700/938]\n",
      "loss: 0.000570  [800/938]\n",
      "loss: 0.000614  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.083531 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.000285  [  0/938]\n",
      "loss: 0.000610  [100/938]\n",
      "loss: 0.001641  [200/938]\n",
      "loss: 0.001007  [300/938]\n",
      "loss: 0.000524  [400/938]\n",
      "loss: 0.000627  [500/938]\n",
      "loss: 0.000917  [600/938]\n",
      "loss: 0.000233  [700/938]\n",
      "loss: 0.000782  [800/938]\n",
      "loss: 0.000425  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.083936 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.000027  [  0/938]\n",
      "loss: 0.000089  [100/938]\n",
      "loss: 0.001570  [200/938]\n",
      "loss: 0.000199  [300/938]\n",
      "loss: 0.000539  [400/938]\n",
      "loss: 0.000990  [500/938]\n",
      "loss: 0.000261  [600/938]\n",
      "loss: 0.001005  [700/938]\n",
      "loss: 0.000410  [800/938]\n",
      "loss: 0.000083  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.085082 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.000616  [  0/938]\n",
      "loss: 0.000522  [100/938]\n",
      "loss: 0.000168  [200/938]\n",
      "loss: 0.000975  [300/938]\n",
      "loss: 0.000602  [400/938]\n",
      "loss: 0.000689  [500/938]\n",
      "loss: 0.000388  [600/938]\n",
      "loss: 0.000822  [700/938]\n",
      "loss: 0.000719  [800/938]\n",
      "loss: 0.000277  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.083627 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.000333  [  0/938]\n",
      "loss: 0.000935  [100/938]\n",
      "loss: 0.000393  [200/938]\n",
      "loss: 0.000549  [300/938]\n",
      "loss: 0.000135  [400/938]\n",
      "loss: 0.000886  [500/938]\n",
      "loss: 0.000178  [600/938]\n",
      "loss: 0.000214  [700/938]\n",
      "loss: 0.000282  [800/938]\n",
      "loss: 0.000395  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.083842 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.000461  [  0/938]\n",
      "loss: 0.000761  [100/938]\n",
      "loss: 0.000342  [200/938]\n",
      "loss: 0.000501  [300/938]\n",
      "loss: 0.000360  [400/938]\n",
      "loss: 0.001358  [500/938]\n",
      "loss: 0.001080  [600/938]\n",
      "loss: 0.000243  [700/938]\n",
      "loss: 0.000633  [800/938]\n",
      "loss: 0.000534  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.083989 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.000017  [  0/938]\n",
      "loss: 0.000725  [100/938]\n",
      "loss: 0.000182  [200/938]\n",
      "loss: 0.000488  [300/938]\n",
      "loss: 0.000878  [400/938]\n",
      "loss: 0.000109  [500/938]\n",
      "loss: 0.001137  [600/938]\n",
      "loss: 0.000284  [700/938]\n",
      "loss: 0.000254  [800/938]\n",
      "loss: 0.000502  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.083806 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.000400  [  0/938]\n",
      "loss: 0.000496  [100/938]\n",
      "loss: 0.000423  [200/938]\n",
      "loss: 0.001005  [300/938]\n",
      "loss: 0.000720  [400/938]\n",
      "loss: 0.000062  [500/938]\n",
      "loss: 0.001114  [600/938]\n",
      "loss: 0.000349  [700/938]\n",
      "loss: 0.000574  [800/938]\n",
      "loss: 0.000284  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.083916 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.000682  [  0/938]\n",
      "loss: 0.000463  [100/938]\n",
      "loss: 0.000070  [200/938]\n",
      "loss: 0.000327  [300/938]\n",
      "loss: 0.000761  [400/938]\n",
      "loss: 0.000655  [500/938]\n",
      "loss: 0.000017  [600/938]\n",
      "loss: 0.000634  [700/938]\n",
      "loss: 0.000290  [800/938]\n",
      "loss: 0.000827  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.084107 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.001151  [  0/938]\n",
      "loss: 0.000398  [100/938]\n",
      "loss: 0.001144  [200/938]\n",
      "loss: 0.000291  [300/938]\n",
      "loss: 0.000564  [400/938]\n",
      "loss: 0.000164  [500/938]\n",
      "loss: 0.000843  [600/938]\n",
      "loss: 0.000507  [700/938]\n",
      "loss: 0.001202  [800/938]\n",
      "loss: 0.000579  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.084170 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.000852  [  0/938]\n",
      "loss: 0.000187  [100/938]\n",
      "loss: 0.000367  [200/938]\n",
      "loss: 0.000278  [300/938]\n",
      "loss: 0.000603  [400/938]\n",
      "loss: 0.000218  [500/938]\n",
      "loss: 0.000237  [600/938]\n",
      "loss: 0.000755  [700/938]\n",
      "loss: 0.000599  [800/938]\n",
      "loss: 0.001031  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.089319 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.000896  [  0/938]\n",
      "loss: 0.000304  [100/938]\n",
      "loss: 0.000357  [200/938]\n",
      "loss: 0.000256  [300/938]\n",
      "loss: 0.000594  [400/938]\n",
      "loss: 0.000535  [500/938]\n",
      "loss: 0.000145  [600/938]\n",
      "loss: 0.000391  [700/938]\n",
      "loss: 0.000242  [800/938]\n",
      "loss: 0.000428  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.084382 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.000271  [  0/938]\n",
      "loss: 0.000333  [100/938]\n",
      "loss: 0.000389  [200/938]\n",
      "loss: 0.000232  [300/938]\n",
      "loss: 0.000281  [400/938]\n",
      "loss: 0.000228  [500/938]\n",
      "loss: 0.001126  [600/938]\n",
      "loss: 0.000323  [700/938]\n",
      "loss: 0.000404  [800/938]\n",
      "loss: 0.000234  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.084852 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.000848  [  0/938]\n",
      "loss: 0.000698  [100/938]\n",
      "loss: 0.000201  [200/938]\n",
      "loss: 0.000515  [300/938]\n",
      "loss: 0.000303  [400/938]\n",
      "loss: 0.000096  [500/938]\n",
      "loss: 0.000041  [600/938]\n",
      "loss: 0.001137  [700/938]\n",
      "loss: 0.000466  [800/938]\n",
      "loss: 0.000418  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.084363 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.000325  [  0/938]\n",
      "loss: 0.000316  [100/938]\n",
      "loss: 0.000620  [200/938]\n",
      "loss: 0.000382  [300/938]\n",
      "loss: 0.001177  [400/938]\n",
      "loss: 0.000323  [500/938]\n",
      "loss: 0.000524  [600/938]\n",
      "loss: 0.000374  [700/938]\n",
      "loss: 0.000464  [800/938]\n",
      "loss: 0.000498  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.084552 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.000070  [  0/938]\n",
      "loss: 0.000327  [100/938]\n",
      "loss: 0.000299  [200/938]\n",
      "loss: 0.001170  [300/938]\n",
      "loss: 0.000957  [400/938]\n",
      "loss: 0.000480  [500/938]\n",
      "loss: 0.001227  [600/938]\n",
      "loss: 0.000398  [700/938]\n",
      "loss: 0.000560  [800/938]\n",
      "loss: 0.000294  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.084414 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.000081  [  0/938]\n",
      "loss: 0.000582  [100/938]\n",
      "loss: 0.000363  [200/938]\n",
      "loss: 0.000344  [300/938]\n",
      "loss: 0.000487  [400/938]\n",
      "loss: 0.000486  [500/938]\n",
      "loss: 0.000137  [600/938]\n",
      "loss: 0.000406  [700/938]\n",
      "loss: 0.000342  [800/938]\n",
      "loss: 0.001787  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.9%, Avg loss: 0.084614 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.000392  [  0/938]\n",
      "loss: 0.000607  [100/938]\n",
      "loss: 0.000233  [200/938]\n",
      "loss: 0.000198  [300/938]\n",
      "loss: 0.000392  [400/938]\n",
      "loss: 0.000554  [500/938]\n",
      "loss: 0.000365  [600/938]\n",
      "loss: 0.000495  [700/938]\n",
      "loss: 0.000374  [800/938]\n",
      "loss: 0.001044  [900/938]\n",
      "Test: \n",
      " Accuracy: 98.0%, Avg loss: 0.084717 \n",
      "\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCH):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train(model2, train_dataset, loss_fn, optimizer2)      # 训练模型\n",
    "    test(model2, test_dataset, loss_fn)                   # 测试模型\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42acd7",
   "metadata": {},
   "source": [
    "# 7、模型预测\n",
    "\n",
    "模型训练完成后，将训练好的模型保存至指定的路径，方便后续的实验中在使用该模型的时候能够加载模型。\\\n",
    "实例化一个随机初始化的模型，并将训练好的超参数加载到刚才初始化的模型中。\\\n",
    "使用测试数据测试模型的识别能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a40c0bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 保存checkpoint时的配置策略\n",
    "mindspore.save_checkpoint(model, \"model.ckpt\")\n",
    "mindspore.save_checkpoint(model2, \"model2.ckpt\")\n",
    "print(\"Saved Model to model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "56229791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化随机初始化的模型 \n",
    "model = Network()\n",
    "model2 = Network2()\n",
    "# 加载检查点，加载参数到模型 \n",
    "param_dict = mindspore.load_checkpoint(\"model.ckpt\")\n",
    "param_dict2 = mindspore.load_checkpoint(\"model2.ckpt\")\n",
    "param_not_load = mindspore.load_param_into_net(model, param_dict)\n",
    "param_not_load2 = mindspore.load_param_into_net(model2, param_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57722031",
   "metadata": {},
   "source": [
    "调用加载dropout层的模型进行预测并显示预测结果和真实结果的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "367cb0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"[1 3 7 5 4 5 4 7 8 0]\", Actual: \"[1 3 7 5 4 5 4 7 8 0]\"\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "for data, label in test_dataset:\n",
    "    pred = model(data)\n",
    "    predicted = pred.argmax(1)\n",
    "    print(f'Predicted: \"{predicted[:10]}\", Actual: \"{label[:10]}\"')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644b64c7",
   "metadata": {},
   "source": [
    "调用不加载dropout层的模型进行预测并显示预测结果和真实结果的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "93868809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"[1 0 0 4 7 2 6 9 0 3]\", Actual: \"[1 0 0 4 7 2 6 9 0 3]\"\n"
     ]
    }
   ],
   "source": [
    "model2.set_train(False)\n",
    "for data, label in test_dataset:\n",
    "    pred = model2(data)\n",
    "    predicted2 = pred.argmax(1)\n",
    "    print(f'Predicted: \"{predicted2[:10]}\", Actual: \"{label[:10]}\"')\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
