{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e59aba3-d52f-4a8d-a8db-30d178fb044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context, Model, load_checkpoint, load_param_into_net\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor\n",
    "import mindspore.dataset.vision as CV\n",
    "import mindspore.dataset.transforms as C\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "from utils.dataset import download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053db1a3-8ef4-4267-a28c-cda457845521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    \"\"\" create dataset for train or test\n",
    "    Args:\n",
    "        data_path: Data path\n",
    "        batch_size: The number of data records in each group\n",
    "        repeat_size: The number of replicated data records\n",
    "        num_parallel_workers: The number of parallel workers\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # define operation parameters\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # define map operations\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)  # Resize images to (32, 32)\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml) # normalize images\n",
    "    rescale_op = CV.Rescale(rescale, shift) # rescale images\n",
    "    hwc2chw_op = CV.HWC2CHW() # change shape from (height, width, channel) to (channel, height, width) to fit network.\n",
    "    type_cast_op = C.TypeCast(mstype.int32) # change data type of label to int32 to fit network\n",
    "\n",
    "    # apply map operations on images\n",
    "    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=resize_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_nml_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=hwc2chw_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    # apply DatasetOps\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)  # 10000 as in LeNet train script\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95128f6a-14e6-4116-8f69-c2345666f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"Lenet network structure.\"\"\"\n",
    "    # define the operator required\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    # use the preceding operators to construct networks\n",
    "    def construct(self, x):\n",
    "        x = self.max_pool2d(self.relu(self.conv1(x)))\n",
    "        x = self.max_pool2d(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524a425b-b3f4-484d-81fc-bc53b21d5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(network_model, epoch_size, data_path, repeat_size, ckpoint_cb, sink_mode):\n",
    "    \"\"\"Define the training method.\"\"\"\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    # load training dataset\n",
    "    ds_train = create_dataset(os.path.join(data_path, \"train\"), 32, repeat_size)\n",
    "    network_model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor()], dataset_sink_mode=sink_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9b94134-cf39-494e-9ec5-553b2d82f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(network, network_model, data_path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    print(\"============== Starting Testing ==============\")\n",
    "    # load the saved model for evaluation\n",
    "    param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
    "    # load parameter to the network\n",
    "    load_param_into_net(network, param_dict)\n",
    "    # load testing dataset\n",
    "    ds_eval = create_dataset(os.path.join(data_path, \"test\"))\n",
    "    acc = network_model.eval(ds_eval, dataset_sink_mode=False)\n",
    "    print(\"============== Accuracy:{} ==============\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc7a6c78-f5ed-42fe-b347-2b5326bd4112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** Downloading the MNIST dataset **************\n",
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.3025779724121094\n",
      "epoch: 1 step: 2, loss is 2.302560806274414\n",
      "epoch: 1 step: 3, loss is 2.302492141723633\n",
      "epoch: 1 step: 4, loss is 2.3028979301452637\n",
      "epoch: 1 step: 5, loss is 2.302887439727783\n",
      "epoch: 1 step: 6, loss is 2.301820993423462\n",
      "epoch: 1 step: 7, loss is 2.3040335178375244\n",
      "epoch: 1 step: 8, loss is 2.302323341369629\n",
      "epoch: 1 step: 9, loss is 2.303865909576416\n",
      "epoch: 1 step: 10, loss is 2.3024492263793945\n",
      "epoch: 1 step: 11, loss is 2.301398754119873\n",
      "epoch: 1 step: 12, loss is 2.3024213314056396\n",
      "epoch: 1 step: 13, loss is 2.3005006313323975\n",
      "epoch: 1 step: 14, loss is 2.3008289337158203\n",
      "epoch: 1 step: 15, loss is 2.3063645362854004\n",
      "epoch: 1 step: 16, loss is 2.300969123840332\n",
      "epoch: 1 step: 17, loss is 2.3008880615234375\n",
      "epoch: 1 step: 18, loss is 2.305079936981201\n",
      "epoch: 1 step: 19, loss is 2.295909881591797\n",
      "epoch: 1 step: 20, loss is 2.3026742935180664\n",
      "epoch: 1 step: 21, loss is 2.3074393272399902\n",
      "epoch: 1 step: 22, loss is 2.303541898727417\n",
      "epoch: 1 step: 23, loss is 2.298584461212158\n",
      "epoch: 1 step: 24, loss is 2.2983670234680176\n",
      "epoch: 1 step: 25, loss is 2.2997684478759766\n",
      "epoch: 1 step: 26, loss is 2.3028273582458496\n",
      "epoch: 1 step: 27, loss is 2.3010146617889404\n",
      "epoch: 1 step: 28, loss is 2.303379535675049\n",
      "epoch: 1 step: 29, loss is 2.303783416748047\n",
      "epoch: 1 step: 30, loss is 2.3045053482055664\n",
      "epoch: 1 step: 31, loss is 2.294759750366211\n",
      "epoch: 1 step: 32, loss is 2.3022913932800293\n",
      "epoch: 1 step: 33, loss is 2.3082895278930664\n",
      "epoch: 1 step: 34, loss is 2.2982711791992188\n",
      "epoch: 1 step: 35, loss is 2.302210807800293\n",
      "epoch: 1 step: 36, loss is 2.3128561973571777\n",
      "epoch: 1 step: 37, loss is 2.2984778881073\n",
      "epoch: 1 step: 38, loss is 2.304457187652588\n",
      "epoch: 1 step: 39, loss is 2.2997913360595703\n",
      "epoch: 1 step: 40, loss is 2.3031740188598633\n",
      "epoch: 1 step: 41, loss is 2.296052932739258\n",
      "epoch: 1 step: 42, loss is 2.308196544647217\n",
      "epoch: 1 step: 43, loss is 2.2992422580718994\n",
      "epoch: 1 step: 44, loss is 2.3005318641662598\n",
      "epoch: 1 step: 45, loss is 2.308750629425049\n",
      "epoch: 1 step: 46, loss is 2.2947731018066406\n",
      "epoch: 1 step: 47, loss is 2.3022634983062744\n",
      "epoch: 1 step: 48, loss is 2.3005146980285645\n",
      "epoch: 1 step: 49, loss is 2.295637607574463\n",
      "epoch: 1 step: 50, loss is 2.303664207458496\n",
      "epoch: 1 step: 51, loss is 2.3019254207611084\n",
      "epoch: 1 step: 52, loss is 2.3000283241271973\n",
      "epoch: 1 step: 53, loss is 2.3094325065612793\n",
      "epoch: 1 step: 54, loss is 2.3007893562316895\n",
      "epoch: 1 step: 55, loss is 2.2955520153045654\n",
      "epoch: 1 step: 56, loss is 2.301541805267334\n",
      "epoch: 1 step: 57, loss is 2.304293155670166\n",
      "epoch: 1 step: 58, loss is 2.3075180053710938\n",
      "epoch: 1 step: 59, loss is 2.306241512298584\n",
      "epoch: 1 step: 60, loss is 2.3055624961853027\n",
      "epoch: 1 step: 61, loss is 2.304201602935791\n",
      "epoch: 1 step: 62, loss is 2.302074432373047\n",
      "epoch: 1 step: 63, loss is 2.3127334117889404\n",
      "epoch: 1 step: 64, loss is 2.299201011657715\n",
      "epoch: 1 step: 65, loss is 2.2986717224121094\n",
      "epoch: 1 step: 66, loss is 2.3107287883758545\n",
      "epoch: 1 step: 67, loss is 2.2891321182250977\n",
      "epoch: 1 step: 68, loss is 2.304593086242676\n",
      "epoch: 1 step: 69, loss is 2.302211046218872\n",
      "epoch: 1 step: 70, loss is 2.30340576171875\n",
      "epoch: 1 step: 71, loss is 2.3059301376342773\n",
      "epoch: 1 step: 72, loss is 2.2901828289031982\n",
      "epoch: 1 step: 73, loss is 2.3134067058563232\n",
      "epoch: 1 step: 74, loss is 2.3073389530181885\n",
      "epoch: 1 step: 75, loss is 2.297417163848877\n",
      "epoch: 1 step: 76, loss is 2.2989258766174316\n",
      "epoch: 1 step: 77, loss is 2.313464641571045\n",
      "epoch: 1 step: 78, loss is 2.3015012741088867\n",
      "epoch: 1 step: 79, loss is 2.300510883331299\n",
      "epoch: 1 step: 80, loss is 2.298041582107544\n",
      "epoch: 1 step: 81, loss is 2.2938625812530518\n",
      "epoch: 1 step: 82, loss is 2.2990667819976807\n",
      "epoch: 1 step: 83, loss is 2.3037633895874023\n",
      "epoch: 1 step: 84, loss is 2.3069217205047607\n",
      "epoch: 1 step: 85, loss is 2.304513931274414\n",
      "epoch: 1 step: 86, loss is 2.3035974502563477\n",
      "epoch: 1 step: 87, loss is 2.3000988960266113\n",
      "epoch: 1 step: 88, loss is 2.311279773712158\n",
      "epoch: 1 step: 89, loss is 2.2980549335479736\n",
      "epoch: 1 step: 90, loss is 2.300218105316162\n",
      "epoch: 1 step: 91, loss is 2.2998151779174805\n",
      "epoch: 1 step: 92, loss is 2.3070571422576904\n",
      "epoch: 1 step: 93, loss is 2.2973670959472656\n",
      "epoch: 1 step: 94, loss is 2.3094217777252197\n",
      "epoch: 1 step: 95, loss is 2.297426223754883\n",
      "epoch: 1 step: 96, loss is 2.2972300052642822\n",
      "epoch: 1 step: 97, loss is 2.3083434104919434\n",
      "epoch: 1 step: 98, loss is 2.3012161254882812\n",
      "epoch: 1 step: 99, loss is 2.3046927452087402\n",
      "epoch: 1 step: 100, loss is 2.3116486072540283\n",
      "epoch: 1 step: 101, loss is 2.2993617057800293\n",
      "epoch: 1 step: 102, loss is 2.287790536880493\n",
      "epoch: 1 step: 103, loss is 2.2922749519348145\n",
      "epoch: 1 step: 104, loss is 2.3144876956939697\n",
      "epoch: 1 step: 105, loss is 2.305070400238037\n",
      "epoch: 1 step: 106, loss is 2.2987053394317627\n",
      "epoch: 1 step: 107, loss is 2.297330379486084\n",
      "epoch: 1 step: 108, loss is 2.2954540252685547\n",
      "epoch: 1 step: 109, loss is 2.3085010051727295\n",
      "epoch: 1 step: 110, loss is 2.293318748474121\n",
      "epoch: 1 step: 111, loss is 2.311945676803589\n",
      "epoch: 1 step: 112, loss is 2.312521457672119\n",
      "epoch: 1 step: 113, loss is 2.2900590896606445\n",
      "epoch: 1 step: 114, loss is 2.3027992248535156\n",
      "epoch: 1 step: 115, loss is 2.292644500732422\n",
      "epoch: 1 step: 116, loss is 2.2986035346984863\n",
      "epoch: 1 step: 117, loss is 2.30131196975708\n",
      "epoch: 1 step: 118, loss is 2.3010993003845215\n",
      "epoch: 1 step: 119, loss is 2.312246322631836\n",
      "epoch: 1 step: 120, loss is 2.297220230102539\n",
      "epoch: 1 step: 121, loss is 2.296351671218872\n",
      "epoch: 1 step: 122, loss is 2.301539897918701\n",
      "epoch: 1 step: 123, loss is 2.312983274459839\n",
      "epoch: 1 step: 124, loss is 2.292985439300537\n",
      "epoch: 1 step: 125, loss is 2.2955026626586914\n",
      "epoch: 1 step: 126, loss is 2.300384998321533\n",
      "epoch: 1 step: 127, loss is 2.3219456672668457\n",
      "epoch: 1 step: 128, loss is 2.32643461227417\n",
      "epoch: 1 step: 129, loss is 2.2990803718566895\n",
      "epoch: 1 step: 130, loss is 2.303584098815918\n",
      "epoch: 1 step: 131, loss is 2.313816785812378\n",
      "epoch: 1 step: 132, loss is 2.2965612411499023\n",
      "epoch: 1 step: 133, loss is 2.303215980529785\n",
      "epoch: 1 step: 134, loss is 2.2917938232421875\n",
      "epoch: 1 step: 135, loss is 2.3044159412384033\n",
      "epoch: 1 step: 136, loss is 2.297231912612915\n",
      "epoch: 1 step: 137, loss is 2.300992012023926\n",
      "epoch: 1 step: 138, loss is 2.3085899353027344\n",
      "epoch: 1 step: 139, loss is 2.300281286239624\n",
      "epoch: 1 step: 140, loss is 2.305187940597534\n",
      "epoch: 1 step: 141, loss is 2.3016414642333984\n",
      "epoch: 1 step: 142, loss is 2.28505277633667\n",
      "epoch: 1 step: 143, loss is 2.301426649093628\n",
      "epoch: 1 step: 144, loss is 2.3048996925354004\n",
      "epoch: 1 step: 145, loss is 2.2978458404541016\n",
      "epoch: 1 step: 146, loss is 2.305624008178711\n",
      "epoch: 1 step: 147, loss is 2.300163507461548\n",
      "epoch: 1 step: 148, loss is 2.3156967163085938\n",
      "epoch: 1 step: 149, loss is 2.3035171031951904\n",
      "epoch: 1 step: 150, loss is 2.2948851585388184\n",
      "epoch: 1 step: 151, loss is 2.308791160583496\n",
      "epoch: 1 step: 152, loss is 2.300408363342285\n",
      "epoch: 1 step: 153, loss is 2.302212715148926\n",
      "epoch: 1 step: 154, loss is 2.309246301651001\n",
      "epoch: 1 step: 155, loss is 2.3044817447662354\n",
      "epoch: 1 step: 156, loss is 2.3126933574676514\n",
      "epoch: 1 step: 157, loss is 2.2944180965423584\n",
      "epoch: 1 step: 158, loss is 2.295208215713501\n",
      "epoch: 1 step: 159, loss is 2.293473243713379\n",
      "epoch: 1 step: 160, loss is 2.314657688140869\n",
      "epoch: 1 step: 161, loss is 2.305393695831299\n",
      "epoch: 1 step: 162, loss is 2.2918949127197266\n",
      "epoch: 1 step: 163, loss is 2.3097331523895264\n",
      "epoch: 1 step: 164, loss is 2.2975594997406006\n",
      "epoch: 1 step: 165, loss is 2.294401168823242\n",
      "epoch: 1 step: 166, loss is 2.290255546569824\n",
      "epoch: 1 step: 167, loss is 2.300185441970825\n",
      "epoch: 1 step: 168, loss is 2.2939443588256836\n",
      "epoch: 1 step: 169, loss is 2.2990198135375977\n",
      "epoch: 1 step: 170, loss is 2.287191867828369\n",
      "epoch: 1 step: 171, loss is 2.3017752170562744\n",
      "epoch: 1 step: 172, loss is 2.2981739044189453\n",
      "epoch: 1 step: 173, loss is 2.2912955284118652\n",
      "epoch: 1 step: 174, loss is 2.2908647060394287\n",
      "epoch: 1 step: 175, loss is 2.296921730041504\n",
      "epoch: 1 step: 176, loss is 2.302494764328003\n",
      "epoch: 1 step: 177, loss is 2.289480686187744\n",
      "epoch: 1 step: 178, loss is 2.310462474822998\n",
      "epoch: 1 step: 179, loss is 2.3159425258636475\n",
      "epoch: 1 step: 180, loss is 2.297408103942871\n",
      "epoch: 1 step: 181, loss is 2.302755832672119\n",
      "epoch: 1 step: 182, loss is 2.3258256912231445\n",
      "epoch: 1 step: 183, loss is 2.311154365539551\n",
      "epoch: 1 step: 184, loss is 2.2988038063049316\n",
      "epoch: 1 step: 185, loss is 2.289623975753784\n",
      "epoch: 1 step: 186, loss is 2.300434112548828\n",
      "epoch: 1 step: 187, loss is 2.2967896461486816\n",
      "epoch: 1 step: 188, loss is 2.304849624633789\n",
      "epoch: 1 step: 189, loss is 2.33329176902771\n",
      "epoch: 1 step: 190, loss is 2.303727149963379\n",
      "epoch: 1 step: 191, loss is 2.2883620262145996\n",
      "epoch: 1 step: 192, loss is 2.304417133331299\n",
      "epoch: 1 step: 193, loss is 2.287313461303711\n",
      "epoch: 1 step: 194, loss is 2.304731845855713\n",
      "epoch: 1 step: 195, loss is 2.3111581802368164\n",
      "epoch: 1 step: 196, loss is 2.309354066848755\n",
      "epoch: 1 step: 197, loss is 2.3032336235046387\n",
      "epoch: 1 step: 198, loss is 2.312753677368164\n",
      "epoch: 1 step: 199, loss is 2.294125556945801\n",
      "epoch: 1 step: 200, loss is 2.2937114238739014\n",
      "epoch: 1 step: 201, loss is 2.288905620574951\n",
      "epoch: 1 step: 202, loss is 2.3048481941223145\n",
      "epoch: 1 step: 203, loss is 2.3210861682891846\n",
      "epoch: 1 step: 204, loss is 2.2987403869628906\n",
      "epoch: 1 step: 205, loss is 2.3007147312164307\n",
      "epoch: 1 step: 206, loss is 2.302727222442627\n",
      "epoch: 1 step: 207, loss is 2.311087131500244\n",
      "epoch: 1 step: 208, loss is 2.2976253032684326\n",
      "epoch: 1 step: 209, loss is 2.298311233520508\n",
      "epoch: 1 step: 210, loss is 2.29996657371521\n",
      "epoch: 1 step: 211, loss is 2.302919387817383\n",
      "epoch: 1 step: 212, loss is 2.2826731204986572\n",
      "epoch: 1 step: 213, loss is 2.2956056594848633\n",
      "epoch: 1 step: 214, loss is 2.2998757362365723\n",
      "epoch: 1 step: 215, loss is 2.318507194519043\n",
      "epoch: 1 step: 216, loss is 2.3066000938415527\n",
      "epoch: 1 step: 217, loss is 2.3070530891418457\n",
      "epoch: 1 step: 218, loss is 2.31082820892334\n",
      "epoch: 1 step: 219, loss is 2.298900842666626\n",
      "epoch: 1 step: 220, loss is 2.3088035583496094\n",
      "epoch: 1 step: 221, loss is 2.3046770095825195\n",
      "epoch: 1 step: 222, loss is 2.3025588989257812\n",
      "epoch: 1 step: 223, loss is 2.2886059284210205\n",
      "epoch: 1 step: 224, loss is 2.2890310287475586\n",
      "epoch: 1 step: 225, loss is 2.3114073276519775\n",
      "epoch: 1 step: 226, loss is 2.2919139862060547\n",
      "epoch: 1 step: 227, loss is 2.309511184692383\n",
      "epoch: 1 step: 228, loss is 2.3083174228668213\n",
      "epoch: 1 step: 229, loss is 2.309131145477295\n",
      "epoch: 1 step: 230, loss is 2.302518129348755\n",
      "epoch: 1 step: 231, loss is 2.286741256713867\n",
      "epoch: 1 step: 232, loss is 2.294132709503174\n",
      "epoch: 1 step: 233, loss is 2.3036751747131348\n",
      "epoch: 1 step: 234, loss is 2.316863536834717\n",
      "epoch: 1 step: 235, loss is 2.2811965942382812\n",
      "epoch: 1 step: 236, loss is 2.317028284072876\n",
      "epoch: 1 step: 237, loss is 2.3057680130004883\n",
      "epoch: 1 step: 238, loss is 2.3029794692993164\n",
      "epoch: 1 step: 239, loss is 2.2987749576568604\n",
      "epoch: 1 step: 240, loss is 2.301147937774658\n",
      "epoch: 1 step: 241, loss is 2.31032133102417\n",
      "epoch: 1 step: 242, loss is 2.3122878074645996\n",
      "epoch: 1 step: 243, loss is 2.3051700592041016\n",
      "epoch: 1 step: 244, loss is 2.2873101234436035\n",
      "epoch: 1 step: 245, loss is 2.282846689224243\n",
      "epoch: 1 step: 246, loss is 2.2997922897338867\n",
      "epoch: 1 step: 247, loss is 2.3251171112060547\n",
      "epoch: 1 step: 248, loss is 2.287078619003296\n",
      "epoch: 1 step: 249, loss is 2.2986221313476562\n",
      "epoch: 1 step: 250, loss is 2.309811592102051\n",
      "epoch: 1 step: 251, loss is 2.27439022064209\n",
      "epoch: 1 step: 252, loss is 2.322327136993408\n",
      "epoch: 1 step: 253, loss is 2.2956738471984863\n",
      "epoch: 1 step: 254, loss is 2.306095600128174\n",
      "epoch: 1 step: 255, loss is 2.299807071685791\n",
      "epoch: 1 step: 256, loss is 2.299647808074951\n",
      "epoch: 1 step: 257, loss is 2.297538995742798\n",
      "epoch: 1 step: 258, loss is 2.304912567138672\n",
      "epoch: 1 step: 259, loss is 2.300262928009033\n",
      "epoch: 1 step: 260, loss is 2.2929956912994385\n",
      "epoch: 1 step: 261, loss is 2.299705982208252\n",
      "epoch: 1 step: 262, loss is 2.302867889404297\n",
      "epoch: 1 step: 263, loss is 2.291872978210449\n",
      "epoch: 1 step: 264, loss is 2.3080224990844727\n",
      "epoch: 1 step: 265, loss is 2.2843642234802246\n",
      "epoch: 1 step: 266, loss is 2.3075344562530518\n",
      "epoch: 1 step: 267, loss is 2.2998569011688232\n",
      "epoch: 1 step: 268, loss is 2.3147263526916504\n",
      "epoch: 1 step: 269, loss is 2.2880711555480957\n",
      "epoch: 1 step: 270, loss is 2.3013644218444824\n",
      "epoch: 1 step: 271, loss is 2.2895607948303223\n",
      "epoch: 1 step: 272, loss is 2.309093475341797\n",
      "epoch: 1 step: 273, loss is 2.298424243927002\n",
      "epoch: 1 step: 274, loss is 2.2986059188842773\n",
      "epoch: 1 step: 275, loss is 2.2828266620635986\n",
      "epoch: 1 step: 276, loss is 2.302335262298584\n",
      "epoch: 1 step: 277, loss is 2.301597833633423\n",
      "epoch: 1 step: 278, loss is 2.29673433303833\n",
      "epoch: 1 step: 279, loss is 2.301825523376465\n",
      "epoch: 1 step: 280, loss is 2.309390068054199\n",
      "epoch: 1 step: 281, loss is 2.315718650817871\n",
      "epoch: 1 step: 282, loss is 2.306499481201172\n",
      "epoch: 1 step: 283, loss is 2.31719708442688\n",
      "epoch: 1 step: 284, loss is 2.2945590019226074\n",
      "epoch: 1 step: 285, loss is 2.3017489910125732\n",
      "epoch: 1 step: 286, loss is 2.2836389541625977\n",
      "epoch: 1 step: 287, loss is 2.2963690757751465\n",
      "epoch: 1 step: 288, loss is 2.3088815212249756\n",
      "epoch: 1 step: 289, loss is 2.3092892169952393\n",
      "epoch: 1 step: 290, loss is 2.3362507820129395\n",
      "epoch: 1 step: 291, loss is 2.310763120651245\n",
      "epoch: 1 step: 292, loss is 2.300189256668091\n",
      "epoch: 1 step: 293, loss is 2.295681953430176\n",
      "epoch: 1 step: 294, loss is 2.292022705078125\n",
      "epoch: 1 step: 295, loss is 2.3169379234313965\n",
      "epoch: 1 step: 296, loss is 2.3096742630004883\n",
      "epoch: 1 step: 297, loss is 2.3117916584014893\n",
      "epoch: 1 step: 298, loss is 2.3242814540863037\n",
      "epoch: 1 step: 299, loss is 2.306692600250244\n",
      "epoch: 1 step: 300, loss is 2.2959156036376953\n",
      "epoch: 1 step: 301, loss is 2.320718288421631\n",
      "epoch: 1 step: 302, loss is 2.2983951568603516\n",
      "epoch: 1 step: 303, loss is 2.293928623199463\n",
      "epoch: 1 step: 304, loss is 2.314202070236206\n",
      "epoch: 1 step: 305, loss is 2.313728094100952\n",
      "epoch: 1 step: 306, loss is 2.3024067878723145\n",
      "epoch: 1 step: 307, loss is 2.3225178718566895\n",
      "epoch: 1 step: 308, loss is 2.2880866527557373\n",
      "epoch: 1 step: 309, loss is 2.2779347896575928\n",
      "epoch: 1 step: 310, loss is 2.29876446723938\n",
      "epoch: 1 step: 311, loss is 2.3114261627197266\n",
      "epoch: 1 step: 312, loss is 2.3066439628601074\n",
      "epoch: 1 step: 313, loss is 2.287731170654297\n",
      "epoch: 1 step: 314, loss is 2.301403760910034\n",
      "epoch: 1 step: 315, loss is 2.3169469833374023\n",
      "epoch: 1 step: 316, loss is 2.2962608337402344\n",
      "epoch: 1 step: 317, loss is 2.304476261138916\n",
      "epoch: 1 step: 318, loss is 2.3012938499450684\n",
      "epoch: 1 step: 319, loss is 2.292924404144287\n",
      "epoch: 1 step: 320, loss is 2.318645477294922\n",
      "epoch: 1 step: 321, loss is 2.3019843101501465\n",
      "epoch: 1 step: 322, loss is 2.3106324672698975\n",
      "epoch: 1 step: 323, loss is 2.30595064163208\n",
      "epoch: 1 step: 324, loss is 2.309224843978882\n",
      "epoch: 1 step: 325, loss is 2.3072025775909424\n",
      "epoch: 1 step: 326, loss is 2.3062753677368164\n",
      "epoch: 1 step: 327, loss is 2.29752779006958\n",
      "epoch: 1 step: 328, loss is 2.303290605545044\n",
      "epoch: 1 step: 329, loss is 2.295774221420288\n",
      "epoch: 1 step: 330, loss is 2.2936501502990723\n",
      "epoch: 1 step: 331, loss is 2.3091912269592285\n",
      "epoch: 1 step: 332, loss is 2.3037822246551514\n",
      "epoch: 1 step: 333, loss is 2.296623468399048\n",
      "epoch: 1 step: 334, loss is 2.2906699180603027\n",
      "epoch: 1 step: 335, loss is 2.304821491241455\n",
      "epoch: 1 step: 336, loss is 2.29219913482666\n",
      "epoch: 1 step: 337, loss is 2.3121018409729004\n",
      "epoch: 1 step: 338, loss is 2.3045382499694824\n",
      "epoch: 1 step: 339, loss is 2.306272506713867\n",
      "epoch: 1 step: 340, loss is 2.316633462905884\n",
      "epoch: 1 step: 341, loss is 2.305795669555664\n",
      "epoch: 1 step: 342, loss is 2.3156113624572754\n",
      "epoch: 1 step: 343, loss is 2.304806709289551\n",
      "epoch: 1 step: 344, loss is 2.289022445678711\n",
      "epoch: 1 step: 345, loss is 2.310831308364868\n",
      "epoch: 1 step: 346, loss is 2.299476385116577\n",
      "epoch: 1 step: 347, loss is 2.2937886714935303\n",
      "epoch: 1 step: 348, loss is 2.312868118286133\n",
      "epoch: 1 step: 349, loss is 2.304208278656006\n",
      "epoch: 1 step: 350, loss is 2.2962799072265625\n",
      "epoch: 1 step: 351, loss is 2.307222366333008\n",
      "epoch: 1 step: 352, loss is 2.2935543060302734\n",
      "epoch: 1 step: 353, loss is 2.304338216781616\n",
      "epoch: 1 step: 354, loss is 2.3068814277648926\n",
      "epoch: 1 step: 355, loss is 2.299685001373291\n",
      "epoch: 1 step: 356, loss is 2.2915456295013428\n",
      "epoch: 1 step: 357, loss is 2.281491994857788\n",
      "epoch: 1 step: 358, loss is 2.2966856956481934\n",
      "epoch: 1 step: 359, loss is 2.2978157997131348\n",
      "epoch: 1 step: 360, loss is 2.2994229793548584\n",
      "epoch: 1 step: 361, loss is 2.3078391551971436\n",
      "epoch: 1 step: 362, loss is 2.2985875606536865\n",
      "epoch: 1 step: 363, loss is 2.2890219688415527\n",
      "epoch: 1 step: 364, loss is 2.305471897125244\n",
      "epoch: 1 step: 365, loss is 2.294456958770752\n",
      "epoch: 1 step: 366, loss is 2.2967233657836914\n",
      "epoch: 1 step: 367, loss is 2.2924509048461914\n",
      "epoch: 1 step: 368, loss is 2.3134958744049072\n",
      "epoch: 1 step: 369, loss is 2.306645631790161\n",
      "epoch: 1 step: 370, loss is 2.3016197681427\n",
      "epoch: 1 step: 371, loss is 2.31221342086792\n",
      "epoch: 1 step: 372, loss is 2.2945680618286133\n",
      "epoch: 1 step: 373, loss is 2.300808906555176\n",
      "epoch: 1 step: 374, loss is 2.3119077682495117\n",
      "epoch: 1 step: 375, loss is 2.2944207191467285\n",
      "epoch: 1 step: 376, loss is 2.2989206314086914\n",
      "epoch: 1 step: 377, loss is 2.3232579231262207\n",
      "epoch: 1 step: 378, loss is 2.2823803424835205\n",
      "epoch: 1 step: 379, loss is 2.2991061210632324\n",
      "epoch: 1 step: 380, loss is 2.269852638244629\n",
      "epoch: 1 step: 381, loss is 2.297667980194092\n",
      "epoch: 1 step: 382, loss is 2.3017771244049072\n",
      "epoch: 1 step: 383, loss is 2.309617757797241\n",
      "epoch: 1 step: 384, loss is 2.2626914978027344\n",
      "epoch: 1 step: 385, loss is 2.2914886474609375\n",
      "epoch: 1 step: 386, loss is 2.300034999847412\n",
      "epoch: 1 step: 387, loss is 2.3195958137512207\n",
      "epoch: 1 step: 388, loss is 2.2928459644317627\n",
      "epoch: 1 step: 389, loss is 2.3015289306640625\n",
      "epoch: 1 step: 390, loss is 2.312371015548706\n",
      "epoch: 1 step: 391, loss is 2.2820000648498535\n",
      "epoch: 1 step: 392, loss is 2.3021278381347656\n",
      "epoch: 1 step: 393, loss is 2.3132550716400146\n",
      "epoch: 1 step: 394, loss is 2.276860475540161\n",
      "epoch: 1 step: 395, loss is 2.2744998931884766\n",
      "epoch: 1 step: 396, loss is 2.323011875152588\n",
      "epoch: 1 step: 397, loss is 2.289113998413086\n",
      "epoch: 1 step: 398, loss is 2.301933526992798\n",
      "epoch: 1 step: 399, loss is 2.318984031677246\n",
      "epoch: 1 step: 400, loss is 2.3145675659179688\n",
      "epoch: 1 step: 401, loss is 2.3239593505859375\n",
      "epoch: 1 step: 402, loss is 2.2813925743103027\n",
      "epoch: 1 step: 403, loss is 2.3134007453918457\n",
      "epoch: 1 step: 404, loss is 2.301011085510254\n",
      "epoch: 1 step: 405, loss is 2.2897040843963623\n",
      "epoch: 1 step: 406, loss is 2.2866134643554688\n",
      "epoch: 1 step: 407, loss is 2.315430164337158\n",
      "epoch: 1 step: 408, loss is 2.315160036087036\n",
      "epoch: 1 step: 409, loss is 2.3136348724365234\n",
      "epoch: 1 step: 410, loss is 2.2902398109436035\n",
      "epoch: 1 step: 411, loss is 2.291459083557129\n",
      "epoch: 1 step: 412, loss is 2.3047566413879395\n",
      "epoch: 1 step: 413, loss is 2.2934792041778564\n",
      "epoch: 1 step: 414, loss is 2.2942755222320557\n",
      "epoch: 1 step: 415, loss is 2.277355909347534\n",
      "epoch: 1 step: 416, loss is 2.315033435821533\n",
      "epoch: 1 step: 417, loss is 2.302450180053711\n",
      "epoch: 1 step: 418, loss is 2.29439377784729\n",
      "epoch: 1 step: 419, loss is 2.307741165161133\n",
      "epoch: 1 step: 420, loss is 2.297837018966675\n",
      "epoch: 1 step: 421, loss is 2.2827939987182617\n",
      "epoch: 1 step: 422, loss is 2.3001675605773926\n",
      "epoch: 1 step: 423, loss is 2.308079242706299\n",
      "epoch: 1 step: 424, loss is 2.3286213874816895\n",
      "epoch: 1 step: 425, loss is 2.3252623081207275\n",
      "epoch: 1 step: 426, loss is 2.3301825523376465\n",
      "epoch: 1 step: 427, loss is 2.2912404537200928\n",
      "epoch: 1 step: 428, loss is 2.2980000972747803\n",
      "epoch: 1 step: 429, loss is 2.3142638206481934\n",
      "epoch: 1 step: 430, loss is 2.296903133392334\n",
      "epoch: 1 step: 431, loss is 2.317293167114258\n",
      "epoch: 1 step: 432, loss is 2.303502321243286\n",
      "epoch: 1 step: 433, loss is 2.2943928241729736\n",
      "epoch: 1 step: 434, loss is 2.3103349208831787\n",
      "epoch: 1 step: 435, loss is 2.3157896995544434\n",
      "epoch: 1 step: 436, loss is 2.313621759414673\n",
      "epoch: 1 step: 437, loss is 2.3091025352478027\n",
      "epoch: 1 step: 438, loss is 2.3119473457336426\n",
      "epoch: 1 step: 439, loss is 2.298755168914795\n",
      "epoch: 1 step: 440, loss is 2.3409595489501953\n",
      "epoch: 1 step: 441, loss is 2.288518190383911\n",
      "epoch: 1 step: 442, loss is 2.292853355407715\n",
      "epoch: 1 step: 443, loss is 2.2817282676696777\n",
      "epoch: 1 step: 444, loss is 2.298281669616699\n",
      "epoch: 1 step: 445, loss is 2.286536931991577\n",
      "epoch: 1 step: 446, loss is 2.322721481323242\n",
      "epoch: 1 step: 447, loss is 2.2955918312072754\n",
      "epoch: 1 step: 448, loss is 2.313457489013672\n",
      "epoch: 1 step: 449, loss is 2.298387050628662\n",
      "epoch: 1 step: 450, loss is 2.3041183948516846\n",
      "epoch: 1 step: 451, loss is 2.285539388656616\n",
      "epoch: 1 step: 452, loss is 2.3048176765441895\n",
      "epoch: 1 step: 453, loss is 2.285996913909912\n",
      "epoch: 1 step: 454, loss is 2.290961742401123\n",
      "epoch: 1 step: 455, loss is 2.282372236251831\n",
      "epoch: 1 step: 456, loss is 2.283757209777832\n",
      "epoch: 1 step: 457, loss is 2.3234310150146484\n",
      "epoch: 1 step: 458, loss is 2.3057913780212402\n",
      "epoch: 1 step: 459, loss is 2.2929153442382812\n",
      "epoch: 1 step: 460, loss is 2.3006844520568848\n",
      "epoch: 1 step: 461, loss is 2.2963695526123047\n",
      "epoch: 1 step: 462, loss is 2.299625873565674\n",
      "epoch: 1 step: 463, loss is 2.3051438331604004\n",
      "epoch: 1 step: 464, loss is 2.315615177154541\n",
      "epoch: 1 step: 465, loss is 2.3132286071777344\n",
      "epoch: 1 step: 466, loss is 2.3075451850891113\n",
      "epoch: 1 step: 467, loss is 2.2931368350982666\n",
      "epoch: 1 step: 468, loss is 2.2833051681518555\n",
      "epoch: 1 step: 469, loss is 2.3091588020324707\n",
      "epoch: 1 step: 470, loss is 2.303436756134033\n",
      "epoch: 1 step: 471, loss is 2.30297589302063\n",
      "epoch: 1 step: 472, loss is 2.293504476547241\n",
      "epoch: 1 step: 473, loss is 2.2922885417938232\n",
      "epoch: 1 step: 474, loss is 2.3008365631103516\n",
      "epoch: 1 step: 475, loss is 2.3041343688964844\n",
      "epoch: 1 step: 476, loss is 2.303115129470825\n",
      "epoch: 1 step: 477, loss is 2.288207769393921\n",
      "epoch: 1 step: 478, loss is 2.306549072265625\n",
      "epoch: 1 step: 479, loss is 2.2934298515319824\n",
      "epoch: 1 step: 480, loss is 2.3253722190856934\n",
      "epoch: 1 step: 481, loss is 2.2925610542297363\n",
      "epoch: 1 step: 482, loss is 2.3100240230560303\n",
      "epoch: 1 step: 483, loss is 2.2980263233184814\n",
      "epoch: 1 step: 484, loss is 2.2919530868530273\n",
      "epoch: 1 step: 485, loss is 2.304081439971924\n",
      "epoch: 1 step: 486, loss is 2.2987217903137207\n",
      "epoch: 1 step: 487, loss is 2.30747389793396\n",
      "epoch: 1 step: 488, loss is 2.307985544204712\n",
      "epoch: 1 step: 489, loss is 2.315446376800537\n",
      "epoch: 1 step: 490, loss is 2.3123579025268555\n",
      "epoch: 1 step: 491, loss is 2.321078300476074\n",
      "epoch: 1 step: 492, loss is 2.318570375442505\n",
      "epoch: 1 step: 493, loss is 2.3040788173675537\n",
      "epoch: 1 step: 494, loss is 2.306396961212158\n",
      "epoch: 1 step: 495, loss is 2.3036553859710693\n",
      "epoch: 1 step: 496, loss is 2.2894933223724365\n",
      "epoch: 1 step: 497, loss is 2.2911205291748047\n",
      "epoch: 1 step: 498, loss is 2.2864584922790527\n",
      "epoch: 1 step: 499, loss is 2.281717538833618\n",
      "epoch: 1 step: 500, loss is 2.2999379634857178\n",
      "epoch: 1 step: 501, loss is 2.2964730262756348\n",
      "epoch: 1 step: 502, loss is 2.308224678039551\n",
      "epoch: 1 step: 503, loss is 2.2953505516052246\n",
      "epoch: 1 step: 504, loss is 2.301331043243408\n",
      "epoch: 1 step: 505, loss is 2.2944271564483643\n",
      "epoch: 1 step: 506, loss is 2.3165650367736816\n",
      "epoch: 1 step: 507, loss is 2.304544448852539\n",
      "epoch: 1 step: 508, loss is 2.304997444152832\n",
      "epoch: 1 step: 509, loss is 2.2922046184539795\n",
      "epoch: 1 step: 510, loss is 2.2931268215179443\n",
      "epoch: 1 step: 511, loss is 2.3103387355804443\n",
      "epoch: 1 step: 512, loss is 2.317678928375244\n",
      "epoch: 1 step: 513, loss is 2.3098480701446533\n",
      "epoch: 1 step: 514, loss is 2.3074898719787598\n",
      "epoch: 1 step: 515, loss is 2.2920663356781006\n",
      "epoch: 1 step: 516, loss is 2.3086299896240234\n",
      "epoch: 1 step: 517, loss is 2.306427240371704\n",
      "epoch: 1 step: 518, loss is 2.2980690002441406\n",
      "epoch: 1 step: 519, loss is 2.298659086227417\n",
      "epoch: 1 step: 520, loss is 2.3112754821777344\n",
      "epoch: 1 step: 521, loss is 2.3066141605377197\n",
      "epoch: 1 step: 522, loss is 2.309854030609131\n",
      "epoch: 1 step: 523, loss is 2.317784070968628\n",
      "epoch: 1 step: 524, loss is 2.2976574897766113\n",
      "epoch: 1 step: 525, loss is 2.277395725250244\n",
      "epoch: 1 step: 526, loss is 2.3046555519104004\n",
      "epoch: 1 step: 527, loss is 2.307755470275879\n",
      "epoch: 1 step: 528, loss is 2.31424617767334\n",
      "epoch: 1 step: 529, loss is 2.2972421646118164\n",
      "epoch: 1 step: 530, loss is 2.305983543395996\n",
      "epoch: 1 step: 531, loss is 2.3036487102508545\n",
      "epoch: 1 step: 532, loss is 2.3095996379852295\n",
      "epoch: 1 step: 533, loss is 2.2873387336730957\n",
      "epoch: 1 step: 534, loss is 2.321171283721924\n",
      "epoch: 1 step: 535, loss is 2.2805562019348145\n",
      "epoch: 1 step: 536, loss is 2.3091964721679688\n",
      "epoch: 1 step: 537, loss is 2.3084163665771484\n",
      "epoch: 1 step: 538, loss is 2.2998833656311035\n",
      "epoch: 1 step: 539, loss is 2.274507761001587\n",
      "epoch: 1 step: 540, loss is 2.3038148880004883\n",
      "epoch: 1 step: 541, loss is 2.2800726890563965\n",
      "epoch: 1 step: 542, loss is 2.2980852127075195\n",
      "epoch: 1 step: 543, loss is 2.3076112270355225\n",
      "epoch: 1 step: 544, loss is 2.2917098999023438\n",
      "epoch: 1 step: 545, loss is 2.306919574737549\n",
      "epoch: 1 step: 546, loss is 2.3080177307128906\n",
      "epoch: 1 step: 547, loss is 2.292102336883545\n",
      "epoch: 1 step: 548, loss is 2.308882713317871\n",
      "epoch: 1 step: 549, loss is 2.2957239151000977\n",
      "epoch: 1 step: 550, loss is 2.2910852432250977\n",
      "epoch: 1 step: 551, loss is 2.313368797302246\n",
      "epoch: 1 step: 552, loss is 2.304128646850586\n",
      "epoch: 1 step: 553, loss is 2.289965867996216\n",
      "epoch: 1 step: 554, loss is 2.290400505065918\n",
      "epoch: 1 step: 555, loss is 2.2858660221099854\n",
      "epoch: 1 step: 556, loss is 2.3086071014404297\n",
      "epoch: 1 step: 557, loss is 2.2982873916625977\n",
      "epoch: 1 step: 558, loss is 2.3145384788513184\n",
      "epoch: 1 step: 559, loss is 2.2939343452453613\n",
      "epoch: 1 step: 560, loss is 2.300992250442505\n",
      "epoch: 1 step: 561, loss is 2.3007888793945312\n",
      "epoch: 1 step: 562, loss is 2.316959857940674\n",
      "epoch: 1 step: 563, loss is 2.2980780601501465\n",
      "epoch: 1 step: 564, loss is 2.2892723083496094\n",
      "epoch: 1 step: 565, loss is 2.307508707046509\n",
      "epoch: 1 step: 566, loss is 2.2782342433929443\n",
      "epoch: 1 step: 567, loss is 2.284205436706543\n",
      "epoch: 1 step: 568, loss is 2.2945775985717773\n",
      "epoch: 1 step: 569, loss is 2.301412582397461\n",
      "epoch: 1 step: 570, loss is 2.29807448387146\n",
      "epoch: 1 step: 571, loss is 2.290895700454712\n",
      "epoch: 1 step: 572, loss is 2.2957515716552734\n",
      "epoch: 1 step: 573, loss is 2.320009469985962\n",
      "epoch: 1 step: 574, loss is 2.301027297973633\n",
      "epoch: 1 step: 575, loss is 2.289463996887207\n",
      "epoch: 1 step: 576, loss is 2.289947986602783\n",
      "epoch: 1 step: 577, loss is 2.300182342529297\n",
      "epoch: 1 step: 578, loss is 2.313121795654297\n",
      "epoch: 1 step: 579, loss is 2.3012375831604004\n",
      "epoch: 1 step: 580, loss is 2.301288366317749\n",
      "epoch: 1 step: 581, loss is 2.307849884033203\n",
      "epoch: 1 step: 582, loss is 2.2740554809570312\n",
      "epoch: 1 step: 583, loss is 2.3112354278564453\n",
      "epoch: 1 step: 584, loss is 2.284630298614502\n",
      "epoch: 1 step: 585, loss is 2.268897294998169\n",
      "epoch: 1 step: 586, loss is 2.2887163162231445\n",
      "epoch: 1 step: 587, loss is 2.304769277572632\n",
      "epoch: 1 step: 588, loss is 2.3083438873291016\n",
      "epoch: 1 step: 589, loss is 2.300415515899658\n",
      "epoch: 1 step: 590, loss is 2.291957378387451\n",
      "epoch: 1 step: 591, loss is 2.3053536415100098\n",
      "epoch: 1 step: 592, loss is 2.311098098754883\n",
      "epoch: 1 step: 593, loss is 2.31351375579834\n",
      "epoch: 1 step: 594, loss is 2.2824201583862305\n",
      "epoch: 1 step: 595, loss is 2.308464527130127\n",
      "epoch: 1 step: 596, loss is 2.311013698577881\n",
      "epoch: 1 step: 597, loss is 2.2983453273773193\n",
      "epoch: 1 step: 598, loss is 2.3056259155273438\n",
      "epoch: 1 step: 599, loss is 2.3243532180786133\n",
      "epoch: 1 step: 600, loss is 2.305166006088257\n",
      "epoch: 1 step: 601, loss is 2.294102668762207\n",
      "epoch: 1 step: 602, loss is 2.3289220333099365\n",
      "epoch: 1 step: 603, loss is 2.309230327606201\n",
      "epoch: 1 step: 604, loss is 2.2997260093688965\n",
      "epoch: 1 step: 605, loss is 2.2869532108306885\n",
      "epoch: 1 step: 606, loss is 2.3075289726257324\n",
      "epoch: 1 step: 607, loss is 2.2947447299957275\n",
      "epoch: 1 step: 608, loss is 2.310920238494873\n",
      "epoch: 1 step: 609, loss is 2.304898738861084\n",
      "epoch: 1 step: 610, loss is 2.2966842651367188\n",
      "epoch: 1 step: 611, loss is 2.3138113021850586\n",
      "epoch: 1 step: 612, loss is 2.305687665939331\n",
      "epoch: 1 step: 613, loss is 2.284254550933838\n",
      "epoch: 1 step: 614, loss is 2.2965564727783203\n",
      "epoch: 1 step: 615, loss is 2.2955219745635986\n",
      "epoch: 1 step: 616, loss is 2.3187456130981445\n",
      "epoch: 1 step: 617, loss is 2.315807342529297\n",
      "epoch: 1 step: 618, loss is 2.2998046875\n",
      "epoch: 1 step: 619, loss is 2.2861838340759277\n",
      "epoch: 1 step: 620, loss is 2.301464796066284\n",
      "epoch: 1 step: 621, loss is 2.3107986450195312\n",
      "epoch: 1 step: 622, loss is 2.3085014820098877\n",
      "epoch: 1 step: 623, loss is 2.3118937015533447\n",
      "epoch: 1 step: 624, loss is 2.2893004417419434\n",
      "epoch: 1 step: 625, loss is 2.303071975708008\n",
      "epoch: 1 step: 626, loss is 2.284153699874878\n",
      "epoch: 1 step: 627, loss is 2.307222843170166\n",
      "epoch: 1 step: 628, loss is 2.2956666946411133\n",
      "epoch: 1 step: 629, loss is 2.304018497467041\n",
      "epoch: 1 step: 630, loss is 2.2888636589050293\n",
      "epoch: 1 step: 631, loss is 2.31610107421875\n",
      "epoch: 1 step: 632, loss is 2.307767868041992\n",
      "epoch: 1 step: 633, loss is 2.301060199737549\n",
      "epoch: 1 step: 634, loss is 2.302030086517334\n",
      "epoch: 1 step: 635, loss is 2.2998831272125244\n",
      "epoch: 1 step: 636, loss is 2.2939705848693848\n",
      "epoch: 1 step: 637, loss is 2.299760580062866\n",
      "epoch: 1 step: 638, loss is 2.29315185546875\n",
      "epoch: 1 step: 639, loss is 2.297990083694458\n",
      "epoch: 1 step: 640, loss is 2.291632652282715\n",
      "epoch: 1 step: 641, loss is 2.2874536514282227\n",
      "epoch: 1 step: 642, loss is 2.291350841522217\n",
      "epoch: 1 step: 643, loss is 2.310264825820923\n",
      "epoch: 1 step: 644, loss is 2.2948174476623535\n",
      "epoch: 1 step: 645, loss is 2.3150572776794434\n",
      "epoch: 1 step: 646, loss is 2.314810037612915\n",
      "epoch: 1 step: 647, loss is 2.2884745597839355\n",
      "epoch: 1 step: 648, loss is 2.3163418769836426\n",
      "epoch: 1 step: 649, loss is 2.301565647125244\n",
      "epoch: 1 step: 650, loss is 2.316269874572754\n",
      "epoch: 1 step: 651, loss is 2.302793264389038\n",
      "epoch: 1 step: 652, loss is 2.2943079471588135\n",
      "epoch: 1 step: 653, loss is 2.309793472290039\n",
      "epoch: 1 step: 654, loss is 2.3238954544067383\n",
      "epoch: 1 step: 655, loss is 2.304596424102783\n",
      "epoch: 1 step: 656, loss is 2.2823410034179688\n",
      "epoch: 1 step: 657, loss is 2.3214850425720215\n",
      "epoch: 1 step: 658, loss is 2.2875890731811523\n",
      "epoch: 1 step: 659, loss is 2.304527759552002\n",
      "epoch: 1 step: 660, loss is 2.2990870475769043\n",
      "epoch: 1 step: 661, loss is 2.307135581970215\n",
      "epoch: 1 step: 662, loss is 2.294504165649414\n",
      "epoch: 1 step: 663, loss is 2.3144748210906982\n",
      "epoch: 1 step: 664, loss is 2.298707962036133\n",
      "epoch: 1 step: 665, loss is 2.3211264610290527\n",
      "epoch: 1 step: 666, loss is 2.2987027168273926\n",
      "epoch: 1 step: 667, loss is 2.2887938022613525\n",
      "epoch: 1 step: 668, loss is 2.3050551414489746\n",
      "epoch: 1 step: 669, loss is 2.3079347610473633\n",
      "epoch: 1 step: 670, loss is 2.2867584228515625\n",
      "epoch: 1 step: 671, loss is 2.3003079891204834\n",
      "epoch: 1 step: 672, loss is 2.299434185028076\n",
      "epoch: 1 step: 673, loss is 2.3142361640930176\n",
      "epoch: 1 step: 674, loss is 2.303940773010254\n",
      "epoch: 1 step: 675, loss is 2.3191123008728027\n",
      "epoch: 1 step: 676, loss is 2.2871220111846924\n",
      "epoch: 1 step: 677, loss is 2.2987060546875\n",
      "epoch: 1 step: 678, loss is 2.3168118000030518\n",
      "epoch: 1 step: 679, loss is 2.303711414337158\n",
      "epoch: 1 step: 680, loss is 2.298391819000244\n",
      "epoch: 1 step: 681, loss is 2.3013620376586914\n",
      "epoch: 1 step: 682, loss is 2.3068182468414307\n",
      "epoch: 1 step: 683, loss is 2.3198184967041016\n",
      "epoch: 1 step: 684, loss is 2.295307159423828\n",
      "epoch: 1 step: 685, loss is 2.2951340675354004\n",
      "epoch: 1 step: 686, loss is 2.3303611278533936\n",
      "epoch: 1 step: 687, loss is 2.3045947551727295\n",
      "epoch: 1 step: 688, loss is 2.285367488861084\n",
      "epoch: 1 step: 689, loss is 2.302929401397705\n",
      "epoch: 1 step: 690, loss is 2.2911183834075928\n",
      "epoch: 1 step: 691, loss is 2.3006370067596436\n",
      "epoch: 1 step: 692, loss is 2.2954723834991455\n",
      "epoch: 1 step: 693, loss is 2.28717041015625\n",
      "epoch: 1 step: 694, loss is 2.3055663108825684\n",
      "epoch: 1 step: 695, loss is 2.304070234298706\n",
      "epoch: 1 step: 696, loss is 2.3079423904418945\n",
      "epoch: 1 step: 697, loss is 2.3128862380981445\n",
      "epoch: 1 step: 698, loss is 2.3152945041656494\n",
      "epoch: 1 step: 699, loss is 2.295701265335083\n",
      "epoch: 1 step: 700, loss is 2.2953102588653564\n",
      "epoch: 1 step: 701, loss is 2.3096914291381836\n",
      "epoch: 1 step: 702, loss is 2.309872627258301\n",
      "epoch: 1 step: 703, loss is 2.314335823059082\n",
      "epoch: 1 step: 704, loss is 2.324415922164917\n",
      "epoch: 1 step: 705, loss is 2.305671215057373\n",
      "epoch: 1 step: 706, loss is 2.2994470596313477\n",
      "epoch: 1 step: 707, loss is 2.30200457572937\n",
      "epoch: 1 step: 708, loss is 2.289557695388794\n",
      "epoch: 1 step: 709, loss is 2.3041844367980957\n",
      "epoch: 1 step: 710, loss is 2.292961359024048\n",
      "epoch: 1 step: 711, loss is 2.3029561042785645\n",
      "epoch: 1 step: 712, loss is 2.3035945892333984\n",
      "epoch: 1 step: 713, loss is 2.3050460815429688\n",
      "epoch: 1 step: 714, loss is 2.292813301086426\n",
      "epoch: 1 step: 715, loss is 2.3110132217407227\n",
      "epoch: 1 step: 716, loss is 2.31618070602417\n",
      "epoch: 1 step: 717, loss is 2.3134965896606445\n",
      "epoch: 1 step: 718, loss is 2.3101065158843994\n",
      "epoch: 1 step: 719, loss is 2.30002498626709\n",
      "epoch: 1 step: 720, loss is 2.2986557483673096\n",
      "epoch: 1 step: 721, loss is 2.310504913330078\n",
      "epoch: 1 step: 722, loss is 2.303959369659424\n",
      "epoch: 1 step: 723, loss is 2.307579278945923\n",
      "epoch: 1 step: 724, loss is 2.3142528533935547\n",
      "epoch: 1 step: 725, loss is 2.307490348815918\n",
      "epoch: 1 step: 726, loss is 2.2985520362854004\n",
      "epoch: 1 step: 727, loss is 2.288545608520508\n",
      "epoch: 1 step: 728, loss is 2.300976514816284\n",
      "epoch: 1 step: 729, loss is 2.2977981567382812\n",
      "epoch: 1 step: 730, loss is 2.296074867248535\n",
      "epoch: 1 step: 731, loss is 2.3040192127227783\n",
      "epoch: 1 step: 732, loss is 2.3060946464538574\n",
      "epoch: 1 step: 733, loss is 2.307344436645508\n",
      "epoch: 1 step: 734, loss is 2.3017971515655518\n",
      "epoch: 1 step: 735, loss is 2.296862840652466\n",
      "epoch: 1 step: 736, loss is 2.2994203567504883\n",
      "epoch: 1 step: 737, loss is 2.2946462631225586\n",
      "epoch: 1 step: 738, loss is 2.3021316528320312\n",
      "epoch: 1 step: 739, loss is 2.3035924434661865\n",
      "epoch: 1 step: 740, loss is 2.3057618141174316\n",
      "epoch: 1 step: 741, loss is 2.2981438636779785\n",
      "epoch: 1 step: 742, loss is 2.2974472045898438\n",
      "epoch: 1 step: 743, loss is 2.294999599456787\n",
      "epoch: 1 step: 744, loss is 2.298281669616699\n",
      "epoch: 1 step: 745, loss is 2.2968297004699707\n",
      "epoch: 1 step: 746, loss is 2.2964017391204834\n",
      "epoch: 1 step: 747, loss is 2.282700300216675\n",
      "epoch: 1 step: 748, loss is 2.3014779090881348\n",
      "epoch: 1 step: 749, loss is 2.299360990524292\n",
      "epoch: 1 step: 750, loss is 2.290747880935669\n",
      "epoch: 1 step: 751, loss is 2.29667329788208\n",
      "epoch: 1 step: 752, loss is 2.304138422012329\n",
      "epoch: 1 step: 753, loss is 2.3062210083007812\n",
      "epoch: 1 step: 754, loss is 2.3078041076660156\n",
      "epoch: 1 step: 755, loss is 2.3101954460144043\n",
      "epoch: 1 step: 756, loss is 2.301635265350342\n",
      "epoch: 1 step: 757, loss is 2.299849033355713\n",
      "epoch: 1 step: 758, loss is 2.2913761138916016\n",
      "epoch: 1 step: 759, loss is 2.29892635345459\n",
      "epoch: 1 step: 760, loss is 2.304879903793335\n",
      "epoch: 1 step: 761, loss is 2.3173515796661377\n",
      "epoch: 1 step: 762, loss is 2.303006172180176\n",
      "epoch: 1 step: 763, loss is 2.3007593154907227\n",
      "epoch: 1 step: 764, loss is 2.2939035892486572\n",
      "epoch: 1 step: 765, loss is 2.30257248878479\n",
      "epoch: 1 step: 766, loss is 2.2905077934265137\n",
      "epoch: 1 step: 767, loss is 2.3075919151306152\n",
      "epoch: 1 step: 768, loss is 2.3003220558166504\n",
      "epoch: 1 step: 769, loss is 2.2829017639160156\n",
      "epoch: 1 step: 770, loss is 2.310478687286377\n",
      "epoch: 1 step: 771, loss is 2.303936004638672\n",
      "epoch: 1 step: 772, loss is 2.2942700386047363\n",
      "epoch: 1 step: 773, loss is 2.307262420654297\n",
      "epoch: 1 step: 774, loss is 2.306309700012207\n",
      "epoch: 1 step: 775, loss is 2.294973373413086\n",
      "epoch: 1 step: 776, loss is 2.3009800910949707\n",
      "epoch: 1 step: 777, loss is 2.3130650520324707\n",
      "epoch: 1 step: 778, loss is 2.285429000854492\n",
      "epoch: 1 step: 779, loss is 2.2883689403533936\n",
      "epoch: 1 step: 780, loss is 2.3034183979034424\n",
      "epoch: 1 step: 781, loss is 2.2965409755706787\n",
      "epoch: 1 step: 782, loss is 2.294363498687744\n",
      "epoch: 1 step: 783, loss is 2.300915479660034\n",
      "epoch: 1 step: 784, loss is 2.321188449859619\n",
      "epoch: 1 step: 785, loss is 2.2840843200683594\n",
      "epoch: 1 step: 786, loss is 2.298762321472168\n",
      "epoch: 1 step: 787, loss is 2.302471160888672\n",
      "epoch: 1 step: 788, loss is 2.3032665252685547\n",
      "epoch: 1 step: 789, loss is 2.3052706718444824\n",
      "epoch: 1 step: 790, loss is 2.3230838775634766\n",
      "epoch: 1 step: 791, loss is 2.3056933879852295\n",
      "epoch: 1 step: 792, loss is 2.3016250133514404\n",
      "epoch: 1 step: 793, loss is 2.2989342212677\n",
      "epoch: 1 step: 794, loss is 2.309720993041992\n",
      "epoch: 1 step: 795, loss is 2.2979750633239746\n",
      "epoch: 1 step: 796, loss is 2.2967848777770996\n",
      "epoch: 1 step: 797, loss is 2.3073580265045166\n",
      "epoch: 1 step: 798, loss is 2.2967910766601562\n",
      "epoch: 1 step: 799, loss is 2.298992395401001\n",
      "epoch: 1 step: 800, loss is 2.2930214405059814\n",
      "epoch: 1 step: 801, loss is 2.2853946685791016\n",
      "epoch: 1 step: 802, loss is 2.293488025665283\n",
      "epoch: 1 step: 803, loss is 2.306304454803467\n",
      "epoch: 1 step: 804, loss is 2.3057053089141846\n",
      "epoch: 1 step: 805, loss is 2.296881675720215\n",
      "epoch: 1 step: 806, loss is 2.3153436183929443\n",
      "epoch: 1 step: 807, loss is 2.299948215484619\n",
      "epoch: 1 step: 808, loss is 2.2918663024902344\n",
      "epoch: 1 step: 809, loss is 2.3121728897094727\n",
      "epoch: 1 step: 810, loss is 2.287468433380127\n",
      "epoch: 1 step: 811, loss is 2.284855842590332\n",
      "epoch: 1 step: 812, loss is 2.2987589836120605\n",
      "epoch: 1 step: 813, loss is 2.294887065887451\n",
      "epoch: 1 step: 814, loss is 2.302564859390259\n",
      "epoch: 1 step: 815, loss is 2.3139331340789795\n",
      "epoch: 1 step: 816, loss is 2.2741074562072754\n",
      "epoch: 1 step: 817, loss is 2.2884225845336914\n",
      "epoch: 1 step: 818, loss is 2.299518346786499\n",
      "epoch: 1 step: 819, loss is 2.296515941619873\n",
      "epoch: 1 step: 820, loss is 2.2853565216064453\n",
      "epoch: 1 step: 821, loss is 2.2980215549468994\n",
      "epoch: 1 step: 822, loss is 2.3129916191101074\n",
      "epoch: 1 step: 823, loss is 2.302020311355591\n",
      "epoch: 1 step: 824, loss is 2.2876672744750977\n",
      "epoch: 1 step: 825, loss is 2.2957022190093994\n",
      "epoch: 1 step: 826, loss is 2.296870231628418\n",
      "epoch: 1 step: 827, loss is 2.3073244094848633\n",
      "epoch: 1 step: 828, loss is 2.297811985015869\n",
      "epoch: 1 step: 829, loss is 2.292693853378296\n",
      "epoch: 1 step: 830, loss is 2.2929062843322754\n",
      "epoch: 1 step: 831, loss is 2.291280508041382\n",
      "epoch: 1 step: 832, loss is 2.2687840461730957\n",
      "epoch: 1 step: 833, loss is 2.2958719730377197\n",
      "epoch: 1 step: 834, loss is 2.3033928871154785\n",
      "epoch: 1 step: 835, loss is 2.294588565826416\n",
      "epoch: 1 step: 836, loss is 2.2803032398223877\n",
      "epoch: 1 step: 837, loss is 2.3023593425750732\n",
      "epoch: 1 step: 838, loss is 2.2863712310791016\n",
      "epoch: 1 step: 839, loss is 2.2904052734375\n",
      "epoch: 1 step: 840, loss is 2.2948670387268066\n",
      "epoch: 1 step: 841, loss is 2.285888195037842\n",
      "epoch: 1 step: 842, loss is 2.2835192680358887\n",
      "epoch: 1 step: 843, loss is 2.2841579914093018\n",
      "epoch: 1 step: 844, loss is 2.297698736190796\n",
      "epoch: 1 step: 845, loss is 2.313199043273926\n",
      "epoch: 1 step: 846, loss is 2.3037197589874268\n",
      "epoch: 1 step: 847, loss is 2.2963476181030273\n",
      "epoch: 1 step: 848, loss is 2.2965588569641113\n",
      "epoch: 1 step: 849, loss is 2.2727599143981934\n",
      "epoch: 1 step: 850, loss is 2.292224168777466\n",
      "epoch: 1 step: 851, loss is 2.268108367919922\n",
      "epoch: 1 step: 852, loss is 2.2879645824432373\n",
      "epoch: 1 step: 853, loss is 2.2953057289123535\n",
      "epoch: 1 step: 854, loss is 2.281970977783203\n",
      "epoch: 1 step: 855, loss is 2.276275873184204\n",
      "epoch: 1 step: 856, loss is 2.295069694519043\n",
      "epoch: 1 step: 857, loss is 2.2965779304504395\n",
      "epoch: 1 step: 858, loss is 2.28610897064209\n",
      "epoch: 1 step: 859, loss is 2.2963485717773438\n",
      "epoch: 1 step: 860, loss is 2.263002872467041\n",
      "epoch: 1 step: 861, loss is 2.295849323272705\n",
      "epoch: 1 step: 862, loss is 2.2755861282348633\n",
      "epoch: 1 step: 863, loss is 2.2658584117889404\n",
      "epoch: 1 step: 864, loss is 2.269063949584961\n",
      "epoch: 1 step: 865, loss is 2.299363136291504\n",
      "epoch: 1 step: 866, loss is 2.284189462661743\n",
      "epoch: 1 step: 867, loss is 2.294597625732422\n",
      "epoch: 1 step: 868, loss is 2.2745845317840576\n",
      "epoch: 1 step: 869, loss is 2.2608189582824707\n",
      "epoch: 1 step: 870, loss is 2.2744107246398926\n",
      "epoch: 1 step: 871, loss is 2.283102512359619\n",
      "epoch: 1 step: 872, loss is 2.2835683822631836\n",
      "epoch: 1 step: 873, loss is 2.2410695552825928\n",
      "epoch: 1 step: 874, loss is 2.294943332672119\n",
      "epoch: 1 step: 875, loss is 2.272531747817993\n",
      "epoch: 1 step: 876, loss is 2.2705812454223633\n",
      "epoch: 1 step: 877, loss is 2.287410259246826\n",
      "epoch: 1 step: 878, loss is 2.2893130779266357\n",
      "epoch: 1 step: 879, loss is 2.2739696502685547\n",
      "epoch: 1 step: 880, loss is 2.266425609588623\n",
      "epoch: 1 step: 881, loss is 2.264436960220337\n",
      "epoch: 1 step: 882, loss is 2.2845969200134277\n",
      "epoch: 1 step: 883, loss is 2.2449898719787598\n",
      "epoch: 1 step: 884, loss is 2.270829200744629\n",
      "epoch: 1 step: 885, loss is 2.239629030227661\n",
      "epoch: 1 step: 886, loss is 2.2207274436950684\n",
      "epoch: 1 step: 887, loss is 2.264843702316284\n",
      "epoch: 1 step: 888, loss is 2.2403197288513184\n",
      "epoch: 1 step: 889, loss is 2.274252414703369\n",
      "epoch: 1 step: 890, loss is 2.2611515522003174\n",
      "epoch: 1 step: 891, loss is 2.2276158332824707\n",
      "epoch: 1 step: 892, loss is 2.225126266479492\n",
      "epoch: 1 step: 893, loss is 2.1998579502105713\n",
      "epoch: 1 step: 894, loss is 2.2255375385284424\n",
      "epoch: 1 step: 895, loss is 2.2395424842834473\n",
      "epoch: 1 step: 896, loss is 2.1953625679016113\n",
      "epoch: 1 step: 897, loss is 2.2049851417541504\n",
      "epoch: 1 step: 898, loss is 2.2211732864379883\n",
      "epoch: 1 step: 899, loss is 2.186892509460449\n",
      "epoch: 1 step: 900, loss is 2.2120180130004883\n",
      "epoch: 1 step: 901, loss is 2.1454219818115234\n",
      "epoch: 1 step: 902, loss is 2.107487201690674\n",
      "epoch: 1 step: 903, loss is 2.131276845932007\n",
      "epoch: 1 step: 904, loss is 2.12143874168396\n",
      "epoch: 1 step: 905, loss is 2.044971466064453\n",
      "epoch: 1 step: 906, loss is 1.9761022329330444\n",
      "epoch: 1 step: 907, loss is 2.083017587661743\n",
      "epoch: 1 step: 908, loss is 1.9927623271942139\n",
      "epoch: 1 step: 909, loss is 2.065136671066284\n",
      "epoch: 1 step: 910, loss is 1.835006833076477\n",
      "epoch: 1 step: 911, loss is 1.8866280317306519\n",
      "epoch: 1 step: 912, loss is 1.7245327234268188\n",
      "epoch: 1 step: 913, loss is 1.7871859073638916\n",
      "epoch: 1 step: 914, loss is 1.5654711723327637\n",
      "epoch: 1 step: 915, loss is 1.6698012351989746\n",
      "epoch: 1 step: 916, loss is 1.5304532051086426\n",
      "epoch: 1 step: 917, loss is 1.3849769830703735\n",
      "epoch: 1 step: 918, loss is 1.5257446765899658\n",
      "epoch: 1 step: 919, loss is 1.2107656002044678\n",
      "epoch: 1 step: 920, loss is 1.7213938236236572\n",
      "epoch: 1 step: 921, loss is 1.2774592638015747\n",
      "epoch: 1 step: 922, loss is 1.263303518295288\n",
      "epoch: 1 step: 923, loss is 1.4295215606689453\n",
      "epoch: 1 step: 924, loss is 1.3719562292099\n",
      "epoch: 1 step: 925, loss is 1.5404984951019287\n",
      "epoch: 1 step: 926, loss is 1.2614045143127441\n",
      "epoch: 1 step: 927, loss is 1.4042222499847412\n",
      "epoch: 1 step: 928, loss is 0.7999719977378845\n",
      "epoch: 1 step: 929, loss is 1.5793335437774658\n",
      "epoch: 1 step: 930, loss is 1.4816765785217285\n",
      "epoch: 1 step: 931, loss is 1.1782112121582031\n",
      "epoch: 1 step: 932, loss is 1.3229279518127441\n",
      "epoch: 1 step: 933, loss is 1.5931981801986694\n",
      "epoch: 1 step: 934, loss is 1.298699975013733\n",
      "epoch: 1 step: 935, loss is 1.306667447090149\n",
      "epoch: 1 step: 936, loss is 1.2864148616790771\n",
      "epoch: 1 step: 937, loss is 1.1741387844085693\n",
      "epoch: 1 step: 938, loss is 1.1178205013275146\n",
      "epoch: 1 step: 939, loss is 1.1284602880477905\n",
      "epoch: 1 step: 940, loss is 1.385860800743103\n",
      "epoch: 1 step: 941, loss is 1.350820779800415\n",
      "epoch: 1 step: 942, loss is 1.4583957195281982\n",
      "epoch: 1 step: 943, loss is 1.405592679977417\n",
      "epoch: 1 step: 944, loss is 1.0601190328598022\n",
      "epoch: 1 step: 945, loss is 0.799640417098999\n",
      "epoch: 1 step: 946, loss is 1.2567636966705322\n",
      "epoch: 1 step: 947, loss is 0.8276633024215698\n",
      "epoch: 1 step: 948, loss is 0.7280160784721375\n",
      "epoch: 1 step: 949, loss is 0.8980695009231567\n",
      "epoch: 1 step: 950, loss is 0.7147472500801086\n",
      "epoch: 1 step: 951, loss is 1.489688515663147\n",
      "epoch: 1 step: 952, loss is 0.7103725671768188\n",
      "epoch: 1 step: 953, loss is 0.9169273972511292\n",
      "epoch: 1 step: 954, loss is 0.7646287083625793\n",
      "epoch: 1 step: 955, loss is 0.6976931691169739\n",
      "epoch: 1 step: 956, loss is 0.5437743663787842\n",
      "epoch: 1 step: 957, loss is 1.0544151067733765\n",
      "epoch: 1 step: 958, loss is 0.5753629207611084\n",
      "epoch: 1 step: 959, loss is 0.7905005812644958\n",
      "epoch: 1 step: 960, loss is 0.33352208137512207\n",
      "epoch: 1 step: 961, loss is 0.6929081678390503\n",
      "epoch: 1 step: 962, loss is 1.4424490928649902\n",
      "epoch: 1 step: 963, loss is 0.8981469869613647\n",
      "epoch: 1 step: 964, loss is 1.3261855840682983\n",
      "epoch: 1 step: 965, loss is 1.0074291229248047\n",
      "epoch: 1 step: 966, loss is 0.847113847732544\n",
      "epoch: 1 step: 967, loss is 0.9603301286697388\n",
      "epoch: 1 step: 968, loss is 0.8935704231262207\n",
      "epoch: 1 step: 969, loss is 0.6119394302368164\n",
      "epoch: 1 step: 970, loss is 1.0262632369995117\n",
      "epoch: 1 step: 971, loss is 0.8452969193458557\n",
      "epoch: 1 step: 972, loss is 0.5273094177246094\n",
      "epoch: 1 step: 973, loss is 0.6930891275405884\n",
      "epoch: 1 step: 974, loss is 0.7316017746925354\n",
      "epoch: 1 step: 975, loss is 0.8601052165031433\n",
      "epoch: 1 step: 976, loss is 0.5092469453811646\n",
      "epoch: 1 step: 977, loss is 0.7049400210380554\n",
      "epoch: 1 step: 978, loss is 1.177072525024414\n",
      "epoch: 1 step: 979, loss is 0.971461296081543\n",
      "epoch: 1 step: 980, loss is 0.7119125127792358\n",
      "epoch: 1 step: 981, loss is 0.5273135304450989\n",
      "epoch: 1 step: 982, loss is 0.4602096378803253\n",
      "epoch: 1 step: 983, loss is 0.32587647438049316\n",
      "epoch: 1 step: 984, loss is 0.6115076541900635\n",
      "epoch: 1 step: 985, loss is 0.6628243923187256\n",
      "epoch: 1 step: 986, loss is 0.6227841377258301\n",
      "epoch: 1 step: 987, loss is 0.8077588081359863\n",
      "epoch: 1 step: 988, loss is 0.4224511682987213\n",
      "epoch: 1 step: 989, loss is 0.549592137336731\n",
      "epoch: 1 step: 990, loss is 0.5887297987937927\n",
      "epoch: 1 step: 991, loss is 0.9236767292022705\n",
      "epoch: 1 step: 992, loss is 0.3715536594390869\n",
      "epoch: 1 step: 993, loss is 0.43710434436798096\n",
      "epoch: 1 step: 994, loss is 0.30044659972190857\n",
      "epoch: 1 step: 995, loss is 0.5106936097145081\n",
      "epoch: 1 step: 996, loss is 0.43902236223220825\n",
      "epoch: 1 step: 997, loss is 0.28237825632095337\n",
      "epoch: 1 step: 998, loss is 0.4081615209579468\n",
      "epoch: 1 step: 999, loss is 0.47569775581359863\n",
      "epoch: 1 step: 1000, loss is 0.645527720451355\n",
      "epoch: 1 step: 1001, loss is 0.7219129800796509\n",
      "epoch: 1 step: 1002, loss is 0.4925323724746704\n",
      "epoch: 1 step: 1003, loss is 0.7075324058532715\n",
      "epoch: 1 step: 1004, loss is 0.859996497631073\n",
      "epoch: 1 step: 1005, loss is 0.9041873216629028\n",
      "epoch: 1 step: 1006, loss is 0.7666066288948059\n",
      "epoch: 1 step: 1007, loss is 0.5082585215568542\n",
      "epoch: 1 step: 1008, loss is 0.3692707419395447\n",
      "epoch: 1 step: 1009, loss is 0.6555547714233398\n",
      "epoch: 1 step: 1010, loss is 0.7813495397567749\n",
      "epoch: 1 step: 1011, loss is 0.4989786148071289\n",
      "epoch: 1 step: 1012, loss is 0.6585968732833862\n",
      "epoch: 1 step: 1013, loss is 0.6562236547470093\n",
      "epoch: 1 step: 1014, loss is 0.3516780734062195\n",
      "epoch: 1 step: 1015, loss is 0.5327575206756592\n",
      "epoch: 1 step: 1016, loss is 0.7702457308769226\n",
      "epoch: 1 step: 1017, loss is 0.5718956589698792\n",
      "epoch: 1 step: 1018, loss is 0.38073986768722534\n",
      "epoch: 1 step: 1019, loss is 0.5468862652778625\n",
      "epoch: 1 step: 1020, loss is 0.4921302795410156\n",
      "epoch: 1 step: 1021, loss is 0.7939680814743042\n",
      "epoch: 1 step: 1022, loss is 0.6183199882507324\n",
      "epoch: 1 step: 1023, loss is 0.7033411264419556\n",
      "epoch: 1 step: 1024, loss is 0.4928058981895447\n",
      "epoch: 1 step: 1025, loss is 0.5528243184089661\n",
      "epoch: 1 step: 1026, loss is 0.33488547801971436\n",
      "epoch: 1 step: 1027, loss is 0.5176119804382324\n",
      "epoch: 1 step: 1028, loss is 0.21796303987503052\n",
      "epoch: 1 step: 1029, loss is 0.7597923874855042\n",
      "epoch: 1 step: 1030, loss is 0.44461190700531006\n",
      "epoch: 1 step: 1031, loss is 0.30865222215652466\n",
      "epoch: 1 step: 1032, loss is 0.572811484336853\n",
      "epoch: 1 step: 1033, loss is 0.6101489067077637\n",
      "epoch: 1 step: 1034, loss is 0.45272859930992126\n",
      "epoch: 1 step: 1035, loss is 0.4518246054649353\n",
      "epoch: 1 step: 1036, loss is 0.3709060549736023\n",
      "epoch: 1 step: 1037, loss is 0.33225250244140625\n",
      "epoch: 1 step: 1038, loss is 0.4382370114326477\n",
      "epoch: 1 step: 1039, loss is 0.42971521615982056\n",
      "epoch: 1 step: 1040, loss is 0.38586071133613586\n",
      "epoch: 1 step: 1041, loss is 0.38089519739151\n",
      "epoch: 1 step: 1042, loss is 0.18658418953418732\n",
      "epoch: 1 step: 1043, loss is 0.40499669313430786\n",
      "epoch: 1 step: 1044, loss is 0.48269665241241455\n",
      "epoch: 1 step: 1045, loss is 0.21490082144737244\n",
      "epoch: 1 step: 1046, loss is 0.4451817572116852\n",
      "epoch: 1 step: 1047, loss is 0.392639696598053\n",
      "epoch: 1 step: 1048, loss is 0.2891351580619812\n",
      "epoch: 1 step: 1049, loss is 0.9498989582061768\n",
      "epoch: 1 step: 1050, loss is 0.7511106133460999\n",
      "epoch: 1 step: 1051, loss is 0.11328879743814468\n",
      "epoch: 1 step: 1052, loss is 0.3206831216812134\n",
      "epoch: 1 step: 1053, loss is 0.2687307596206665\n",
      "epoch: 1 step: 1054, loss is 0.22154118120670319\n",
      "epoch: 1 step: 1055, loss is 0.32440176606178284\n",
      "epoch: 1 step: 1056, loss is 0.5115342140197754\n",
      "epoch: 1 step: 1057, loss is 0.8175923228263855\n",
      "epoch: 1 step: 1058, loss is 0.24206659197807312\n",
      "epoch: 1 step: 1059, loss is 0.22029127180576324\n",
      "epoch: 1 step: 1060, loss is 0.5067911148071289\n",
      "epoch: 1 step: 1061, loss is 0.33899590373039246\n",
      "epoch: 1 step: 1062, loss is 0.18962782621383667\n",
      "epoch: 1 step: 1063, loss is 0.2273431122303009\n",
      "epoch: 1 step: 1064, loss is 0.29728075861930847\n",
      "epoch: 1 step: 1065, loss is 0.3642551600933075\n",
      "epoch: 1 step: 1066, loss is 0.44880160689353943\n",
      "epoch: 1 step: 1067, loss is 0.27931392192840576\n",
      "epoch: 1 step: 1068, loss is 0.6261383891105652\n",
      "epoch: 1 step: 1069, loss is 0.29992175102233887\n",
      "epoch: 1 step: 1070, loss is 0.26451975107192993\n",
      "epoch: 1 step: 1071, loss is 0.17489652335643768\n",
      "epoch: 1 step: 1072, loss is 0.17370595037937164\n",
      "epoch: 1 step: 1073, loss is 0.11513858288526535\n",
      "epoch: 1 step: 1074, loss is 0.4084348678588867\n",
      "epoch: 1 step: 1075, loss is 0.17312200367450714\n",
      "epoch: 1 step: 1076, loss is 0.2151593118906021\n",
      "epoch: 1 step: 1077, loss is 0.40255051851272583\n",
      "epoch: 1 step: 1078, loss is 0.15227070450782776\n",
      "epoch: 1 step: 1079, loss is 0.3275505006313324\n",
      "epoch: 1 step: 1080, loss is 0.5123289823532104\n",
      "epoch: 1 step: 1081, loss is 0.5517711639404297\n",
      "epoch: 1 step: 1082, loss is 0.3979147672653198\n",
      "epoch: 1 step: 1083, loss is 0.2706123888492584\n",
      "epoch: 1 step: 1084, loss is 0.2551608085632324\n",
      "epoch: 1 step: 1085, loss is 0.5068957209587097\n",
      "epoch: 1 step: 1086, loss is 0.21312874555587769\n",
      "epoch: 1 step: 1087, loss is 0.13101376593112946\n",
      "epoch: 1 step: 1088, loss is 0.22946536540985107\n",
      "epoch: 1 step: 1089, loss is 0.22085098922252655\n",
      "epoch: 1 step: 1090, loss is 0.34607136249542236\n",
      "epoch: 1 step: 1091, loss is 0.22564882040023804\n",
      "epoch: 1 step: 1092, loss is 0.4743621051311493\n",
      "epoch: 1 step: 1093, loss is 0.26613038778305054\n",
      "epoch: 1 step: 1094, loss is 0.4266911745071411\n",
      "epoch: 1 step: 1095, loss is 0.25078481435775757\n",
      "epoch: 1 step: 1096, loss is 0.0820130705833435\n",
      "epoch: 1 step: 1097, loss is 0.180607408285141\n",
      "epoch: 1 step: 1098, loss is 0.16568422317504883\n",
      "epoch: 1 step: 1099, loss is 0.7128018140792847\n",
      "epoch: 1 step: 1100, loss is 0.768743097782135\n",
      "epoch: 1 step: 1101, loss is 0.1893661618232727\n",
      "epoch: 1 step: 1102, loss is 0.13190001249313354\n",
      "epoch: 1 step: 1103, loss is 0.29269006848335266\n",
      "epoch: 1 step: 1104, loss is 0.31445059180259705\n",
      "epoch: 1 step: 1105, loss is 0.31455564498901367\n",
      "epoch: 1 step: 1106, loss is 0.18104493618011475\n",
      "epoch: 1 step: 1107, loss is 0.1485980749130249\n",
      "epoch: 1 step: 1108, loss is 0.13620324432849884\n",
      "epoch: 1 step: 1109, loss is 0.2305516004562378\n",
      "epoch: 1 step: 1110, loss is 0.19602477550506592\n",
      "epoch: 1 step: 1111, loss is 0.13326777517795563\n",
      "epoch: 1 step: 1112, loss is 0.11873241513967514\n",
      "epoch: 1 step: 1113, loss is 0.3930394947528839\n",
      "epoch: 1 step: 1114, loss is 1.0486021041870117\n",
      "epoch: 1 step: 1115, loss is 0.3878593444824219\n",
      "epoch: 1 step: 1116, loss is 0.4090459942817688\n",
      "epoch: 1 step: 1117, loss is 0.6215866804122925\n",
      "epoch: 1 step: 1118, loss is 0.20684191584587097\n",
      "epoch: 1 step: 1119, loss is 0.11920088529586792\n",
      "epoch: 1 step: 1120, loss is 0.28867781162261963\n",
      "epoch: 1 step: 1121, loss is 0.3351847529411316\n",
      "epoch: 1 step: 1122, loss is 0.3238753080368042\n",
      "epoch: 1 step: 1123, loss is 0.17707613110542297\n",
      "epoch: 1 step: 1124, loss is 0.22143253684043884\n",
      "epoch: 1 step: 1125, loss is 0.39249932765960693\n",
      "epoch: 1 step: 1126, loss is 0.3610932230949402\n",
      "epoch: 1 step: 1127, loss is 0.27401936054229736\n",
      "epoch: 1 step: 1128, loss is 0.22669027745723724\n",
      "epoch: 1 step: 1129, loss is 0.3970673978328705\n",
      "epoch: 1 step: 1130, loss is 0.24668967723846436\n",
      "epoch: 1 step: 1131, loss is 0.3275497257709503\n",
      "epoch: 1 step: 1132, loss is 0.13613086938858032\n",
      "epoch: 1 step: 1133, loss is 0.23787043988704681\n",
      "epoch: 1 step: 1134, loss is 0.0851362869143486\n",
      "epoch: 1 step: 1135, loss is 0.27585989236831665\n",
      "epoch: 1 step: 1136, loss is 0.9552536010742188\n",
      "epoch: 1 step: 1137, loss is 0.9490845203399658\n",
      "epoch: 1 step: 1138, loss is 0.27316609025001526\n",
      "epoch: 1 step: 1139, loss is 0.29373064637184143\n",
      "epoch: 1 step: 1140, loss is 0.6995511054992676\n",
      "epoch: 1 step: 1141, loss is 0.1408517211675644\n",
      "epoch: 1 step: 1142, loss is 0.5842766761779785\n",
      "epoch: 1 step: 1143, loss is 0.19023072719573975\n",
      "epoch: 1 step: 1144, loss is 0.21833431720733643\n",
      "epoch: 1 step: 1145, loss is 0.21745193004608154\n",
      "epoch: 1 step: 1146, loss is 0.3109845817089081\n",
      "epoch: 1 step: 1147, loss is 0.7887716293334961\n",
      "epoch: 1 step: 1148, loss is 0.307223379611969\n",
      "epoch: 1 step: 1149, loss is 0.4114360809326172\n",
      "epoch: 1 step: 1150, loss is 0.37042132019996643\n",
      "epoch: 1 step: 1151, loss is 0.22561311721801758\n",
      "epoch: 1 step: 1152, loss is 0.38696491718292236\n",
      "epoch: 1 step: 1153, loss is 0.23312896490097046\n",
      "epoch: 1 step: 1154, loss is 0.20172004401683807\n",
      "epoch: 1 step: 1155, loss is 0.3602665364742279\n",
      "epoch: 1 step: 1156, loss is 0.19976380467414856\n",
      "epoch: 1 step: 1157, loss is 0.5597682595252991\n",
      "epoch: 1 step: 1158, loss is 0.36078158020973206\n",
      "epoch: 1 step: 1159, loss is 0.16444924473762512\n",
      "epoch: 1 step: 1160, loss is 0.17921490967273712\n",
      "epoch: 1 step: 1161, loss is 0.3407159447669983\n",
      "epoch: 1 step: 1162, loss is 0.21587823331356049\n",
      "epoch: 1 step: 1163, loss is 0.23959478735923767\n",
      "epoch: 1 step: 1164, loss is 0.1900424361228943\n",
      "epoch: 1 step: 1165, loss is 0.19886383414268494\n",
      "epoch: 1 step: 1166, loss is 0.27947306632995605\n",
      "epoch: 1 step: 1167, loss is 0.1034749448299408\n",
      "epoch: 1 step: 1168, loss is 0.3384925127029419\n",
      "epoch: 1 step: 1169, loss is 0.5406767129898071\n",
      "epoch: 1 step: 1170, loss is 0.5030757784843445\n",
      "epoch: 1 step: 1171, loss is 0.16435851156711578\n",
      "epoch: 1 step: 1172, loss is 0.4024481475353241\n",
      "epoch: 1 step: 1173, loss is 0.12925007939338684\n",
      "epoch: 1 step: 1174, loss is 0.19108319282531738\n",
      "epoch: 1 step: 1175, loss is 0.07818402349948883\n",
      "epoch: 1 step: 1176, loss is 0.09003422409296036\n",
      "epoch: 1 step: 1177, loss is 0.3476419448852539\n",
      "epoch: 1 step: 1178, loss is 0.3943381607532501\n",
      "epoch: 1 step: 1179, loss is 0.2734965682029724\n",
      "epoch: 1 step: 1180, loss is 0.11160658299922943\n",
      "epoch: 1 step: 1181, loss is 0.1236855536699295\n",
      "epoch: 1 step: 1182, loss is 0.3101150095462799\n",
      "epoch: 1 step: 1183, loss is 0.17641249299049377\n",
      "epoch: 1 step: 1184, loss is 0.048690326511859894\n",
      "epoch: 1 step: 1185, loss is 0.07314690947532654\n",
      "epoch: 1 step: 1186, loss is 0.1289764642715454\n",
      "epoch: 1 step: 1187, loss is 0.23419132828712463\n",
      "epoch: 1 step: 1188, loss is 0.15686900913715363\n",
      "epoch: 1 step: 1189, loss is 0.37268972396850586\n",
      "epoch: 1 step: 1190, loss is 0.1490592658519745\n",
      "epoch: 1 step: 1191, loss is 0.02082282304763794\n",
      "epoch: 1 step: 1192, loss is 0.3167073130607605\n",
      "epoch: 1 step: 1193, loss is 0.22480058670043945\n",
      "epoch: 1 step: 1194, loss is 0.4114654064178467\n",
      "epoch: 1 step: 1195, loss is 0.4075544476509094\n",
      "epoch: 1 step: 1196, loss is 0.0981336459517479\n",
      "epoch: 1 step: 1197, loss is 0.2889665961265564\n",
      "epoch: 1 step: 1198, loss is 0.2415596842765808\n",
      "epoch: 1 step: 1199, loss is 0.03919985145330429\n",
      "epoch: 1 step: 1200, loss is 0.15558749437332153\n",
      "epoch: 1 step: 1201, loss is 0.4544670581817627\n",
      "epoch: 1 step: 1202, loss is 0.5809329152107239\n",
      "epoch: 1 step: 1203, loss is 0.38510662317276\n",
      "epoch: 1 step: 1204, loss is 0.36981528997421265\n",
      "epoch: 1 step: 1205, loss is 0.29879435896873474\n",
      "epoch: 1 step: 1206, loss is 0.42931509017944336\n",
      "epoch: 1 step: 1207, loss is 0.2212209403514862\n",
      "epoch: 1 step: 1208, loss is 0.08359967917203903\n",
      "epoch: 1 step: 1209, loss is 0.30500251054763794\n",
      "epoch: 1 step: 1210, loss is 0.13696923851966858\n",
      "epoch: 1 step: 1211, loss is 0.6603761315345764\n",
      "epoch: 1 step: 1212, loss is 0.4578492045402527\n",
      "epoch: 1 step: 1213, loss is 0.6488022804260254\n",
      "epoch: 1 step: 1214, loss is 0.28480374813079834\n",
      "epoch: 1 step: 1215, loss is 0.15359711647033691\n",
      "epoch: 1 step: 1216, loss is 0.3182218074798584\n",
      "epoch: 1 step: 1217, loss is 0.1723080575466156\n",
      "epoch: 1 step: 1218, loss is 0.19719353318214417\n",
      "epoch: 1 step: 1219, loss is 0.3296973705291748\n",
      "epoch: 1 step: 1220, loss is 0.29402264952659607\n",
      "epoch: 1 step: 1221, loss is 0.3246702551841736\n",
      "epoch: 1 step: 1222, loss is 0.22812731564044952\n",
      "epoch: 1 step: 1223, loss is 0.43256598711013794\n",
      "epoch: 1 step: 1224, loss is 0.15534129738807678\n",
      "epoch: 1 step: 1225, loss is 0.14366017282009125\n",
      "epoch: 1 step: 1226, loss is 0.10830102860927582\n",
      "epoch: 1 step: 1227, loss is 0.03160979598760605\n",
      "epoch: 1 step: 1228, loss is 0.3096860647201538\n",
      "epoch: 1 step: 1229, loss is 0.11036191880702972\n",
      "epoch: 1 step: 1230, loss is 0.22323644161224365\n",
      "epoch: 1 step: 1231, loss is 0.1517494171857834\n",
      "epoch: 1 step: 1232, loss is 0.10136143863201141\n",
      "epoch: 1 step: 1233, loss is 0.19788457453250885\n",
      "epoch: 1 step: 1234, loss is 0.4595000147819519\n",
      "epoch: 1 step: 1235, loss is 0.02820425108075142\n",
      "epoch: 1 step: 1236, loss is 0.21006590127944946\n",
      "epoch: 1 step: 1237, loss is 0.02465345710515976\n",
      "epoch: 1 step: 1238, loss is 0.14401663839817047\n",
      "epoch: 1 step: 1239, loss is 0.20041726529598236\n",
      "epoch: 1 step: 1240, loss is 0.18542057275772095\n",
      "epoch: 1 step: 1241, loss is 0.2813394069671631\n",
      "epoch: 1 step: 1242, loss is 0.16243606805801392\n",
      "epoch: 1 step: 1243, loss is 0.16890481114387512\n",
      "epoch: 1 step: 1244, loss is 0.022851068526506424\n",
      "epoch: 1 step: 1245, loss is 0.188298299908638\n",
      "epoch: 1 step: 1246, loss is 0.5240920186042786\n",
      "epoch: 1 step: 1247, loss is 0.06724990159273148\n",
      "epoch: 1 step: 1248, loss is 0.057465456426143646\n",
      "epoch: 1 step: 1249, loss is 0.18990477919578552\n",
      "epoch: 1 step: 1250, loss is 0.22201265394687653\n",
      "epoch: 1 step: 1251, loss is 0.161464661359787\n",
      "epoch: 1 step: 1252, loss is 0.27215611934661865\n",
      "epoch: 1 step: 1253, loss is 0.1366843283176422\n",
      "epoch: 1 step: 1254, loss is 0.2294912487268448\n",
      "epoch: 1 step: 1255, loss is 0.09038607776165009\n",
      "epoch: 1 step: 1256, loss is 0.09246983379125595\n",
      "epoch: 1 step: 1257, loss is 0.08668394386768341\n",
      "epoch: 1 step: 1258, loss is 0.3168979287147522\n",
      "epoch: 1 step: 1259, loss is 0.14415372908115387\n",
      "epoch: 1 step: 1260, loss is 0.12992975115776062\n",
      "epoch: 1 step: 1261, loss is 0.08241309970617294\n",
      "epoch: 1 step: 1262, loss is 0.18912361562252045\n",
      "epoch: 1 step: 1263, loss is 0.27481818199157715\n",
      "epoch: 1 step: 1264, loss is 0.16306322813034058\n",
      "epoch: 1 step: 1265, loss is 0.5493113398551941\n",
      "epoch: 1 step: 1266, loss is 0.2737005949020386\n",
      "epoch: 1 step: 1267, loss is 0.13726532459259033\n",
      "epoch: 1 step: 1268, loss is 0.3385869264602661\n",
      "epoch: 1 step: 1269, loss is 0.11305847764015198\n",
      "epoch: 1 step: 1270, loss is 0.0360521525144577\n",
      "epoch: 1 step: 1271, loss is 0.17130285501480103\n",
      "epoch: 1 step: 1272, loss is 0.3555315434932709\n",
      "epoch: 1 step: 1273, loss is 0.420337438583374\n",
      "epoch: 1 step: 1274, loss is 0.08363696932792664\n",
      "epoch: 1 step: 1275, loss is 0.038770534098148346\n",
      "epoch: 1 step: 1276, loss is 0.15727022290229797\n",
      "epoch: 1 step: 1277, loss is 0.19539278745651245\n",
      "epoch: 1 step: 1278, loss is 0.09066058695316315\n",
      "epoch: 1 step: 1279, loss is 0.15550442039966583\n",
      "epoch: 1 step: 1280, loss is 0.28645989298820496\n",
      "epoch: 1 step: 1281, loss is 0.36809927225112915\n",
      "epoch: 1 step: 1282, loss is 0.38090217113494873\n",
      "epoch: 1 step: 1283, loss is 0.18904715776443481\n",
      "epoch: 1 step: 1284, loss is 0.17463171482086182\n",
      "epoch: 1 step: 1285, loss is 0.1983160674571991\n",
      "epoch: 1 step: 1286, loss is 0.2615762948989868\n",
      "epoch: 1 step: 1287, loss is 0.293802946805954\n",
      "epoch: 1 step: 1288, loss is 0.29339513182640076\n",
      "epoch: 1 step: 1289, loss is 0.259924054145813\n",
      "epoch: 1 step: 1290, loss is 0.40785127878189087\n",
      "epoch: 1 step: 1291, loss is 0.05485176295042038\n",
      "epoch: 1 step: 1292, loss is 0.2912229895591736\n",
      "epoch: 1 step: 1293, loss is 0.21009385585784912\n",
      "epoch: 1 step: 1294, loss is 0.0993194580078125\n",
      "epoch: 1 step: 1295, loss is 0.14841468632221222\n",
      "epoch: 1 step: 1296, loss is 0.03497612848877907\n",
      "epoch: 1 step: 1297, loss is 0.28661268949508667\n",
      "epoch: 1 step: 1298, loss is 0.1927119791507721\n",
      "epoch: 1 step: 1299, loss is 0.11681683361530304\n",
      "epoch: 1 step: 1300, loss is 0.09593503922224045\n",
      "epoch: 1 step: 1301, loss is 0.10779480636119843\n",
      "epoch: 1 step: 1302, loss is 0.267869234085083\n",
      "epoch: 1 step: 1303, loss is 0.12440814077854156\n",
      "epoch: 1 step: 1304, loss is 0.0966157466173172\n",
      "epoch: 1 step: 1305, loss is 0.3481740355491638\n",
      "epoch: 1 step: 1306, loss is 0.595885694026947\n",
      "epoch: 1 step: 1307, loss is 0.28651589155197144\n",
      "epoch: 1 step: 1308, loss is 0.3731955587863922\n",
      "epoch: 1 step: 1309, loss is 0.312019944190979\n",
      "epoch: 1 step: 1310, loss is 0.2925926148891449\n",
      "epoch: 1 step: 1311, loss is 0.15113399922847748\n",
      "epoch: 1 step: 1312, loss is 0.2555484175682068\n",
      "epoch: 1 step: 1313, loss is 0.10366566479206085\n",
      "epoch: 1 step: 1314, loss is 0.09960900247097015\n",
      "epoch: 1 step: 1315, loss is 0.09837707877159119\n",
      "epoch: 1 step: 1316, loss is 0.049217816442251205\n",
      "epoch: 1 step: 1317, loss is 0.08347073197364807\n",
      "epoch: 1 step: 1318, loss is 0.24232858419418335\n",
      "epoch: 1 step: 1319, loss is 0.02314777486026287\n",
      "epoch: 1 step: 1320, loss is 0.18342678248882294\n",
      "epoch: 1 step: 1321, loss is 0.31542524695396423\n",
      "epoch: 1 step: 1322, loss is 0.19134217500686646\n",
      "epoch: 1 step: 1323, loss is 0.16034632921218872\n",
      "epoch: 1 step: 1324, loss is 0.408243864774704\n",
      "epoch: 1 step: 1325, loss is 0.2527397871017456\n",
      "epoch: 1 step: 1326, loss is 0.14286673069000244\n",
      "epoch: 1 step: 1327, loss is 0.2998138666152954\n",
      "epoch: 1 step: 1328, loss is 0.2673555016517639\n",
      "epoch: 1 step: 1329, loss is 0.13421165943145752\n",
      "epoch: 1 step: 1330, loss is 0.3072798252105713\n",
      "epoch: 1 step: 1331, loss is 0.1754910796880722\n",
      "epoch: 1 step: 1332, loss is 0.11936846375465393\n",
      "epoch: 1 step: 1333, loss is 0.1588122844696045\n",
      "epoch: 1 step: 1334, loss is 0.04268653318285942\n",
      "epoch: 1 step: 1335, loss is 0.10135328769683838\n",
      "epoch: 1 step: 1336, loss is 0.1426248699426651\n",
      "epoch: 1 step: 1337, loss is 0.1414983570575714\n",
      "epoch: 1 step: 1338, loss is 0.03495193272829056\n",
      "epoch: 1 step: 1339, loss is 0.07959546893835068\n",
      "epoch: 1 step: 1340, loss is 0.40244582295417786\n",
      "epoch: 1 step: 1341, loss is 0.17336535453796387\n",
      "epoch: 1 step: 1342, loss is 0.07244797796010971\n",
      "epoch: 1 step: 1343, loss is 0.35583287477493286\n",
      "epoch: 1 step: 1344, loss is 0.24858836829662323\n",
      "epoch: 1 step: 1345, loss is 0.1410840004682541\n",
      "epoch: 1 step: 1346, loss is 0.23520582914352417\n",
      "epoch: 1 step: 1347, loss is 0.25566697120666504\n",
      "epoch: 1 step: 1348, loss is 0.06922642141580582\n",
      "epoch: 1 step: 1349, loss is 0.062096670269966125\n",
      "epoch: 1 step: 1350, loss is 0.01554480753839016\n",
      "epoch: 1 step: 1351, loss is 0.08714312314987183\n",
      "epoch: 1 step: 1352, loss is 0.21727342903614044\n",
      "epoch: 1 step: 1353, loss is 0.07174379378557205\n",
      "epoch: 1 step: 1354, loss is 0.02790229022502899\n",
      "epoch: 1 step: 1355, loss is 0.1502133309841156\n",
      "epoch: 1 step: 1356, loss is 0.09750720113515854\n",
      "epoch: 1 step: 1357, loss is 0.10395540297031403\n",
      "epoch: 1 step: 1358, loss is 0.26017525792121887\n",
      "epoch: 1 step: 1359, loss is 0.20391350984573364\n",
      "epoch: 1 step: 1360, loss is 0.16789647936820984\n",
      "epoch: 1 step: 1361, loss is 0.2827399969100952\n",
      "epoch: 1 step: 1362, loss is 0.06906238943338394\n",
      "epoch: 1 step: 1363, loss is 0.021188566461205482\n",
      "epoch: 1 step: 1364, loss is 0.3158368766307831\n",
      "epoch: 1 step: 1365, loss is 0.43799853324890137\n",
      "epoch: 1 step: 1366, loss is 0.45523935556411743\n",
      "epoch: 1 step: 1367, loss is 0.12531723082065582\n",
      "epoch: 1 step: 1368, loss is 0.061555273830890656\n",
      "epoch: 1 step: 1369, loss is 0.1672021597623825\n",
      "epoch: 1 step: 1370, loss is 0.194532573223114\n",
      "epoch: 1 step: 1371, loss is 0.3083272874355316\n",
      "epoch: 1 step: 1372, loss is 0.08448150753974915\n",
      "epoch: 1 step: 1373, loss is 0.12076207995414734\n",
      "epoch: 1 step: 1374, loss is 0.11033248901367188\n",
      "epoch: 1 step: 1375, loss is 0.11881063133478165\n",
      "epoch: 1 step: 1376, loss is 0.16229544579982758\n",
      "epoch: 1 step: 1377, loss is 0.13526900112628937\n",
      "epoch: 1 step: 1378, loss is 0.09440109878778458\n",
      "epoch: 1 step: 1379, loss is 0.30107682943344116\n",
      "epoch: 1 step: 1380, loss is 0.011427978985011578\n",
      "epoch: 1 step: 1381, loss is 0.26991480588912964\n",
      "epoch: 1 step: 1382, loss is 0.07044093310832977\n",
      "epoch: 1 step: 1383, loss is 0.03455265983939171\n",
      "epoch: 1 step: 1384, loss is 0.14172551035881042\n",
      "epoch: 1 step: 1385, loss is 0.26893195509910583\n",
      "epoch: 1 step: 1386, loss is 0.24656029045581818\n",
      "epoch: 1 step: 1387, loss is 0.24054798483848572\n",
      "epoch: 1 step: 1388, loss is 0.1601133644580841\n",
      "epoch: 1 step: 1389, loss is 0.21738994121551514\n",
      "epoch: 1 step: 1390, loss is 0.04417135566473007\n",
      "epoch: 1 step: 1391, loss is 0.10917223989963531\n",
      "epoch: 1 step: 1392, loss is 0.19884051382541656\n",
      "epoch: 1 step: 1393, loss is 0.3324693739414215\n",
      "epoch: 1 step: 1394, loss is 0.3890765309333801\n",
      "epoch: 1 step: 1395, loss is 0.09839454293251038\n",
      "epoch: 1 step: 1396, loss is 0.3115462064743042\n",
      "epoch: 1 step: 1397, loss is 0.3294503092765808\n",
      "epoch: 1 step: 1398, loss is 0.03267116844654083\n",
      "epoch: 1 step: 1399, loss is 0.06979145109653473\n",
      "epoch: 1 step: 1400, loss is 0.1037365049123764\n",
      "epoch: 1 step: 1401, loss is 0.13606907427310944\n",
      "epoch: 1 step: 1402, loss is 0.268233060836792\n",
      "epoch: 1 step: 1403, loss is 0.10571374744176865\n",
      "epoch: 1 step: 1404, loss is 0.2035924792289734\n",
      "epoch: 1 step: 1405, loss is 0.32803428173065186\n",
      "epoch: 1 step: 1406, loss is 0.24882784485816956\n",
      "epoch: 1 step: 1407, loss is 0.05787050351500511\n",
      "epoch: 1 step: 1408, loss is 0.20329394936561584\n",
      "epoch: 1 step: 1409, loss is 0.33138737082481384\n",
      "epoch: 1 step: 1410, loss is 0.11862324923276901\n",
      "epoch: 1 step: 1411, loss is 0.12811312079429626\n",
      "epoch: 1 step: 1412, loss is 0.3309286832809448\n",
      "epoch: 1 step: 1413, loss is 0.19266270101070404\n",
      "epoch: 1 step: 1414, loss is 0.11824768781661987\n",
      "epoch: 1 step: 1415, loss is 0.16136100888252258\n",
      "epoch: 1 step: 1416, loss is 0.23531880974769592\n",
      "epoch: 1 step: 1417, loss is 0.140736922621727\n",
      "epoch: 1 step: 1418, loss is 0.13762730360031128\n",
      "epoch: 1 step: 1419, loss is 0.054475486278533936\n",
      "epoch: 1 step: 1420, loss is 0.03317827358841896\n",
      "epoch: 1 step: 1421, loss is 0.19574011862277985\n",
      "epoch: 1 step: 1422, loss is 0.20455026626586914\n",
      "epoch: 1 step: 1423, loss is 0.2592720091342926\n",
      "epoch: 1 step: 1424, loss is 0.18765050172805786\n",
      "epoch: 1 step: 1425, loss is 0.1589994728565216\n",
      "epoch: 1 step: 1426, loss is 0.056655656546354294\n",
      "epoch: 1 step: 1427, loss is 0.041863441467285156\n",
      "epoch: 1 step: 1428, loss is 0.04938094690442085\n",
      "epoch: 1 step: 1429, loss is 0.26657891273498535\n",
      "epoch: 1 step: 1430, loss is 0.342934250831604\n",
      "epoch: 1 step: 1431, loss is 0.03297712281346321\n",
      "epoch: 1 step: 1432, loss is 0.0772259309887886\n",
      "epoch: 1 step: 1433, loss is 0.3341454863548279\n",
      "epoch: 1 step: 1434, loss is 0.33692067861557007\n",
      "epoch: 1 step: 1435, loss is 0.17704524099826813\n",
      "epoch: 1 step: 1436, loss is 0.2701877951622009\n",
      "epoch: 1 step: 1437, loss is 0.12798762321472168\n",
      "epoch: 1 step: 1438, loss is 0.5291533470153809\n",
      "epoch: 1 step: 1439, loss is 0.32221564650535583\n",
      "epoch: 1 step: 1440, loss is 0.11333299428224564\n",
      "epoch: 1 step: 1441, loss is 0.06265820562839508\n",
      "epoch: 1 step: 1442, loss is 0.06661973893642426\n",
      "epoch: 1 step: 1443, loss is 0.15000003576278687\n",
      "epoch: 1 step: 1444, loss is 0.0380055233836174\n",
      "epoch: 1 step: 1445, loss is 0.3132025897502899\n",
      "epoch: 1 step: 1446, loss is 0.1533692181110382\n",
      "epoch: 1 step: 1447, loss is 0.046547744423151016\n",
      "epoch: 1 step: 1448, loss is 0.38122984766960144\n",
      "epoch: 1 step: 1449, loss is 0.24802330136299133\n",
      "epoch: 1 step: 1450, loss is 0.21885152161121368\n",
      "epoch: 1 step: 1451, loss is 0.16306908428668976\n",
      "epoch: 1 step: 1452, loss is 0.14064468443393707\n",
      "epoch: 1 step: 1453, loss is 0.14428552985191345\n",
      "epoch: 1 step: 1454, loss is 0.28118032217025757\n",
      "epoch: 1 step: 1455, loss is 0.04120245203375816\n",
      "epoch: 1 step: 1456, loss is 0.10792724788188934\n",
      "epoch: 1 step: 1457, loss is 0.4944304823875427\n",
      "epoch: 1 step: 1458, loss is 0.11999013274908066\n",
      "epoch: 1 step: 1459, loss is 0.1995057314634323\n",
      "epoch: 1 step: 1460, loss is 0.05112295597791672\n",
      "epoch: 1 step: 1461, loss is 0.21482472121715546\n",
      "epoch: 1 step: 1462, loss is 0.18980272114276886\n",
      "epoch: 1 step: 1463, loss is 0.03855786472558975\n",
      "epoch: 1 step: 1464, loss is 0.17926818132400513\n",
      "epoch: 1 step: 1465, loss is 0.1246238648891449\n",
      "epoch: 1 step: 1466, loss is 0.0863921046257019\n",
      "epoch: 1 step: 1467, loss is 0.3615139126777649\n",
      "epoch: 1 step: 1468, loss is 0.13016894459724426\n",
      "epoch: 1 step: 1469, loss is 0.06185372173786163\n",
      "epoch: 1 step: 1470, loss is 0.31555014848709106\n",
      "epoch: 1 step: 1471, loss is 0.22474196553230286\n",
      "epoch: 1 step: 1472, loss is 0.17337468266487122\n",
      "epoch: 1 step: 1473, loss is 0.19656626880168915\n",
      "epoch: 1 step: 1474, loss is 0.11998845636844635\n",
      "epoch: 1 step: 1475, loss is 0.2098916471004486\n",
      "epoch: 1 step: 1476, loss is 0.21872110664844513\n",
      "epoch: 1 step: 1477, loss is 0.0814470499753952\n",
      "epoch: 1 step: 1478, loss is 0.15623100101947784\n",
      "epoch: 1 step: 1479, loss is 0.05509914085268974\n",
      "epoch: 1 step: 1480, loss is 0.07942061871290207\n",
      "epoch: 1 step: 1481, loss is 0.18352556228637695\n",
      "epoch: 1 step: 1482, loss is 0.04474622383713722\n",
      "epoch: 1 step: 1483, loss is 0.13273049890995026\n",
      "epoch: 1 step: 1484, loss is 0.09864136576652527\n",
      "epoch: 1 step: 1485, loss is 0.0788307636976242\n",
      "epoch: 1 step: 1486, loss is 0.1492318958044052\n",
      "epoch: 1 step: 1487, loss is 0.24754801392555237\n",
      "epoch: 1 step: 1488, loss is 0.30055615305900574\n",
      "epoch: 1 step: 1489, loss is 0.023373672738671303\n",
      "epoch: 1 step: 1490, loss is 0.02439265511929989\n",
      "epoch: 1 step: 1491, loss is 0.16658350825309753\n",
      "epoch: 1 step: 1492, loss is 0.1489940881729126\n",
      "epoch: 1 step: 1493, loss is 0.06664122641086578\n",
      "epoch: 1 step: 1494, loss is 0.06994844973087311\n",
      "epoch: 1 step: 1495, loss is 0.06383202224969864\n",
      "epoch: 1 step: 1496, loss is 0.2021501213312149\n",
      "epoch: 1 step: 1497, loss is 0.04708234965801239\n",
      "epoch: 1 step: 1498, loss is 0.36540767550468445\n",
      "epoch: 1 step: 1499, loss is 0.1338157206773758\n",
      "epoch: 1 step: 1500, loss is 0.36177948117256165\n",
      "epoch: 1 step: 1501, loss is 0.09145084768533707\n",
      "epoch: 1 step: 1502, loss is 0.263847291469574\n",
      "epoch: 1 step: 1503, loss is 0.2499198615550995\n",
      "epoch: 1 step: 1504, loss is 0.11150535941123962\n",
      "epoch: 1 step: 1505, loss is 0.22307530045509338\n",
      "epoch: 1 step: 1506, loss is 0.11537927389144897\n",
      "epoch: 1 step: 1507, loss is 0.179799884557724\n",
      "epoch: 1 step: 1508, loss is 0.15216343104839325\n",
      "epoch: 1 step: 1509, loss is 0.19537916779518127\n",
      "epoch: 1 step: 1510, loss is 0.07743680477142334\n",
      "epoch: 1 step: 1511, loss is 0.30878302454948425\n",
      "epoch: 1 step: 1512, loss is 0.1779150515794754\n",
      "epoch: 1 step: 1513, loss is 0.15792055428028107\n",
      "epoch: 1 step: 1514, loss is 0.05873342230916023\n",
      "epoch: 1 step: 1515, loss is 0.1823814958333969\n",
      "epoch: 1 step: 1516, loss is 0.1615563929080963\n",
      "epoch: 1 step: 1517, loss is 0.13040098547935486\n",
      "epoch: 1 step: 1518, loss is 0.0252341628074646\n",
      "epoch: 1 step: 1519, loss is 0.18695147335529327\n",
      "epoch: 1 step: 1520, loss is 0.19153863191604614\n",
      "epoch: 1 step: 1521, loss is 0.32716482877731323\n",
      "epoch: 1 step: 1522, loss is 0.13522198796272278\n",
      "epoch: 1 step: 1523, loss is 0.12389933317899704\n",
      "epoch: 1 step: 1524, loss is 0.32858768105506897\n",
      "epoch: 1 step: 1525, loss is 0.4787452816963196\n",
      "epoch: 1 step: 1526, loss is 0.318686842918396\n",
      "epoch: 1 step: 1527, loss is 0.23605871200561523\n",
      "epoch: 1 step: 1528, loss is 0.03881855309009552\n",
      "epoch: 1 step: 1529, loss is 0.055546537041664124\n",
      "epoch: 1 step: 1530, loss is 0.05467752367258072\n",
      "epoch: 1 step: 1531, loss is 0.056964173913002014\n",
      "epoch: 1 step: 1532, loss is 0.8077918291091919\n",
      "epoch: 1 step: 1533, loss is 0.18383754789829254\n",
      "epoch: 1 step: 1534, loss is 0.01268628891557455\n",
      "epoch: 1 step: 1535, loss is 0.3047693073749542\n",
      "epoch: 1 step: 1536, loss is 0.1404331773519516\n",
      "epoch: 1 step: 1537, loss is 0.15088078379631042\n",
      "epoch: 1 step: 1538, loss is 0.3417392671108246\n",
      "epoch: 1 step: 1539, loss is 0.07542228698730469\n",
      "epoch: 1 step: 1540, loss is 0.20281948149204254\n",
      "epoch: 1 step: 1541, loss is 0.10916506499052048\n",
      "epoch: 1 step: 1542, loss is 0.13274288177490234\n",
      "epoch: 1 step: 1543, loss is 0.19451743364334106\n",
      "epoch: 1 step: 1544, loss is 0.06093606725335121\n",
      "epoch: 1 step: 1545, loss is 0.03983324021100998\n",
      "epoch: 1 step: 1546, loss is 0.1307947039604187\n",
      "epoch: 1 step: 1547, loss is 0.31699874997138977\n",
      "epoch: 1 step: 1548, loss is 0.4697931408882141\n",
      "epoch: 1 step: 1549, loss is 0.06032039225101471\n",
      "epoch: 1 step: 1550, loss is 0.05296491086483002\n",
      "epoch: 1 step: 1551, loss is 0.05614813417196274\n",
      "epoch: 1 step: 1552, loss is 0.11855245381593704\n",
      "epoch: 1 step: 1553, loss is 0.19202131032943726\n",
      "epoch: 1 step: 1554, loss is 0.27075785398483276\n",
      "epoch: 1 step: 1555, loss is 0.16387346386909485\n",
      "epoch: 1 step: 1556, loss is 0.08008221536874771\n",
      "epoch: 1 step: 1557, loss is 0.14275102317333221\n",
      "epoch: 1 step: 1558, loss is 0.11878009140491486\n",
      "epoch: 1 step: 1559, loss is 0.2718552052974701\n",
      "epoch: 1 step: 1560, loss is 0.2663634419441223\n",
      "epoch: 1 step: 1561, loss is 0.30915576219558716\n",
      "epoch: 1 step: 1562, loss is 0.0573776438832283\n",
      "epoch: 1 step: 1563, loss is 0.12659138441085815\n",
      "epoch: 1 step: 1564, loss is 0.05217468738555908\n",
      "epoch: 1 step: 1565, loss is 0.0588061586022377\n",
      "epoch: 1 step: 1566, loss is 0.17200854420661926\n",
      "epoch: 1 step: 1567, loss is 0.04556535929441452\n",
      "epoch: 1 step: 1568, loss is 0.08950676023960114\n",
      "epoch: 1 step: 1569, loss is 0.12040576338768005\n",
      "epoch: 1 step: 1570, loss is 0.08185453712940216\n",
      "epoch: 1 step: 1571, loss is 0.2014143168926239\n",
      "epoch: 1 step: 1572, loss is 0.054800502955913544\n",
      "epoch: 1 step: 1573, loss is 0.2082194983959198\n",
      "epoch: 1 step: 1574, loss is 0.324984610080719\n",
      "epoch: 1 step: 1575, loss is 0.08490416407585144\n",
      "epoch: 1 step: 1576, loss is 0.16456477344036102\n",
      "epoch: 1 step: 1577, loss is 0.21109457314014435\n",
      "epoch: 1 step: 1578, loss is 0.3204851448535919\n",
      "epoch: 1 step: 1579, loss is 0.017877208068966866\n",
      "epoch: 1 step: 1580, loss is 0.1385047733783722\n",
      "epoch: 1 step: 1581, loss is 0.021069247275590897\n",
      "epoch: 1 step: 1582, loss is 0.23294943571090698\n",
      "epoch: 1 step: 1583, loss is 0.049446165561676025\n",
      "epoch: 1 step: 1584, loss is 0.11958486586809158\n",
      "epoch: 1 step: 1585, loss is 0.306479275226593\n",
      "epoch: 1 step: 1586, loss is 0.28960829973220825\n",
      "epoch: 1 step: 1587, loss is 0.15309655666351318\n",
      "epoch: 1 step: 1588, loss is 0.03629319742321968\n",
      "epoch: 1 step: 1589, loss is 0.0960950180888176\n",
      "epoch: 1 step: 1590, loss is 0.024343665689229965\n",
      "epoch: 1 step: 1591, loss is 0.3248617947101593\n",
      "epoch: 1 step: 1592, loss is 0.09157011657953262\n",
      "epoch: 1 step: 1593, loss is 0.07348036766052246\n",
      "epoch: 1 step: 1594, loss is 0.1247033178806305\n",
      "epoch: 1 step: 1595, loss is 0.19107288122177124\n",
      "epoch: 1 step: 1596, loss is 0.27878105640411377\n",
      "epoch: 1 step: 1597, loss is 0.08937868475914001\n",
      "epoch: 1 step: 1598, loss is 0.07804426550865173\n",
      "epoch: 1 step: 1599, loss is 0.16612868010997772\n",
      "epoch: 1 step: 1600, loss is 0.07489368319511414\n",
      "epoch: 1 step: 1601, loss is 0.1333983689546585\n",
      "epoch: 1 step: 1602, loss is 0.11266918480396271\n",
      "epoch: 1 step: 1603, loss is 0.06463318318128586\n",
      "epoch: 1 step: 1604, loss is 0.05410362035036087\n",
      "epoch: 1 step: 1605, loss is 0.2753201425075531\n",
      "epoch: 1 step: 1606, loss is 0.16168034076690674\n",
      "epoch: 1 step: 1607, loss is 0.485778272151947\n",
      "epoch: 1 step: 1608, loss is 0.13371455669403076\n",
      "epoch: 1 step: 1609, loss is 0.01814338192343712\n",
      "epoch: 1 step: 1610, loss is 0.02302045002579689\n",
      "epoch: 1 step: 1611, loss is 0.21958181262016296\n",
      "epoch: 1 step: 1612, loss is 0.29445528984069824\n",
      "epoch: 1 step: 1613, loss is 0.22850652039051056\n",
      "epoch: 1 step: 1614, loss is 0.10327523201704025\n",
      "epoch: 1 step: 1615, loss is 0.27045488357543945\n",
      "epoch: 1 step: 1616, loss is 0.060778114944696426\n",
      "epoch: 1 step: 1617, loss is 0.1371881514787674\n",
      "epoch: 1 step: 1618, loss is 0.267782986164093\n",
      "epoch: 1 step: 1619, loss is 0.20847904682159424\n",
      "epoch: 1 step: 1620, loss is 0.12008582055568695\n",
      "epoch: 1 step: 1621, loss is 0.07000139355659485\n",
      "epoch: 1 step: 1622, loss is 0.05380386859178543\n",
      "epoch: 1 step: 1623, loss is 0.08097346127033234\n",
      "epoch: 1 step: 1624, loss is 0.1746169924736023\n",
      "epoch: 1 step: 1625, loss is 0.057177118957042694\n",
      "epoch: 1 step: 1626, loss is 0.13887439668178558\n",
      "epoch: 1 step: 1627, loss is 0.34078913927078247\n",
      "epoch: 1 step: 1628, loss is 0.162061870098114\n",
      "epoch: 1 step: 1629, loss is 0.5106632709503174\n",
      "epoch: 1 step: 1630, loss is 0.046552084386348724\n",
      "epoch: 1 step: 1631, loss is 0.2800021767616272\n",
      "epoch: 1 step: 1632, loss is 0.07424159348011017\n",
      "epoch: 1 step: 1633, loss is 0.04262332245707512\n",
      "epoch: 1 step: 1634, loss is 0.014641322195529938\n",
      "epoch: 1 step: 1635, loss is 0.0632740706205368\n",
      "epoch: 1 step: 1636, loss is 0.024586953222751617\n",
      "epoch: 1 step: 1637, loss is 0.06510724872350693\n",
      "epoch: 1 step: 1638, loss is 0.1095067709684372\n",
      "epoch: 1 step: 1639, loss is 0.17033636569976807\n",
      "epoch: 1 step: 1640, loss is 0.060924019664525986\n",
      "epoch: 1 step: 1641, loss is 0.13687144219875336\n",
      "epoch: 1 step: 1642, loss is 0.21524259448051453\n",
      "epoch: 1 step: 1643, loss is 0.3379647433757782\n",
      "epoch: 1 step: 1644, loss is 0.10483992844820023\n",
      "epoch: 1 step: 1645, loss is 0.09045343101024628\n",
      "epoch: 1 step: 1646, loss is 0.10192880034446716\n",
      "epoch: 1 step: 1647, loss is 0.08590461313724518\n",
      "epoch: 1 step: 1648, loss is 0.028788458555936813\n",
      "epoch: 1 step: 1649, loss is 0.06599318981170654\n",
      "epoch: 1 step: 1650, loss is 0.04794129729270935\n",
      "epoch: 1 step: 1651, loss is 0.0978853777050972\n",
      "epoch: 1 step: 1652, loss is 0.07184091955423355\n",
      "epoch: 1 step: 1653, loss is 0.1860281378030777\n",
      "epoch: 1 step: 1654, loss is 0.1372258961200714\n",
      "epoch: 1 step: 1655, loss is 0.3129688799381256\n",
      "epoch: 1 step: 1656, loss is 0.09219522774219513\n",
      "epoch: 1 step: 1657, loss is 0.020393159240484238\n",
      "epoch: 1 step: 1658, loss is 0.1585606336593628\n",
      "epoch: 1 step: 1659, loss is 0.1781703233718872\n",
      "epoch: 1 step: 1660, loss is 0.18218345940113068\n",
      "epoch: 1 step: 1661, loss is 0.17812851071357727\n",
      "epoch: 1 step: 1662, loss is 0.2175160050392151\n",
      "epoch: 1 step: 1663, loss is 0.06720112264156342\n",
      "epoch: 1 step: 1664, loss is 0.09531207382678986\n",
      "epoch: 1 step: 1665, loss is 0.016573000699281693\n",
      "epoch: 1 step: 1666, loss is 0.009663430973887444\n",
      "epoch: 1 step: 1667, loss is 0.1655469536781311\n",
      "epoch: 1 step: 1668, loss is 0.01740274205803871\n",
      "epoch: 1 step: 1669, loss is 0.02161462977528572\n",
      "epoch: 1 step: 1670, loss is 0.09590831398963928\n",
      "epoch: 1 step: 1671, loss is 0.18394829332828522\n",
      "epoch: 1 step: 1672, loss is 0.08627729117870331\n",
      "epoch: 1 step: 1673, loss is 0.46129336953163147\n",
      "epoch: 1 step: 1674, loss is 0.08585427701473236\n",
      "epoch: 1 step: 1675, loss is 0.020429542288184166\n",
      "epoch: 1 step: 1676, loss is 0.35523074865341187\n",
      "epoch: 1 step: 1677, loss is 0.20872312784194946\n",
      "epoch: 1 step: 1678, loss is 0.09120812267065048\n",
      "epoch: 1 step: 1679, loss is 0.2434460073709488\n",
      "epoch: 1 step: 1680, loss is 0.11422067880630493\n",
      "epoch: 1 step: 1681, loss is 0.11676181107759476\n",
      "epoch: 1 step: 1682, loss is 0.33305057883262634\n",
      "epoch: 1 step: 1683, loss is 0.09429456293582916\n",
      "epoch: 1 step: 1684, loss is 0.08100461214780807\n",
      "epoch: 1 step: 1685, loss is 0.06904569268226624\n",
      "epoch: 1 step: 1686, loss is 0.09001120179891586\n",
      "epoch: 1 step: 1687, loss is 0.3157263994216919\n",
      "epoch: 1 step: 1688, loss is 0.07189808040857315\n",
      "epoch: 1 step: 1689, loss is 0.0680570900440216\n",
      "epoch: 1 step: 1690, loss is 0.06052451580762863\n",
      "epoch: 1 step: 1691, loss is 0.3540630340576172\n",
      "epoch: 1 step: 1692, loss is 0.03036770224571228\n",
      "epoch: 1 step: 1693, loss is 0.17192518711090088\n",
      "epoch: 1 step: 1694, loss is 0.04099399596452713\n",
      "epoch: 1 step: 1695, loss is 0.04549367353320122\n",
      "epoch: 1 step: 1696, loss is 0.03182161599397659\n",
      "epoch: 1 step: 1697, loss is 0.07176922261714935\n",
      "epoch: 1 step: 1698, loss is 0.1327837109565735\n",
      "epoch: 1 step: 1699, loss is 0.008970396593213081\n",
      "epoch: 1 step: 1700, loss is 0.2681612968444824\n",
      "epoch: 1 step: 1701, loss is 0.25176531076431274\n",
      "epoch: 1 step: 1702, loss is 0.3973075747489929\n",
      "epoch: 1 step: 1703, loss is 0.1583767533302307\n",
      "epoch: 1 step: 1704, loss is 0.10257287323474884\n",
      "epoch: 1 step: 1705, loss is 0.0810219943523407\n",
      "epoch: 1 step: 1706, loss is 0.18050314486026764\n",
      "epoch: 1 step: 1707, loss is 0.09878311306238174\n",
      "epoch: 1 step: 1708, loss is 0.01651463843882084\n",
      "epoch: 1 step: 1709, loss is 0.04692116007208824\n",
      "epoch: 1 step: 1710, loss is 0.04931323230266571\n",
      "epoch: 1 step: 1711, loss is 0.07406909763813019\n",
      "epoch: 1 step: 1712, loss is 0.3007849454879761\n",
      "epoch: 1 step: 1713, loss is 0.21890342235565186\n",
      "epoch: 1 step: 1714, loss is 0.05882629379630089\n",
      "epoch: 1 step: 1715, loss is 0.06690285354852676\n",
      "epoch: 1 step: 1716, loss is 0.16500453650951385\n",
      "epoch: 1 step: 1717, loss is 0.07619623839855194\n",
      "epoch: 1 step: 1718, loss is 0.10038469731807709\n",
      "epoch: 1 step: 1719, loss is 0.09036490321159363\n",
      "epoch: 1 step: 1720, loss is 0.11398409307003021\n",
      "epoch: 1 step: 1721, loss is 0.050829045474529266\n",
      "epoch: 1 step: 1722, loss is 0.09979648888111115\n",
      "epoch: 1 step: 1723, loss is 0.07376915961503983\n",
      "epoch: 1 step: 1724, loss is 0.07220298796892166\n",
      "epoch: 1 step: 1725, loss is 0.03097686544060707\n",
      "epoch: 1 step: 1726, loss is 0.05999936908483505\n",
      "epoch: 1 step: 1727, loss is 0.11900635808706284\n",
      "epoch: 1 step: 1728, loss is 0.06728805601596832\n",
      "epoch: 1 step: 1729, loss is 0.12620064616203308\n",
      "epoch: 1 step: 1730, loss is 0.22904983162879944\n",
      "epoch: 1 step: 1731, loss is 0.005479194223880768\n",
      "epoch: 1 step: 1732, loss is 0.41244083642959595\n",
      "epoch: 1 step: 1733, loss is 0.32153236865997314\n",
      "epoch: 1 step: 1734, loss is 0.13567298650741577\n",
      "epoch: 1 step: 1735, loss is 0.039513811469078064\n",
      "epoch: 1 step: 1736, loss is 0.10645795613527298\n",
      "epoch: 1 step: 1737, loss is 0.03268689289689064\n",
      "epoch: 1 step: 1738, loss is 0.031092580407857895\n",
      "epoch: 1 step: 1739, loss is 0.033360641449689865\n",
      "epoch: 1 step: 1740, loss is 0.09844598919153214\n",
      "epoch: 1 step: 1741, loss is 0.1247587725520134\n",
      "epoch: 1 step: 1742, loss is 0.4865308701992035\n",
      "epoch: 1 step: 1743, loss is 0.06833917647600174\n",
      "epoch: 1 step: 1744, loss is 0.0637948289513588\n",
      "epoch: 1 step: 1745, loss is 0.04777440056204796\n",
      "epoch: 1 step: 1746, loss is 0.24274183809757233\n",
      "epoch: 1 step: 1747, loss is 0.06204106658697128\n",
      "epoch: 1 step: 1748, loss is 0.01792076788842678\n",
      "epoch: 1 step: 1749, loss is 0.0636318102478981\n",
      "epoch: 1 step: 1750, loss is 0.04548315703868866\n",
      "epoch: 1 step: 1751, loss is 0.07484378665685654\n",
      "epoch: 1 step: 1752, loss is 0.03572654724121094\n",
      "epoch: 1 step: 1753, loss is 0.18423913419246674\n",
      "epoch: 1 step: 1754, loss is 0.09886510670185089\n",
      "epoch: 1 step: 1755, loss is 0.3199000656604767\n",
      "epoch: 1 step: 1756, loss is 0.09551350772380829\n",
      "epoch: 1 step: 1757, loss is 0.0970887616276741\n",
      "epoch: 1 step: 1758, loss is 0.0726763904094696\n",
      "epoch: 1 step: 1759, loss is 0.11252821981906891\n",
      "epoch: 1 step: 1760, loss is 0.004445142578333616\n",
      "epoch: 1 step: 1761, loss is 0.1259731948375702\n",
      "epoch: 1 step: 1762, loss is 0.11545894294977188\n",
      "epoch: 1 step: 1763, loss is 0.11500584334135056\n",
      "epoch: 1 step: 1764, loss is 0.10156573355197906\n",
      "epoch: 1 step: 1765, loss is 0.0557650662958622\n",
      "epoch: 1 step: 1766, loss is 0.3764441907405853\n",
      "epoch: 1 step: 1767, loss is 0.0635383203625679\n",
      "epoch: 1 step: 1768, loss is 0.4296278953552246\n",
      "epoch: 1 step: 1769, loss is 0.04363167658448219\n",
      "epoch: 1 step: 1770, loss is 0.011329986155033112\n",
      "epoch: 1 step: 1771, loss is 0.09878753125667572\n",
      "epoch: 1 step: 1772, loss is 0.3172387182712555\n",
      "epoch: 1 step: 1773, loss is 0.040931664407253265\n",
      "epoch: 1 step: 1774, loss is 0.03278566896915436\n",
      "epoch: 1 step: 1775, loss is 0.12933577597141266\n",
      "epoch: 1 step: 1776, loss is 0.3352193832397461\n",
      "epoch: 1 step: 1777, loss is 0.21068839728832245\n",
      "epoch: 1 step: 1778, loss is 0.22504228353500366\n",
      "epoch: 1 step: 1779, loss is 0.17557179927825928\n",
      "epoch: 1 step: 1780, loss is 0.10299723595380783\n",
      "epoch: 1 step: 1781, loss is 0.02086538076400757\n",
      "epoch: 1 step: 1782, loss is 0.15917930006980896\n",
      "epoch: 1 step: 1783, loss is 0.07947876304388046\n",
      "epoch: 1 step: 1784, loss is 0.12762968242168427\n",
      "epoch: 1 step: 1785, loss is 0.33919093012809753\n",
      "epoch: 1 step: 1786, loss is 0.007154862862080336\n",
      "epoch: 1 step: 1787, loss is 0.033984821289777756\n",
      "epoch: 1 step: 1788, loss is 0.034149397164583206\n",
      "epoch: 1 step: 1789, loss is 0.18352317810058594\n",
      "epoch: 1 step: 1790, loss is 0.021121634170413017\n",
      "epoch: 1 step: 1791, loss is 0.22327284514904022\n",
      "epoch: 1 step: 1792, loss is 0.11315398663282394\n",
      "epoch: 1 step: 1793, loss is 0.13500365614891052\n",
      "epoch: 1 step: 1794, loss is 0.008999809622764587\n",
      "epoch: 1 step: 1795, loss is 0.03723844140768051\n",
      "epoch: 1 step: 1796, loss is 0.125840425491333\n",
      "epoch: 1 step: 1797, loss is 0.1252439171075821\n",
      "epoch: 1 step: 1798, loss is 0.24687926471233368\n",
      "epoch: 1 step: 1799, loss is 0.07397777587175369\n",
      "epoch: 1 step: 1800, loss is 0.18316039443016052\n",
      "epoch: 1 step: 1801, loss is 0.047485146671533585\n",
      "epoch: 1 step: 1802, loss is 0.07409901916980743\n",
      "epoch: 1 step: 1803, loss is 0.05758574604988098\n",
      "epoch: 1 step: 1804, loss is 0.2734123468399048\n",
      "epoch: 1 step: 1805, loss is 0.1366654336452484\n",
      "epoch: 1 step: 1806, loss is 0.07846014946699142\n",
      "epoch: 1 step: 1807, loss is 0.03952880576252937\n",
      "epoch: 1 step: 1808, loss is 0.004067461006343365\n",
      "epoch: 1 step: 1809, loss is 0.008112959563732147\n",
      "epoch: 1 step: 1810, loss is 0.010893338359892368\n",
      "epoch: 1 step: 1811, loss is 0.016620151698589325\n",
      "epoch: 1 step: 1812, loss is 0.08897808194160461\n",
      "epoch: 1 step: 1813, loss is 0.1396893858909607\n",
      "epoch: 1 step: 1814, loss is 0.04961913451552391\n",
      "epoch: 1 step: 1815, loss is 0.12394864857196808\n",
      "epoch: 1 step: 1816, loss is 0.07067340612411499\n",
      "epoch: 1 step: 1817, loss is 0.20493069291114807\n",
      "epoch: 1 step: 1818, loss is 0.07059389352798462\n",
      "epoch: 1 step: 1819, loss is 0.041386719793081284\n",
      "epoch: 1 step: 1820, loss is 0.061855655163526535\n",
      "epoch: 1 step: 1821, loss is 0.01022188551723957\n",
      "epoch: 1 step: 1822, loss is 0.042788878083229065\n",
      "epoch: 1 step: 1823, loss is 0.11231585592031479\n",
      "epoch: 1 step: 1824, loss is 0.1975102722644806\n",
      "epoch: 1 step: 1825, loss is 0.15429753065109253\n",
      "epoch: 1 step: 1826, loss is 0.18214932084083557\n",
      "epoch: 1 step: 1827, loss is 0.08315859735012054\n",
      "epoch: 1 step: 1828, loss is 0.13251739740371704\n",
      "epoch: 1 step: 1829, loss is 0.04701704531908035\n",
      "epoch: 1 step: 1830, loss is 0.10909175127744675\n",
      "epoch: 1 step: 1831, loss is 0.2613610029220581\n",
      "epoch: 1 step: 1832, loss is 0.16838374733924866\n",
      "epoch: 1 step: 1833, loss is 0.1397639513015747\n",
      "epoch: 1 step: 1834, loss is 0.11364123225212097\n",
      "epoch: 1 step: 1835, loss is 0.07456450909376144\n",
      "epoch: 1 step: 1836, loss is 0.13928350806236267\n",
      "epoch: 1 step: 1837, loss is 0.10934806615114212\n",
      "epoch: 1 step: 1838, loss is 0.024537092074751854\n",
      "epoch: 1 step: 1839, loss is 0.0963127389550209\n",
      "epoch: 1 step: 1840, loss is 0.09125587344169617\n",
      "epoch: 1 step: 1841, loss is 0.032411523163318634\n",
      "epoch: 1 step: 1842, loss is 0.08716504275798798\n",
      "epoch: 1 step: 1843, loss is 0.07813091576099396\n",
      "epoch: 1 step: 1844, loss is 0.10115844756364822\n",
      "epoch: 1 step: 1845, loss is 0.16462185978889465\n",
      "epoch: 1 step: 1846, loss is 0.01877753995358944\n",
      "epoch: 1 step: 1847, loss is 0.04290672391653061\n",
      "epoch: 1 step: 1848, loss is 0.08193713426589966\n",
      "epoch: 1 step: 1849, loss is 0.046996526420116425\n",
      "epoch: 1 step: 1850, loss is 0.2307499796152115\n",
      "epoch: 1 step: 1851, loss is 0.014802583493292332\n",
      "epoch: 1 step: 1852, loss is 0.003703695721924305\n",
      "epoch: 1 step: 1853, loss is 0.1942603886127472\n",
      "epoch: 1 step: 1854, loss is 0.08315175771713257\n",
      "epoch: 1 step: 1855, loss is 0.02604842185974121\n",
      "epoch: 1 step: 1856, loss is 0.020697038620710373\n",
      "epoch: 1 step: 1857, loss is 0.036778587847948074\n",
      "epoch: 1 step: 1858, loss is 0.23128707706928253\n",
      "epoch: 1 step: 1859, loss is 0.04226788878440857\n",
      "epoch: 1 step: 1860, loss is 0.22854195535182953\n",
      "epoch: 1 step: 1861, loss is 0.1199796199798584\n",
      "epoch: 1 step: 1862, loss is 0.2765915095806122\n",
      "epoch: 1 step: 1863, loss is 0.007575636729598045\n",
      "epoch: 1 step: 1864, loss is 0.07654624432325363\n",
      "epoch: 1 step: 1865, loss is 0.37817251682281494\n",
      "epoch: 1 step: 1866, loss is 0.39557576179504395\n",
      "epoch: 1 step: 1867, loss is 0.07937737554311752\n",
      "epoch: 1 step: 1868, loss is 0.15896637737751007\n",
      "epoch: 1 step: 1869, loss is 0.0643785372376442\n",
      "epoch: 1 step: 1870, loss is 0.04123429208993912\n",
      "epoch: 1 step: 1871, loss is 0.05114792659878731\n",
      "epoch: 1 step: 1872, loss is 0.014550329186022282\n",
      "epoch: 1 step: 1873, loss is 0.04185980185866356\n",
      "epoch: 1 step: 1874, loss is 0.17949020862579346\n",
      "epoch: 1 step: 1875, loss is 0.0735073834657669\n",
      "============== Starting Testing ==============\n",
      "============== Accuracy:{'Accuracy': 0.9678485576923077} ==============\n"
     ]
    }
   ],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target='Ascend')\n",
    "dataset_sink_mode = False\n",
    "# download mnist dataset\n",
    "download_dataset()\n",
    "# learning rate setting\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "dataset_size = 1\n",
    "mnist_path = \"./MNIST_Data\"\n",
    "# define the loss function\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "train_epoch = 1\n",
    "# create the network\n",
    "net = LeNet5()\n",
    "# define the optimizer\n",
    "net_opt = nn.Momentum(net.trainable_params(), lr, momentum)\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "# save the network model and parameters for subsequence fine-tuning\n",
    "ckpoint = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)\n",
    "# group layers into an object with training and evaluation features\n",
    "model = Model(net, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "\n",
    "train_net(model, train_epoch, mnist_path, dataset_size, ckpoint, dataset_sink_mode)\n",
    "test_net(net, model, mnist_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
