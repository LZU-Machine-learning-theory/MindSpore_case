{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EucQZ28SltO"
   },
   "source": [
    "# 使用Unet做图像分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AMjRJwiTIGQ"
   },
   "source": [
    "导入模块\n",
    "导入minspore模块和辅助模块，设置Mindspore上下文，如执行模式、设备等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h_FMyXZlQ6-M"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops.operations as F\n",
    "from mindspore import Model, context\n",
    "from mindspore.nn.loss.loss import LossBase\n",
    "from mindspore.communication.management import init, get_group_size\n",
    "from mindspore.train.callback import CheckpointConfig, ModelCheckpoint\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "\n",
    "from src.unet import UNet\n",
    "from src.data_loader import create_dataset\n",
    "from src.loss import CrossEntropyWithLogits\n",
    "from src.utils import StepLossTimeMonitor\n",
    "from src.config import cfg_unet\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OdOJuPrzRH2_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "device_id = int(0)\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\", save_graphs=False)\n",
    "\n",
    "mindspore.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JndRH1mKTa48"
   },
   "source": [
    "定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NzTAuiMpRKm4"
   },
   "outputs": [],
   "source": [
    "class CrossEntropyWithLogits(LossBase):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyWithLogits, self).__init__()\n",
    "        self.transpose_fn = F.Transpose()\n",
    "        self.reshape_fn = F.Reshape()\n",
    "        self.softmax_cross_entropy_loss = nn.SoftmaxCrossEntropyWithLogits()\n",
    "        self.cast = F.Cast()\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        # NCHW->NHWC\n",
    "        logits = self.transpose_fn(logits, (0, 2, 3, 1))\n",
    "        logits = self.cast(logits, mindspore.float32)\n",
    "        label = self.transpose_fn(label, (0, 2, 3, 1))\n",
    "\n",
    "        loss = self.reduce_mean(self.softmax_cross_entropy_loss(self.reshape_fn(logits, (-1, 2)),\n",
    "                                                                self.reshape_fn(label, (-1, 2))))\n",
    "        return self.get_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ytT3-BdTmqZ"
   },
   "source": [
    "定义验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ykZuYZB4RNho"
   },
   "outputs": [],
   "source": [
    "class dice_coeff(nn.Metric):\n",
    "    def __init__(self):\n",
    "        super(dice_coeff, self).__init__()\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self._dice_coeff_sum = 0\n",
    "        self._samples_num = 0\n",
    "\n",
    "    def update(self, *inputs):\n",
    "        if len(inputs) != 2:\n",
    "            raise ValueError('Mean dice coeffcient need 2 inputs (y_pred, y), but got {}'.format(len(inputs)))\n",
    "\n",
    "        y_pred = self._convert_data(inputs[0])\n",
    "        y = self._convert_data(inputs[1])\n",
    "        self._samples_num += y.shape[0]\n",
    "        y_pred = y_pred.transpose(0, 2, 3, 1)\n",
    "        y = y.transpose(0, 2, 3, 1)\n",
    "        y_pred = softmax(y_pred, axis=3)\n",
    "\n",
    "        inter = np.dot(y_pred.flatten(), y.flatten())\n",
    "        union = np.dot(y_pred.flatten(), y_pred.flatten()) + np.dot(y.flatten(), y.flatten())\n",
    "\n",
    "        single_dice_coeff = 2 * float(inter) / float(union + 1e-6)\n",
    "        print(\"single dice coeff is:\", single_dice_coeff)\n",
    "        self._dice_coeff_sum += single_dice_coeff\n",
    "\n",
    "    def eval(self):\n",
    "        if self._samples_num == 0:\n",
    "            raise RuntimeError('Total samples num must not be 0.')\n",
    "        return self._dice_coeff_sum / float(self._samples_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTO5Vu__Tpmg"
   },
   "source": [
    "定义训练过程\n",
    "\n",
    "参数传入训练数据集和训练参数，构建网络，损失函数，优化器等，并配置好checkpoint生成信息，然后使用model.train接口，进行模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fysEtqLURSLY"
   },
   "outputs": [],
   "source": [
    "class dice_coeff(nn.Metric):\n",
    "    def __init__(self):\n",
    "        super(dice_coeff, self).__init__()\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self._dice_coeff_sum = 0\n",
    "        self._samples_num = 0\n",
    "\n",
    "    def update(self, *inputs):\n",
    "        if len(inputs) != 2:\n",
    "            raise ValueError('Mean dice coeffcient need 2 inputs (y_pred, y), but got {}'.format(len(inputs)))\n",
    "\n",
    "        y_pred = self._convert_data(inputs[0])\n",
    "        y = self._convert_data(inputs[1])\n",
    "        self._samples_num += y.shape[0]\n",
    "        y_pred = y_pred.transpose(0, 2, 3, 1)\n",
    "        y = y.transpose(0, 2, 3, 1)\n",
    "        y_pred = softmax(y_pred, axis=3)\n",
    "\n",
    "        inter = np.dot(y_pred.flatten(), y.flatten())\n",
    "        union = np.dot(y_pred.flatten(), y_pred.flatten()) + np.dot(y.flatten(), y.flatten())\n",
    "\n",
    "        single_dice_coeff = 2 * float(inter) / float(union + 1e-6)\n",
    "        print(\"single dice coeff is:\", single_dice_coeff)\n",
    "        self._dice_coeff_sum += single_dice_coeff\n",
    "\n",
    "    def eval(self):\n",
    "        if self._samples_num == 0:\n",
    "            raise RuntimeError('Total samples num must not be 0.')\n",
    "        return self._dice_coeff_sum / float(self._samples_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDJKMa76T5Bg"
   },
   "source": [
    "定义模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l1TUr-KPRWnv"
   },
   "outputs": [],
   "source": [
    "def train_net(data_dir, cross_valid_ind=1, epochs=400, batch_size=16, lr=0.0001, run_distribute=False, cfg=None):\n",
    "    if run_distribute:\n",
    "        init()\n",
    "        group_size = get_group_size()\n",
    "        parallel_mode = ParallelMode.DATA_PARALLEL\n",
    "        context.set_auto_parallel_context(parallel_mode=parallel_mode,\n",
    "                                          device_num=group_size,\n",
    "                                          gradients_mean=False)\n",
    "    net = UNet(n_channels=cfg['num_channels'], n_classes=cfg['num_classes'])\n",
    "\n",
    "    if cfg['resume']:\n",
    "        param_dict = load_checkpoint(cfg['resume_ckpt'])\n",
    "        load_param_into_net(net, param_dict)\n",
    "\n",
    "    criterion = CrossEntropyWithLogits()\n",
    "    train_dataset, _ = create_dataset(data_dir, epochs, batch_size, True, cross_valid_ind, run_distribute)\n",
    "    train_data_size = train_dataset.get_dataset_size()\n",
    "    print(\"dataset length is:\", train_data_size)\n",
    "    ckpt_config = CheckpointConfig(save_checkpoint_steps=train_data_size,\n",
    "                                   keep_checkpoint_max=cfg['keep_checkpoint_max'])\n",
    "    ckpoint_cb = ModelCheckpoint(prefix='ckpt_unet_medical_adam',\n",
    "                                 directory='./ckpt_{}/'.format(device_id),\n",
    "                                 config=ckpt_config)\n",
    "\n",
    "    optimizer = nn.Adam(params=net.trainable_params(), learning_rate=lr, weight_decay=cfg['weight_decay'],\n",
    "                        loss_scale=cfg['loss_scale'])\n",
    "\n",
    "    loss_scale_manager = mindspore.train.loss_scale_manager.FixedLossScaleManager(cfg['FixedLossScaleManager'], False)\n",
    "\n",
    "    model = Model(net, loss_fn=criterion, loss_scale_manager=loss_scale_manager, optimizer=optimizer, amp_level=\"O3\")\n",
    "\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    model.train(1, train_dataset, callbacks=[StepLossTimeMonitor(batch_size=batch_size), ckpoint_cb],\n",
    "                dataset_sink_mode=False)\n",
    "    print(\"============== End Training ==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lND2kXOOT7fr"
   },
   "source": [
    "定义训练验证\n",
    "\n",
    "定义数据集路径以及保存的ckpt文件路径以用于模型的训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fuip398lRY7S"
   },
   "outputs": [],
   "source": [
    "def test_net(data_dir, ckpt_path, cross_valid_ind=1, cfg=None):\n",
    "    net = UNet(n_channels=cfg['num_channels'], n_classes=cfg['num_classes'])\n",
    "    param_dict = load_checkpoint(ckpt_path)\n",
    "    load_param_into_net(net, param_dict)\n",
    "\n",
    "    criterion = CrossEntropyWithLogits()\n",
    "    _, valid_dataset = create_dataset(data_dir, 1, 1, False, cross_valid_ind, False)\n",
    "    model = Model(net, loss_fn=criterion, metrics={\"dice_coeff\": dice_coeff()})\n",
    "\n",
    "    print(\"============== Starting Evaluating ============\")\n",
    "    dice_score = model.eval(valid_dataset, dataset_sink_mode=False)\n",
    "    print(\"Cross valid dice coeff is:\", dice_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KeWgDU3jRbvd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length is: 600\n",
      "============== Starting Training ==============\n",
      "============== End Training ==============\n"
     ]
    }
   ],
   "source": [
    "data_url = './data'\n",
    "run_distribute = False\n",
    "epoch_size = cfg_unet['epochs'] if not run_distribute else cfg_unet['distribute_epochs']\n",
    "\n",
    "train_net(data_dir=data_url,\n",
    "          cross_valid_ind=cfg_unet['cross_valid_ind'],\n",
    "          epochs=epoch_size,\n",
    "          batch_size=cfg_unet['batchsize'],\n",
    "          lr=cfg_unet['lr'],\n",
    "          run_distribute=run_distribute,\n",
    "          cfg=cfg_unet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "============== Starting Evaluating ============\n",
      "single dice coeff is: 0.9020679659064226\n",
      "single dice coeff is: 0.9024182041821139\n",
      "single dice coeff is: 0.9243325460010725\n",
      "single dice coeff is: 0.9230363692917839\n",
      "single dice coeff is: 0.9130276201203033\n",
      "single dice coeff is: 0.9020846599599416\n",
      "Cross valid dice coeff is: {'dice_coeff': 0.9111612275769395}\n"
     ]
    }
   ],
   "source": [
    "print('*' * 60)\n",
    "ckpt_path = './ckpt_0/ckpt_unet_medical_adam-1_600.ckpt'\n",
    "test_net(data_dir=data_url,\n",
    "          ckpt_path=ckpt_path,\n",
    "          cross_valid_ind=cfg_unet['cross_valid_ind'],\n",
    "            cfg=cfg_unet)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCk4n1G3AaHXte3RbVpkNM",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
