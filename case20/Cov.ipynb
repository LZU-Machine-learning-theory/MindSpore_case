{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积层简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积层（Convolution Layer）：由若干个卷积核f（filter）和偏移值b组成，（这里的卷积核相当于权值矩阵），卷积核与输入图片进行点积和累加可以得到一张feature map。\n",
    "\n",
    "卷积层的特征：\n",
    "\n",
    "1. 网络局部连接：卷积核每一次仅作用于图片的局部\n",
    "\n",
    "2. 卷积核权值共享：一个卷积层可以有多个不同的卷积核，每一个filter在与输入矩阵进行点积操作的过程中，其权值是固定不变的。\n",
    "一个卷积层可以有若干个卷积核，卷积核的通道数=输入图片的通道数，每一个卷积核的通道与图片的对应通道进行点积+累加的操作，可以得到1个featrue map，假设有3个通道，那么可以得到3个featrue map，然后把这3个feature map对应的位置相加，即可得到1张featrue map，得到的这一张就是该卷积核与图片进行卷积操作的feature map。一般图片有3个通道，red、green、bule，然后卷积核的通道数为3，分别对应r、g、b，然后这对应的三个通道分别点积+累加，得到3个feature map，最后再把这3张feature ma相加，然后再加上偏移值b，就可以得到1张feature map。\n",
    "\n",
    "卷积层的运行：\n",
    "\n",
    "1. 输入：输入是一张图片，图片：宽w，高h，通道数c，由c个w列h行的矩阵构成。输入矩阵的通道数=卷积核的通道数\n",
    "\n",
    "2. 计算：根据padding（填充）、stride（步长），从图片的左上角开始，假设有3个通道，卷积核的对应通道与输入图片的对应通道进行点积和累加的操作，得到1张feature map，然后把3张feature map的对应位置和偏移矩阵b的对应位置相加，即可得到该卷积核对应的feature map。有多少个卷积核，就有多少个featrue map。n个卷积核经过卷积计算，得到n张featrue map。即，1张feature map中的每一个元素，是由对应的filter的不同维度的矩阵，作用于相应维度输入矩阵的不同位置，进行点积运算和累加，再加上偏移量bias的结果。\n",
    "\n",
    "3. 输出：有n个卷积核，输出n张feature map，也就是n个矩阵\n",
    "\n",
    "卷积层输出矩阵的维度：\n",
    "\n",
    "1. 通道数 = 卷积核的个数 ：k个卷积核有k个feature map，输出矩阵的通道数 / 深度为k\n",
    "\n",
    "2. 高度 / 宽度= （图片的高度or宽度  + 2*padding - 卷积核的高度or宽度）/ 步长 +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mindspore.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, pad_mode=\"same\", padding=0, \n",
    "                          dilation=1, group=1, has_bias=False, weight_init=\"normal\", bias_init=\"zeros\", data_format=\"NCHW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.5083481   0.13354278  0.13354278 ...  0.13354278  0.16333646\n",
      "    -0.17533791]\n",
      "   [ 0.6736675   0.31088686  0.31088686 ...  0.31088686  0.16745621\n",
      "    -0.2734775 ]\n",
      "   [ 0.6736675   0.31088686  0.31088686 ...  0.31088686  0.16745621\n",
      "    -0.2734775 ]\n",
      "   ...\n",
      "   [ 0.6736675   0.31088686  0.31088686 ...  0.31088686  0.16745621\n",
      "    -0.2734775 ]\n",
      "   [ 0.4352988   0.24126685  0.24126685 ...  0.24126685  0.19756407\n",
      "    -0.14160627]\n",
      "   [ 0.36915213  0.2480585   0.2480585  ...  0.2480585   0.07947969\n",
      "    -0.19668007]]\n",
      "\n",
      "  [[ 0.29348177  0.5667593   0.5667593  ...  0.5667593   0.3123672\n",
      "     0.09440076]\n",
      "   [ 0.2904955   0.44997394  0.44997394 ...  0.44997394  0.16649705\n",
      "     0.07670933]\n",
      "   [ 0.2904955   0.44997394  0.44997394 ...  0.44997394  0.16649705\n",
      "     0.07670933]\n",
      "   ...\n",
      "   [ 0.2904955   0.44997394  0.44997394 ...  0.44997394  0.16649705\n",
      "     0.07670933]\n",
      "   [ 0.19929397  0.4212929   0.4212929  ...  0.4212929   0.25238073\n",
      "     0.25973654]\n",
      "   [ 0.23381633  0.13862675  0.13862675 ...  0.13862675 -0.08595091\n",
      "     0.12495691]]\n",
      "\n",
      "  [[ 0.06694615 -0.11791277 -0.11791277 ... -0.11791277 -0.17207861\n",
      "    -0.21540844]\n",
      "   [ 0.19233483 -0.03770357 -0.03770357 ... -0.03770357 -0.11594969\n",
      "    -0.08419418]\n",
      "   [ 0.19233483 -0.03770357 -0.03770357 ... -0.03770357 -0.11594969\n",
      "    -0.08419418]\n",
      "   ...\n",
      "   [ 0.19233483 -0.03770357 -0.03770357 ... -0.03770357 -0.11594969\n",
      "    -0.08419418]\n",
      "   [-0.0296815  -0.16531575 -0.16531575 ... -0.16531575 -0.20246267\n",
      "    -0.04381686]\n",
      "   [ 0.20774895  0.01053107  0.01053107 ...  0.01053107  0.0261327\n",
      "     0.14855462]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.27050614  0.44493157  0.44493157 ...  0.44493157  0.32221675\n",
      "     0.15262008]\n",
      "   [ 0.30439514  0.27823663  0.27823663 ...  0.27823663  0.01269644\n",
      "    -0.30256188]\n",
      "   [ 0.30439514  0.27823663  0.27823663 ...  0.27823663  0.01269644\n",
      "    -0.30256188]\n",
      "   ...\n",
      "   [ 0.30439514  0.27823663  0.27823663 ...  0.27823663  0.01269644\n",
      "    -0.30256188]\n",
      "   [ 0.19697297  0.08174288  0.08174288 ...  0.08174288 -0.12645811\n",
      "    -0.48493552]\n",
      "   [ 0.13827193 -0.13343287 -0.13343287 ... -0.13343287 -0.3074755\n",
      "    -0.60174847]]\n",
      "\n",
      "  [[-0.00481725 -0.09446758 -0.09446758 ... -0.09446758 -0.20226455\n",
      "    -0.3194424 ]\n",
      "   [-0.33231306 -0.4664542  -0.4664542  ... -0.4664542  -0.38870537\n",
      "    -0.46921086]\n",
      "   [-0.33231306 -0.4664542  -0.4664542  ... -0.4664542  -0.38870537\n",
      "    -0.46921086]\n",
      "   ...\n",
      "   [-0.33231306 -0.4664542  -0.4664542  ... -0.4664542  -0.38870537\n",
      "    -0.46921086]\n",
      "   [-0.43564767 -0.4908136  -0.4908136  ... -0.4908136  -0.3999458\n",
      "    -0.49423105]\n",
      "   [-0.39660758 -0.3141907  -0.3141907  ... -0.3141907  -0.23353225\n",
      "    -0.31563866]]\n",
      "\n",
      "  [[-0.45440167 -0.32903397 -0.32903397 ... -0.32903397 -0.48194653\n",
      "    -0.05939412]\n",
      "   [-0.18930674  0.11755341  0.11755341 ...  0.11755341 -0.10261565\n",
      "     0.21164119]\n",
      "   [-0.18930674  0.11755341  0.11755341 ...  0.11755341 -0.10261565\n",
      "     0.21164119]\n",
      "   ...\n",
      "   [-0.18930674  0.11755341  0.11755341 ...  0.11755341 -0.10261565\n",
      "     0.21164119]\n",
      "   [-0.20165426 -0.08763951 -0.08763951 ... -0.08763951 -0.25650305\n",
      "     0.05736619]\n",
      "   [-0.08733362  0.00673169  0.00673169 ...  0.00673169 -0.16539347\n",
      "     0.11305451]]]]\n",
      "(1, 240, 1024, 640)\n"
     ]
    }
   ],
   "source": [
    "from mindspore.common.tensor import Tensor\n",
    "import mindspore.nn as nn\n",
    "import numpy as np\n",
    "import mindspore.common.dtype as mstype\n",
    "\n",
    "net = nn.Conv2d(120, 240, 4, has_bias=False, weight_init='normal')\n",
    "x = Tensor(np.ones([1, 120, 1024, 640]), mstype.float32)\n",
    "output = net(x)\n",
    "print(output)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用参数说明\n",
    "``` python\n",
    "in_channels (int) - Conv2d层输入Tensor的空间维度。\n",
    "\n",
    "out_channels (int) - Conv2d层输出Tensor的空间维度。\n",
    "\n",
    "kernel_size (Union[int, tuple[int]]) - 指定二维卷积核的高度和宽度。数据类型为整型或两个整型的tuple。一个整数表示卷积核的高度和宽度均为该值。两个整数的tuple分别表示卷积核的高度和宽度。\n",
    "\n",
    "stride (Union[int, tuple[int]]) - 二维卷积核的移动步长。数据类型为整型或两个整型的tuple。一个整数表示在高度和宽度方向的移动步长均为该值。两个整数的tuple分别表示在高度和宽度方向的移动步长。默认值：1。\n",
    "\n",
    "pad_mode (str) - 指定填充模式。可选值为”same”、”valid”、”pad”。默认值：”same”。\n",
    "\n",
    "padding (Union[int, tuple[int]]) - 输入的高度和宽度方向上填充的数量。数据类型为int或包含4个整数的tuple。如果 padding 是一个整数，那么上、下、左、右的填充都等于 padding 。如果 padding 是一个有4个整数的tuple，那么上、下、左、右的填充分别等于 padding[0] 、 padding[1] 、 padding[2] 和 padding[3] 。值应该要大于等于0，默认值：0。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自行实现Mindspore Dense API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入所需包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mindspore.nn as nn\n",
    "\n",
    "import mindspore.common.dtype as mstype\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import log as logger\n",
    "from mindspore import context\n",
    "from mindspore.ops import operations as P\n",
    "\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore.common.tensor import Tensor\n",
    "from mindspore._checkparam import Validator, twice\n",
    "from mindspore._extends import cell_attr_register\n",
    "from mindspore.nn.cell import Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Mindspore官方定义的基类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Conv(Cell):\n",
    "    \"\"\"\n",
    "    Applies a N-D convolution over an input signal composed of several input planes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride,\n",
    "                 pad_mode,\n",
    "                 padding,\n",
    "                 dilation,\n",
    "                 group,\n",
    "                 has_bias,\n",
    "                 weight_init,\n",
    "                 bias_init,\n",
    "                 data_format='NCHW',\n",
    "                 transposed=False):\n",
    "        \"\"\"Initialize _Conv.\"\"\"\n",
    "        super(_Conv, self).__init__()\n",
    "        self.in_channels = Validator.check_positive_int(in_channels, 'in_channels', self.cls_name)\n",
    "        self.out_channels = Validator.check_positive_int(out_channels, 'out_channels', self.cls_name)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.pad_mode = pad_mode\n",
    "        self.weight_init = weight_init\n",
    "        self.bias_init = bias_init\n",
    "        self.data_format = Validator.check_string(data_format, ['NCHW', 'NHWC', 'NCDHW'], 'format', self.cls_name)\n",
    "        if context.get_context(\"device_target\") != \"GPU\" and self.data_format == \"NHWC\":\n",
    "            raise ValueError(f\"For '{self.cls_name}', the \\\"NHWC\\\" format only support in GPU target, \"\n",
    "                             f\"but got the 'format' is {self.data_format} and \"\n",
    "                             f\"the platform is {context.get_context('device_target')}.\")\n",
    "        if isinstance(padding, int):\n",
    "            Validator.check_non_negative_int(padding, 'padding', self.cls_name)\n",
    "            self.padding = padding\n",
    "        elif isinstance(padding, tuple):\n",
    "            for pad in padding:\n",
    "                Validator.check_non_negative_int(pad, 'padding item', self.cls_name)\n",
    "            self.padding = padding\n",
    "        else:\n",
    "            raise TypeError(f\"For '{self.cls_name}', the type of 'padding' must be int or tuple(int), \"\n",
    "                            f\"but got {type(padding).__name__}.\")\n",
    "\n",
    "        self.dilation = dilation\n",
    "        self.group = Validator.check_positive_int(group)\n",
    "        self.has_bias = has_bias\n",
    "        for kernel_size_elem in kernel_size:\n",
    "            Validator.check_positive_int(kernel_size_elem, 'kernel_size item', self.cls_name)\n",
    "        for stride_elem in stride:\n",
    "            Validator.check_positive_int(stride_elem, 'stride item', self.cls_name)\n",
    "        for dilation_elem in dilation:\n",
    "            Validator.check_positive_int(dilation_elem, 'dilation item', self.cls_name)\n",
    "        if in_channels % group != 0:\n",
    "            raise ValueError(f\"For '{self.cls_name}', the attr 'in_channels' must be divisible by attr 'group', \"\n",
    "                             f\"but got 'in_channels': {in_channels} and 'group': {group}.\")\n",
    "        if out_channels % group != 0:\n",
    "            raise ValueError(f\"For '{self.cls_name}', the 'out_channels' must be divisible by attr 'group', \"\n",
    "                             f\"but got 'out_channels': {out_channels} and 'group': {group}.\")\n",
    "        if transposed:\n",
    "            shape = [in_channels, out_channels // group, *kernel_size]\n",
    "        else:\n",
    "            shape = [out_channels, *kernel_size, in_channels // group] \\\n",
    "                if self.data_format == \"NHWC\" else [out_channels, in_channels // group, *kernel_size]\n",
    "        self.weight = Parameter(initializer(self.weight_init, shape), name='weight')\n",
    "\n",
    "        if Validator.check_bool(has_bias, \"has_bias\", self.cls_name):\n",
    "            self.bias = Parameter(initializer(self.bias_init, [out_channels]), name='bias')\n",
    "        else:\n",
    "            if self.bias_init != 'zeros':\n",
    "                logger.warning(\"Value of 'has_bias' is False, value of 'bias_init' will be ignored.\")\n",
    "            self.bias = None\n",
    "\n",
    "    def construct(self, *inputs):\n",
    "        \"\"\"Must be overridden by all subclasses.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def extend_repr(self):\n",
    "        s = 'input_channels={}, output_channels={}, kernel_size={}, ' \\\n",
    "            'stride={}, pad_mode={}, padding={}, dilation={}, ' \\\n",
    "            'group={}, has_bias={}, ' \\\n",
    "            'weight_init={}, bias_init={}, format={}'.format(\n",
    "                self.in_channels,\n",
    "                self.out_channels,\n",
    "                self.kernel_size,\n",
    "                self.stride,\n",
    "                self.pad_mode,\n",
    "                self.padding,\n",
    "                self.dilation,\n",
    "                self.group,\n",
    "                self.has_bias,\n",
    "                self.weight_init,\n",
    "                self.bias_init,\n",
    "                self.data_format)\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现二维卷积运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(_Conv):\n",
    "    r\"\"\"\n",
    "    Examples:\n",
    "        >>> net = nn.Conv2d(120, 240, 4, has_bias=False, weight_init='normal')\n",
    "        >>> x = Tensor(np.ones([1, 120, 1024, 640]), mindspore.float32)\n",
    "        >>> output = net(x).shape\n",
    "        >>> print(output)\n",
    "        (1, 240, 1024, 640)\n",
    "    \"\"\"\n",
    "    @cell_attr_register\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 pad_mode='same',\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 group=1,\n",
    "                 has_bias=False,\n",
    "                 weight_init='normal',\n",
    "                 bias_init='zeros',\n",
    "                 data_format='NCHW'):\n",
    "        \"\"\"Initialize Conv2d.\"\"\"\n",
    "        kernel_size = twice(kernel_size)\n",
    "        stride = twice(stride)\n",
    "        self._dilation = dilation\n",
    "        dilation = twice(dilation)\n",
    "        super(Conv2d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            pad_mode,\n",
    "            padding,\n",
    "            dilation,\n",
    "            group,\n",
    "            has_bias,\n",
    "            weight_init,\n",
    "            bias_init,\n",
    "            data_format)\n",
    "        self.conv2d = P.Conv2D(out_channel=self.out_channels,\n",
    "                               kernel_size=self.kernel_size,\n",
    "                               mode=1,\n",
    "                               pad_mode=self.pad_mode,\n",
    "                               pad=self.padding,\n",
    "                               stride=self.stride,\n",
    "                               dilation=self.dilation,\n",
    "                               group=self.group,\n",
    "                               data_format=self.data_format)\n",
    "        self.bias_add = P.BiasAdd(data_format=self.data_format)\n",
    "\n",
    "    def construct(self, x):\n",
    "        output = self.conv2d(x, self.weight)\n",
    "        if self.has_bias:\n",
    "            output = self.bias_add(output, self.bias)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.03865677 -0.09289473 -0.09289473 ... -0.09289473 -0.18384618\n",
      "     0.14986116]\n",
      "   [-0.00735021 -0.00567359 -0.00567359 ... -0.00567359  0.05383307\n",
      "     0.41082746]\n",
      "   [-0.00735021 -0.00567359 -0.00567359 ... -0.00567359  0.05383307\n",
      "     0.41082746]\n",
      "   ...\n",
      "   [-0.00735021 -0.00567359 -0.00567359 ... -0.00567359  0.05383307\n",
      "     0.41082746]\n",
      "   [-0.05453378 -0.23981446 -0.23981446 ... -0.23981446 -0.07942796\n",
      "     0.15003413]\n",
      "   [-0.05329549 -0.10907745 -0.10907745 ... -0.10907745  0.05168867\n",
      "     0.1408208 ]]\n",
      "\n",
      "  [[-0.3786412  -0.44788218 -0.44788218 ... -0.44788218 -0.10487878\n",
      "    -0.14319408]\n",
      "   [-0.12488252 -0.48393095 -0.48393095 ... -0.48393095 -0.23420429\n",
      "    -0.4250518 ]\n",
      "   [-0.12488252 -0.48393095 -0.48393095 ... -0.48393095 -0.23420429\n",
      "    -0.4250518 ]\n",
      "   ...\n",
      "   [-0.12488252 -0.48393095 -0.48393095 ... -0.48393095 -0.23420429\n",
      "    -0.4250518 ]\n",
      "   [-0.03737056 -0.24817032 -0.24817032 ... -0.24817032 -0.08125114\n",
      "    -0.23907983]\n",
      "   [ 0.20399457 -0.05415815 -0.05415815 ... -0.05415815 -0.07184362\n",
      "    -0.2737211 ]]\n",
      "\n",
      "  [[ 0.06731641 -0.04527748 -0.04527748 ... -0.04527748 -0.08535552\n",
      "    -0.27900934]\n",
      "   [ 0.12792832  0.00287461  0.00287461 ...  0.00287461 -0.0746001\n",
      "    -0.25235778]\n",
      "   [ 0.12792832  0.00287461  0.00287461 ...  0.00287461 -0.0746001\n",
      "    -0.25235778]\n",
      "   ...\n",
      "   [ 0.12792832  0.00287461  0.00287461 ...  0.00287461 -0.0746001\n",
      "    -0.25235778]\n",
      "   [-0.03365928 -0.07382786 -0.07382786 ... -0.07382786 -0.13413763\n",
      "    -0.22631574]\n",
      "   [ 0.17410326  0.21180254  0.21180254 ...  0.21180254  0.15074915\n",
      "    -0.01709837]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.07580793 -0.16929919 -0.16929919 ... -0.16929919 -0.18750525\n",
      "    -0.15864629]\n",
      "   [-0.12560636 -0.10482609 -0.10482609 ... -0.10482609 -0.03335309\n",
      "    -0.00674289]\n",
      "   [-0.12560636 -0.10482609 -0.10482609 ... -0.10482609 -0.03335309\n",
      "    -0.00674289]\n",
      "   ...\n",
      "   [-0.12560636 -0.10482609 -0.10482609 ... -0.10482609 -0.03335309\n",
      "    -0.00674289]\n",
      "   [ 0.00683111  0.18332773  0.18332773 ...  0.18332773  0.22397655\n",
      "     0.22437227]\n",
      "   [-0.03171492  0.21320629  0.21320629 ...  0.21320629  0.28778827\n",
      "     0.31001425]]\n",
      "\n",
      "  [[-0.22736466 -0.553577   -0.553577   ... -0.553577   -0.39944988\n",
      "    -0.11149079]\n",
      "   [ 0.1631558  -0.12457353 -0.12457353 ... -0.12457353  0.0662359\n",
      "     0.06804335]\n",
      "   [ 0.1631558  -0.12457353 -0.12457353 ... -0.12457353  0.0662359\n",
      "     0.06804335]\n",
      "   ...\n",
      "   [ 0.1631558  -0.12457353 -0.12457353 ... -0.12457353  0.0662359\n",
      "     0.06804335]\n",
      "   [ 0.140356    0.0699054   0.0699054  ...  0.0699054   0.32376015\n",
      "     0.21166205]\n",
      "   [ 0.2700191   0.41029942  0.41029942 ...  0.41029942  0.5255416\n",
      "     0.3611701 ]]\n",
      "\n",
      "  [[ 0.7147291   0.57946026  0.57946026 ...  0.57946026  0.28337812\n",
      "     0.08777153]\n",
      "   [ 0.6940317   0.7091658   0.7091658  ...  0.7091658   0.47845465\n",
      "     0.2997192 ]\n",
      "   [ 0.6940317   0.7091658   0.7091658  ...  0.7091658   0.47845465\n",
      "     0.2997192 ]\n",
      "   ...\n",
      "   [ 0.6940317   0.7091658   0.7091658  ...  0.7091658   0.47845465\n",
      "     0.2997192 ]\n",
      "   [ 0.4666323   0.5616723   0.5616723  ...  0.5616723   0.41924262\n",
      "     0.25838125]\n",
      "   [ 0.39844084  0.5516213   0.5516213  ...  0.5516213   0.49573106\n",
      "     0.359703  ]]]]\n",
      "(1, 240, 1024, 640)\n"
     ]
    }
   ],
   "source": [
    "net = Conv2d(120, 240, 4, has_bias=False, weight_init='normal')\n",
    "x = Tensor(np.ones([1, 120, 1024, 640]), mstype.float32)\n",
    "output = net(x)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
