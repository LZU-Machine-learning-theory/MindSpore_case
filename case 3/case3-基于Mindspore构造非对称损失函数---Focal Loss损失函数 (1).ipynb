{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c43f128",
   "metadata": {},
   "source": [
    " # 基于Mindspore构造非对称损失函数---Focal Loss损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf21d1",
   "metadata": {},
   "source": [
    " 本小节主要介绍构造非对称损失函数的设计，使用MFocal Loss损失函数作为讲解实例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9b0a5",
   "metadata": {},
   "source": [
    "Focal Loss是常见的非对称损失函数，该损失函数通过引入一个可调节的指数因子来降低容易被正确分类的样本的权重，从而使模型更加关注那些难以分类的样本。FocalLoss函数解决了类别不平衡的问题。\n",
    "\n",
    "FocalLoss函数由Kaiming团队在论文 Focal Loss for Dense Object Detection 中提出，提高了图像目标检测的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50a932",
   "metadata": {},
   "source": [
    "非对称损失函数是指在二元分类问题中，将错误分类的影响不对称地考虑在损失函数中。具体来\n",
    "说，当将正样本错误分类为负样本时，损失函数的值与将负样本错误分类为正样本时所得到的损\n",
    "失不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b53b0",
   "metadata": {},
   "source": [
    "非对称损失函数可以提高模型对于特定类型错误分类的鲁棒性，常用于一些具有特定需求的应用场景，例如医疗诊断中对于假阴性或假阳性的不同考虑，或者金融风控中对于错过欺诈案件和误判为欺诈的不同风险评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a658e14",
   "metadata": {},
   "source": [
    "### 函数如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf756b",
   "metadata": {},
   "source": [
    "$$FL(p_t)=-\\alpha_t（1-p_t）^\\upsilon log(p_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75c87a",
   "metadata": {},
   "source": [
    "### 参数："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b703f",
   "metadata": {},
   "source": [
    "gamma(float)-gamma用于调整Focal Loss的权重曲线的陡峭程度。默认值：2.0。\n",
    "\n",
    "weight(Union[Tensor,None])-Focal Loss的权重，维度为1。如果为None，则不使用权重。默认值：None。\n",
    "\n",
    "reduction(str)-loss的计算方式。取值为”mean”，”sum”，或”none”。默认值：”mean”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b80e4",
   "metadata": {},
   "source": [
    "### 输入："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91436d7",
   "metadata": {},
   "source": [
    "logits(Tensor) - shape为(N,C)、(N,C,H)、或(N,C,H,W)的Tensor，其中C是分类的数量，值大于1。如果shape为 (N,C,H,W)或 (N,C,H)，则H或H和W的乘积应与 labels的相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c33bd9",
   "metadata": {},
   "source": [
    "labels(Tensor)-shape为(N,C)、(N,C,H)、或(N,C,H,W)的Tensor，C的值为1，或者与logits的C相同。如果C不为1，则shape应与logits的shape相同，其中C是分类的数量。如果shape为 (N,C,H,W)或 (N,C,H) ，则H或H和W的乘积应与logits 相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e8154",
   "metadata": {},
   "source": [
    "### 输出："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898223e",
   "metadata": {},
   "source": [
    "Tensor或Scalar，如果reduction为”none”，其shape与logits相同。否则，将返回Scalar。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6c641",
   "metadata": {},
   "source": [
    "### 定义损失函数focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "264fb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "\n",
    "class FocalLoss(nn.loss.SoftmaxCrossEntropyWithLogits):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi-class classification\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, **kwargs):\n",
    "        super(FocalLoss, self).__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.sigmoid = nn.Sigmoid()  # sigmoid激活函数用于将logits转换为概率值\n",
    "        self.reduce_sum = ops.ReduceSum() # 用于计算各维度上元素之和\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        \"\"\"\n",
    "        :param logits: model's predictions, shape of [batch_size, num_classes]\n",
    "        :param label: ground truth labels, shape of [batch_size, num_classes]\n",
    "        :return: focal loss\n",
    "        \"\"\"\n",
    "        logits = self.sigmoid(logits)  # 将logits转换为概率值\n",
    "        ce_loss = super(FocalLoss, self).construct(logits, label) # 计算交叉熵损失\n",
    "\n",
    "        pt = self.reduce_sum(logits * label, axis=1)   # 计算类别预测概率pt\n",
    "        fp = ops.Pow()(1 - pt, self.gamma)            # 计算focusing parameter fp\n",
    "  \n",
    "        weight = label * self.alpha + (1 - label) * (1 - self.alpha)  # 计算balanced weight\n",
    "\n",
    "        fl_loss = weight * fp * ce_loss    # 计算focal loss\n",
    "        fl_loss = self.reduce_sum(fl_loss, axis=1)  # 按照样本维度求和\n",
    "\n",
    "        return fl_loss.mean()    # 返回平均focal loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32307727",
   "metadata": {},
   "source": [
    "### 测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "748e75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "import numpy as np\n",
    "\n",
    "class TestFocalLoss:\n",
    "    def test_focal_loss(self):\n",
    "        batch_size = 4\n",
    "        num_classes = 3\n",
    "        gamma = 2.0\n",
    "        alpha = 0.25\n",
    "\n",
    "        # 创建 Focal Loss 对象\n",
    "        focal_loss = nn.FocalLoss(gamma=gamma, alpha=alpha)\n",
    "\n",
    "        # 生成随机的 logits 和 label\n",
    "        logits = np.random.randn(batch_size, num_classes)\n",
    "        label = np.random.randint(0, num_classes, (batch_size, num_classes))\n",
    "\n",
    "        # 计算损失\n",
    "        output = focal_loss(ms.Tensor(logits), ms.Tensor(label))\n",
    "        assert output.shape == (), f\"Focal Loss shape {output.shape} doesn't match expected shape ()\"\n",
    "\n",
    "        # 检查损失是否为标量\n",
    "        assert output.asnumpy().dtype == np.float32, f\"Focal Loss dtype {output.asnumpy().dtype} doesn't match expected dtype np.float32\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
