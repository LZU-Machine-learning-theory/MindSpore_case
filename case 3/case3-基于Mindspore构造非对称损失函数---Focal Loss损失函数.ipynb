{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c43f128",
   "metadata": {},
   "source": [
    " # 基于Mindspore构造非对称损失函数---Focal Loss损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf21d1",
   "metadata": {},
   "source": [
    " 本小节主要介绍构造非对称损失函数的设计，使用MFocal Loss损失函数作为讲解实例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50a932",
   "metadata": {},
   "source": [
    "非对称损失函数是指在二元分类问题中，将错误分类的影响不对称地考虑在损失函数中。具体来\n",
    "说，当将正样本错误分类为负样本时，损失函数的值与将负样本错误分类为正样本时所得到的损\n",
    "失不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b53b0",
   "metadata": {},
   "source": [
    "非对称损失函数可以提高模型对于特定类型错误分类的鲁棒性，常用于一些具有特定需求的应用场景，例如医疗诊断中对于假阴性或假阳性的不同考虑，或者金融风控中对于错过欺诈案件和误判为欺诈的不同风险评估。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69b03a4a",
   "metadata": {},
   "source": [
    "Focal Loss是常见的非对称损失函数，该损失函数通过引入一个可调节的指数因子来降低容易被正确分类的样本的权重，从而使模型更加关注那些难以分类的样本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf756b",
   "metadata": {},
   "source": [
    "$$FL(p_t)=-\\alpha_t（1-p_t）^\\upsilon log(p_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264fb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "\n",
    "class FocalLoss(nn.loss.SoftmaxCrossEntropyWithLogits):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi-class classification\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, **kwargs):\n",
    "        super(FocalLoss, self).__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.sigmoid = nn.Sigmoid()  # sigmoid激活函数用于将logits转换为概率值\n",
    "        self.reduce_sum = ops.ReduceSum() # 用于计算各维度上元素之和\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        \"\"\"\n",
    "        :param logits: model's predictions, shape of [batch_size, num_classes]\n",
    "        :param label: ground truth labels, shape of [batch_size, num_classes]\n",
    "        :return: focal loss\n",
    "        \"\"\"\n",
    "        logits = self.sigmoid(logits)  # 将logits转换为概率值\n",
    "        ce_loss = super(FocalLoss, self).construct(logits, label) # 计算交叉熵损失\n",
    "\n",
    "        pt = self.reduce_sum(logits * label, axis=1)   # 计算类别预测概率pt\n",
    "        fp = ops.Pow()(1 - pt, self.gamma)            # 计算focusing parameter fp\n",
    "  \n",
    "        weight = label * self.alpha + (1 - label) * (1 - self.alpha)  # 计算balanced weight\n",
    "\n",
    "        fl_loss = weight * fp * ce_loss    # 计算focal loss\n",
    "        fl_loss = self.reduce_sum(fl_loss, axis=1)  # 按照样本维度求和\n",
    "\n",
    "        return fl_loss.mean()    # 返回平均focal loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e39435",
   "metadata": {},
   "source": [
    "这个案例中使用MindSpore中提供的SoftmaxCrossEntropyWithLogits损失函数来计算交叉熵损失。需要先将logits转换为概率值，这里使用Sigmoid激活函数。\n",
    "然后，计算类别预测概率pt和focusing parameter fp，以及平衡参数alpha和平衡权重weight。最后，计算focal loss并返回平均值。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
