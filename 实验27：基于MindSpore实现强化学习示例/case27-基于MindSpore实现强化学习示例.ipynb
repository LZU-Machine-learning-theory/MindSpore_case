{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5732842c",
   "metadata": {},
   "source": [
    "# 27.基于MindSpore实现强化学习示例\n",
    "   \n",
    "   本小节介绍基于mindspore实现强化学习示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3344971",
   "metadata": {},
   "source": [
    "# 1.实验目的\n",
    "- 理解强化学习（Reinforcement Learning）的相关概念。\n",
    "- 理解智能体通过与环境的交互进行学习的流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f100fb1",
   "metadata": {},
   "source": [
    "# 2. 知识点介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70baac5",
   "metadata": {},
   "source": [
    "## 2.1 强化学习（Reinforcement Learning）\n",
    "\n",
    "- 强化学习（Reinforcement Learning）是机器学习的一个分支，旨在让智能体（agent）通过与环境的交互来学习最优行为策略，以最大化累积奖励或获得特定目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f2894",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1.1 强化学习的基本元素包括：\n",
    "\n",
    "- 1.环境（Environment）：智能体与其交互的外部环境，可以是真实世界或模拟环境。\n",
    "- 2.状态（State）：环境的某个特定时刻的观测值，用于描述环境的特征。\n",
    "- 3.动作（Action）：智能体在某个状态下可以执行的操作或决策。\n",
    "- 4.奖励（Reward）：在特定状态下，环境根据智能体的动作给予的反馈信号。\n",
    "- 5.策略（Policy）：智能体的行为策略，决定在给定状态下采取哪个动作。\n",
    "- 6.值函数（Value Function）：评估状态或状态-动作对的优劣程度，用于指导决策。\n",
    "- 7.学习算法（Learning Algorithm）：根据智能体与环境的交互经验，更新策略或值函数的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cc599",
   "metadata": {},
   "source": [
    "### 2.1.2 强化学习的目标：\n",
    "- 通过学习最优策略来最大化累积奖励。为了达到这个目标，智能体通过与环境的交互，观察当前状态，选择动作，接收奖励，并根据奖励信号来调整策略或值函数。强化学习算法可以分为基于值函数的方法（如Q-learning、DQN）和基于策略的方法（如Policy Gradient、Actor-Critic）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5ffe2",
   "metadata": {},
   "source": [
    "### 2.1.3 应用\n",
    "- 强化学习在许多领域都有应用，例如机器人控制、游戏玩法、自动驾驶、金融交易等。它的独特之处在于智能体通过与环境的交互进行学习，而无需依赖标注的数据集，因此适用于很多现实世界的场景，可以在复杂、未知和动态的环境中进行决策和学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ac5a4",
   "metadata": {},
   "source": [
    "## 2.2 DQN 算法\n",
    "\n",
    "- DQN（Deep Q-Network）是一种用于解决强化学习问题的深度学习算法。它结合了Q-learning和神经网络，能够学习并估计状态-动作对的Q值函数，通过使用神经网络来逼近Q值函数，实现了对高维状态空间的学习和泛化能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e0903",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "&Q(s_t,a_t) = Q(s_t,a_t) + \\eta\\cdot (R_{t+1}+\\gamma \\max_a Q(s_{t+1},a)-Q(s_t,a_t))\\\\\n",
    "&Q(s_t,a_t) = R_{t+1}+\\gamma\\cdot \\max_aQ(s_{t+1},a)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc936429",
   "metadata": {},
   "source": [
    "- iteration algorithm\n",
    "- temporal difference error（TD）: $R_{t+1}+\\gamma\\cdot \\max_aQ(s_t,a) - Q(s_t,a_t)$\n",
    "- learning objective：\n",
    "    - MES（square loss）: $E(s_t,a_t)=\\left(R_{t+1}+\\gamma\\max_aQ(s_{t+1},a)-Q(s_t,a_t)\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8dbb8",
   "metadata": {},
   "source": [
    "### 2.2.1 核心知识\n",
    "- experience replay（经验回放）\n",
    "    - 不像 q-table 的 q-learning，每一步都学习（update）该步的内容（experience）\n",
    "    - 对于 q-table 而言，每一步（step）都学习该步的内容，神经网络连续地学习时间上相关性高的内容（事实上，时间 $t$ 的学习内容，和时间         - $t+1$ 的学习内容非常相似，这样的话，收敛就会很慢；\n",
    "    - 而是将每一步（step）的内容存储在经验池（experience pool）并随机从经验池中提取内容（replay，回放）让NN学习；\n",
    "    - 也是一种批次化（batch），使用经验池中的多个步骤的经验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e1d00",
   "metadata": {},
   "source": [
    "###  2.2.2 DQN算法的基本步骤：\n",
    "\n",
    "- 1.初始化Q网络，目标Q网络和经验回放缓冲区。\n",
    "- 2.对于每个回合（episode）循环：\n",
    "     - 重置环境并获取初始状态。\n",
    "     - 对于每一步（step）循环：根据当前状态从Q网络中选择一个动作（通常使用ε-greedy策略，其中ε是一个随时间递减的参数）。\n",
    "     - 执行选定的动作，观察下一个状态和奖励。\n",
    "     - 将转移（当前状态，动作，下一个状态，奖励）存储在经验回放缓冲区中。\n",
    "     - 从经验回放缓冲区中随机采样一批转移。\n",
    "     - 计算目标Q值：对于每个样本转移，计算目标Q值作为\n",
    "      - target_Q = reward + discount_factor * max(Q(next_state, next_action))\n",
    "     - 其中 next_action 是从目标Q网络中选择的下一个动作。\n",
    "     - 通过最小化目标Q值和当前Q值的均方误差来更新Q网络的参数。\n",
    "     - 定期更新目标Q网络的参数，例如每隔一定的步数。\n",
    "     - 更新当前状态为下一个状态。\n",
    "     - 如果达到终止条件（例如，达到最大步数或解决了环境），则跳出循环。\n",
    "- 3.返回训练好的Q网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8a322",
   "metadata": {},
   "source": [
    "## 2.3 gym平台介绍\n",
    "本实验借助了gym平台的环境，该平台由 openai 公司开发，且提供了一整套与平台中虚拟环境进行交互的 api接口，gym 的推出为强化学习算法的研究提供了更好地基准测试平台，同时将各类 环境标准化，使研究员可以专注于算法研究而无需花过多的时间在环境的模拟上。gym 提供一个 step 函数供智能体与环境进行交互，其参数为动作，主要返回值及含义分别为：\n",
    "- state：表示智能体所处环境的当前状态，代表着智能体的观察值即状态。\n",
    "- reward：表示智能体采取操作后从环境中获得的奖励，其类型可能是整数、小数等，但是具体的规模和类型与具体的规模和类型与环境有关，但是智能体的总目标仍然是获取最大的奖励值。\n",
    "- done: 大多数任务都属于阶段性任务，当到达一定条件的时候表示任务已经结束，比如五子棋游戏中的一方五子相连，机器人在路面上摔倒，或者在规定的步数以内没有完成任务，则都代表任务结束。所以 done 是一个判断条件，类型为布尔值，代表当前任务是否结束，如果结束则可以选择使用 reset 函数重置当前任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108f839",
   "metadata": {},
   "source": [
    "# 3. 实验环境\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=1.8；Python环境=3.7\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore1.8 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU CUDA 10.1|Linux-x86_64| MindSpore1.8 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore1.8 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d1d2b",
   "metadata": {},
   "source": [
    "# 4. 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa378e",
   "metadata": {},
   "source": [
    "## 4.1 实验准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6fa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "# 常见算子操作\n",
    "from mindspore import  ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7730d48",
   "metadata": {},
   "source": [
    "## 4.2 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6cd8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 2000\n",
    "# 加载车杆模型\n",
    "env = gym.make('CartPole-v1')\n",
    "env = env.unwrapped\n",
    "# 获取动作数\n",
    "N_ACTIONS = env.action_space.n\n",
    "# 获取状态数\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379b9e0",
   "metadata": {},
   "source": [
    "# 5.算法实现\n",
    "- 使用MindSpore实现DQN算法并进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0197c3",
   "metadata": {},
   "source": [
    "## 5.1 导入Python库并配置运行信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ffbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常见算子操作\n",
    "from mindspore.ops import function as F\n",
    "# 引入张量模块\n",
    "from mindspore import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b9278",
   "metadata": {},
   "source": [
    "## 5.2 定义神经网络模型 Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47ab83",
   "metadata": {},
   "source": [
    "定义了一个神经网络模型 Net，使用了 mindspore 框架的 nn 模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc42fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Cell):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        # 一个全连接层\n",
    "        self.fc1 = nn.Dense(N_STATES, 20)\n",
    "        # 第二个全连接层\n",
    "        self.fc2 = nn.Dense(20, 50)\n",
    "        # 第s三个全连接层\n",
    "        self.fc3 = nn.Dense(50, N_ACTIONS)\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def construct(self, x):\n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        # 全连接层\n",
    "        x = self.fc2(x)\n",
    "        # 激活层\n",
    "        x = self.relu(x)\n",
    "        # 全连接层\n",
    "        x = self.fc3(x)\n",
    "        # 输出\n",
    "        actions_value = self.relu(x)\n",
    "        # 返回输出值\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c666e72",
   "metadata": {},
   "source": [
    "这个网络模型具有三个全连接层和一个激活函数层，可以接受输入 x 并输出 actions_value。在神经网络中，这个模型的作用是将输入映射到输出，用于近似 Q-值函数的估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e4ee5",
   "metadata": {},
   "source": [
    "## 5.3 构建DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf12846",
   "metadata": {},
   "source": [
    "实现DQN（Deep Q-Network）算法的类 DQN，包括网络的初始化、动作选择、记忆存储和学习过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8bb81",
   "metadata": {},
   "source": [
    "### 5.3.1 初始化：\n",
    "\n",
    "- 初始化DQN智能体。创建两个Net类的实例（eval_net和target_net）。\n",
    "- learn_step_counter用于跟踪目标网络更新的步数。\n",
    "- memory_counter用于跟踪存储在内存中的转换数。\n",
    "- memory数组用于存储转换（状态、动作、奖励、下一个状态）。\n",
    "- optimizer是Adam优化器的实例，用于更新神经网络参数。\n",
    "- loss_func是均方误差（MSE）损失函数，用于计算损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a690b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        # 更新目标\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        # 存储记忆\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        # 记忆初始化\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
    "        # Adaptive Moment Estimation 动态调整参数学习率\n",
    "        self.optimizer = nn.Adam(self.eval_net.trainable_params(), learning_rate=LR)\n",
    "       # MSE损失函数\n",
    "        self.loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa69591",
   "metadata": {},
   "source": [
    "### 5.3.2 选择动作：\n",
    "- choose_action方法以状态x作为输入，并根据ε-贪心策略选择动作。\n",
    "- 如果随机生成的数小于探索率（EPSILON），则从评估网络（eval_net）中选择具有最高Q值的动作。\n",
    "- 否则，选择一个随机动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963bb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def choose_action(self, x):\n",
    "        # 输入样本\n",
    "        x = Tensor([x])\n",
    "        # greedy贪心\n",
    "        if np.random.uniform() < EPSILON:   \n",
    "            # 获取所有动作值\n",
    "            actions_value = self.eval_net(x)\n",
    "            # 获取最优动作\n",
    "            action = np.array(ops.max(actions_value), dtype=int)\n",
    "            # 调整维度\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        else:   \n",
    "            # 随机获取动作值\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            # 调整维度\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        action = np.array(action, dtype=int)\n",
    "        # 返回动作值\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3996df",
   "metadata": {},
   "source": [
    "### 5.3.3 存储转换：\n",
    "\n",
    "- store_transition方法将转换（状态、动作、奖励、下一个状态）存储在内存中。\n",
    "- 它将转换的各个组件连接成一个数组，并将其添加到内存的当前内存计数器索引处。\n",
    "- 内存计数器递增，以跟踪存储的转换数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0674b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # 更新记忆内容\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ce4ec",
   "metadata": {},
   "source": [
    "### 5.3.4 学习：\n",
    "\n",
    "- learn方法通过训练评估网络（eval_net）来更新DQN。\n",
    "- 首先，它根据learn_step_counter判断是否是更新目标网络（target_net）的时机。\n",
    "- 然后，从内存中采样一批转换，并将其分离为各个组件（状态、动作、奖励、下一个状态）。\n",
    "- 使用评估网络计算选定动作的当前Q值。\n",
    "- 通过考虑奖励和目标网络中下一个状态的最大Q值来计算目标Q值。\n",
    "- 损失函数用于计算当前Q值和目标Q值之间的损失。\n",
    "- 优化器用于根据损失更新评估网络的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6672a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def learn(self):\n",
    "        # 目标参数更新\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            # 加载网络参数\n",
    "            ms.load_param_into_net(self.target_net, self.eval_net.parameters_dict())\n",
    "        self.learn_step_counter += 1\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = Tensor([b_memory[:, :N_STATES]])\n",
    "        b_a = Tensor([b_memory[:, N_STATES:N_STATES+1].astype(int)])\n",
    "        b_r = Tensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = Tensor([b_memory[:, -N_STATES:]])\n",
    "        # 计算现实Q值\n",
    "        q_eval = self.eval_net(b_s).gather_elements(2, b_a)  # shape (batch, 1\n",
    "        # 从计算图中分离，阻止反向传播\n",
    "        q_next = ops.diag(self.target_net(b_s_))\n",
    "        # 计算目标Q值\n",
    "        q_target = b_r + GAMMA * q_next[0][0][0].max(2).view((BATCH_SIZE, 1))   # shape (batch, 1)\n",
    "        # 损失函数\n",
    "        loss_net = nn.WithLossCell(self.eval_net, loss_fn = self.loss_func(q_eval, q_target))\n",
    "        # 梯度更新\n",
    "        train_net = nn.TrainOneStepCell(loss_net, self.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfecd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20f49d",
   "metadata": {},
   "source": [
    "# 6.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29be0c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "Epoch:  194 | Epoch_reward:  1.39\n",
      "Epoch:  195 | Epoch_reward:  3.15\n",
      "Epoch:  196 | Epoch_reward:  2.67\n",
      "Epoch:  197 | Epoch_reward:  2.46\n",
      "Epoch:  198 | Epoch_reward:  2.84\n",
      "Epoch:  199 | Epoch_reward:  1.62\n",
      "Epoch:  200 | Epoch_reward:  2.59\n",
      "Epoch:  201 | Epoch_reward:  4.15\n",
      "Epoch:  202 | Epoch_reward:  2.66\n",
      "Epoch:  203 | Epoch_reward:  2.27\n",
      "Epoch:  204 | Epoch_reward:  0.77\n",
      "Epoch:  205 | Epoch_reward:  1.58\n",
      "Epoch:  206 | Epoch_reward:  3.05\n",
      "Epoch:  207 | Epoch_reward:  1.11\n",
      "Epoch:  208 | Epoch_reward:  2.52\n",
      "Epoch:  209 | Epoch_reward:  2.5\n",
      "Epoch:  210 | Epoch_reward:  1.71\n",
      "Epoch:  211 | Epoch_reward:  2.42\n",
      "Epoch:  212 | Epoch_reward:  1.51\n",
      "Epoch:  213 | Epoch_reward:  2.91\n",
      "Epoch:  214 | Epoch_reward:  2.32\n",
      "Epoch:  215 | Epoch_reward:  1.89\n",
      "Epoch:  216 | Epoch_reward:  1.79\n",
      "Epoch:  217 | Epoch_reward:  2.19\n",
      "Epoch:  218 | Epoch_reward:  1.35\n",
      "Epoch:  219 | Epoch_reward:  1.26\n",
      "Epoch:  220 | Epoch_reward:  2.25\n",
      "Epoch:  221 | Epoch_reward:  2.66\n",
      "Epoch:  222 | Epoch_reward:  2.35\n",
      "Epoch:  223 | Epoch_reward:  2.54\n",
      "Epoch:  224 | Epoch_reward:  2.94\n",
      "Epoch:  225 | Epoch_reward:  1.66\n",
      "Epoch:  226 | Epoch_reward:  3.24\n",
      "Epoch:  227 | Epoch_reward:  2.58\n",
      "Epoch:  228 | Epoch_reward:  2.94\n",
      "Epoch:  229 | Epoch_reward:  1.12\n",
      "Epoch:  230 | Epoch_reward:  1.14\n",
      "Epoch:  231 | Epoch_reward:  2.34\n",
      "Epoch:  232 | Epoch_reward:  2.31\n",
      "Epoch:  233 | Epoch_reward:  3.0\n",
      "Epoch:  234 | Epoch_reward:  2.52\n",
      "Epoch:  235 | Epoch_reward:  2.65\n",
      "Epoch:  236 | Epoch_reward:  1.38\n",
      "Epoch:  237 | Epoch_reward:  2.58\n",
      "Epoch:  238 | Epoch_reward:  2.83\n",
      "Epoch:  239 | Epoch_reward:  1.91\n",
      "Epoch:  240 | Epoch_reward:  3.22\n",
      "Epoch:  241 | Epoch_reward:  1.47\n",
      "Epoch:  242 | Epoch_reward:  2.53\n",
      "Epoch:  243 | Epoch_reward:  2.41\n",
      "Epoch:  244 | Epoch_reward:  2.09\n",
      "Epoch:  245 | Epoch_reward:  2.98\n",
      "Epoch:  246 | Epoch_reward:  2.71\n",
      "Epoch:  247 | Epoch_reward:  2.28\n",
      "Epoch:  248 | Epoch_reward:  1.25\n",
      "Epoch:  249 | Epoch_reward:  4.11\n",
      "Epoch:  250 | Epoch_reward:  2.75\n",
      "Epoch:  251 | Epoch_reward:  3.69\n",
      "Epoch:  252 | Epoch_reward:  2.74\n",
      "Epoch:  253 | Epoch_reward:  3.39\n",
      "Epoch:  254 | Epoch_reward:  2.77\n",
      "Epoch:  255 | Epoch_reward:  2.83\n",
      "Epoch:  256 | Epoch_reward:  2.37\n",
      "Epoch:  257 | Epoch_reward:  3.44\n",
      "Epoch:  258 | Epoch_reward:  1.55\n",
      "Epoch:  259 | Epoch_reward:  1.06\n",
      "Epoch:  260 | Epoch_reward:  1.75\n",
      "Epoch:  261 | Epoch_reward:  2.06\n",
      "Epoch:  262 | Epoch_reward:  1.08\n",
      "Epoch:  263 | Epoch_reward:  2.92\n",
      "Epoch:  264 | Epoch_reward:  2.17\n",
      "Epoch:  265 | Epoch_reward:  1.06\n",
      "Epoch:  266 | Epoch_reward:  2.71\n",
      "Epoch:  267 | Epoch_reward:  1.59\n",
      "Epoch:  268 | Epoch_reward:  2.41\n",
      "Epoch:  269 | Epoch_reward:  2.83\n",
      "Epoch:  270 | Epoch_reward:  3.56\n",
      "Epoch:  271 | Epoch_reward:  2.85\n",
      "Epoch:  272 | Epoch_reward:  2.51\n",
      "Epoch:  273 | Epoch_reward:  1.52\n",
      "Epoch:  274 | Epoch_reward:  1.0\n",
      "Epoch:  275 | Epoch_reward:  0.99\n",
      "Epoch:  276 | Epoch_reward:  1.74\n",
      "Epoch:  277 | Epoch_reward:  2.96\n",
      "Epoch:  278 | Epoch_reward:  1.37\n",
      "Epoch:  279 | Epoch_reward:  1.17\n",
      "Epoch:  280 | Epoch_reward:  2.8\n",
      "Epoch:  281 | Epoch_reward:  2.23\n",
      "Epoch:  282 | Epoch_reward:  2.76\n",
      "Epoch:  283 | Epoch_reward:  1.22\n",
      "Epoch:  284 | Epoch_reward:  1.64\n",
      "Epoch:  285 | Epoch_reward:  1.14\n",
      "Epoch:  286 | Epoch_reward:  2.65\n",
      "Epoch:  287 | Epoch_reward:  3.25\n",
      "Epoch:  288 | Epoch_reward:  1.66\n",
      "Epoch:  289 | Epoch_reward:  2.24\n",
      "Epoch:  290 | Epoch_reward:  1.12\n",
      "Epoch:  291 | Epoch_reward:  1.91\n",
      "Epoch:  292 | Epoch_reward:  1.95\n",
      "Epoch:  293 | Epoch_reward:  1.29\n",
      "Epoch:  294 | Epoch_reward:  4.11\n",
      "Epoch:  295 | Epoch_reward:  3.32\n",
      "Epoch:  296 | Epoch_reward:  2.88\n",
      "Epoch:  297 | Epoch_reward:  2.69\n",
      "Epoch:  298 | Epoch_reward:  1.24\n",
      "Epoch:  299 | Epoch_reward:  2.8\n"
     ]
    }
   ],
   "source": [
    "print('开始训练...')\n",
    "for i_episode in range(300):\n",
    "    # 重置环境\n",
    "    s = env.reset()\n",
    "    # 总奖励\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        # env.render()\n",
    "        a = dqn.choose_action(s)\n",
    "        # 获取训练过程相关参数\n",
    "        s_, r, done, info = env.step(a)\n",
    "        # 修改奖励\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        # 计算奖励值\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        r = r1 + r2\n",
    "        # 更新记忆\n",
    "        dqn.store_transition(s, a, r, s_)\n",
    "        # 计算总奖励\n",
    "        ep_r += r\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            # 从记忆中学习\n",
    "            dqn.learn()\n",
    "            if done:\n",
    "                print('Epoch: ', i_episode, '| Epoch_reward: ', round(ep_r, 2))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        s = s_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "41c3f9e492ade673739df9dcc3f490d48f000d8ba492379ec9aafb7473bcf948"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
