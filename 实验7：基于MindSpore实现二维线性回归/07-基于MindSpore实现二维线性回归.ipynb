{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于MindSpore实现二维线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验将实现基于MindSpore的二维线性回归，包括人造数据集、模型构建、模型预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、实验目的\n",
    "- 掌握如何使用MindSpore实现二维线性回归模型。\n",
    "- 了解如何使用MindSpore的Adam优化器和MSE损失函数\n",
    "- 了解基于MindSpore的模型训练和模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、整体模型介绍\n",
    "- 二维线性回归原理\n",
    "\n",
    "在机器学习领域，线性回归模型记为：\n",
    "$$y=w_0x_0+w_1x_1+\\cdots+w_nx_n+b=[w_0 w_1 w_2 \\cdots w_n][x_0 x_1 x_2 \\cdots x_n]^T+b$$\n",
    "可以统一形式为：\n",
    "$$y=\\sum_{i=0}^{n}w_ix_i+b=w^Tx+b$$\n",
    "\n",
    "\n",
    "并且我们定义损失函数来度量模型一次预测的好坏，即预测值$\\widehat y$和真实值y的误差，线性损失函数一般取$L=\\frac{1}{2}(y-\\widehat y)^2$，平方损失函数的几何意义是欧氏距离。\n",
    "\n",
    "之后我们便可进行模型训练，采用梯度下降方法求模型参数w，使损失函数最小。\n",
    "\n",
    "梯度下降法顺着当前点梯度反方向，按规定步长$\\alpha$进行迭代搜索，对第i个模型参数进行如下更新：\n",
    "$$w_{i+1}=w_i-\\alpha \\frac{\\partial L(w)}{\\partial (w_i)}$$\n",
    "因为\n",
    "$$\\frac{\\partial L(w)}{\\partial (w_i)}=-\\sum_{j=0}^{m}[y^{(j)}-\\sum_{i=0}^{n}w_i x_i^{(j)}-b]*x_i^{(j)}$$\n",
    "所以\n",
    "$$w_{i+1}=w_i+\\alpha [\\sum_{j=0}^{m}(y^{(j)}-\\sum_{i=0}^{n}w_i x_i^{(j)}-b)*x_i^{(j)}]$$\n",
    "对每个模型参数迭代训练直到收敛即可。因此二维线性回归的自变量为两个，模型便为：\n",
    "$$y=w_0x_0+w_1x_1+b=w^Tx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们需要定义一个模型Net实现二维线性回归功能。<br>\n",
    "可以调用MindSpore的nn.MSELoss计算预测值和目标值之间的均方误差。<br>\n",
    "调用MindSpore的nn.Adam计算最优参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 实验环境\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=2.0；Python环境=3.7\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU CUDA 10.1|Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、数据处理\n",
    "#### 4.1 数据准备\n",
    "为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。任务是使用这个有限样本的数据集来恢复这个模型的参数。我们需要生成一个包含1000个样本的数据集，每个样本包含从标准正态分布中采样的2个特征。 合成的数据集是一个矩阵$X∈R^{(1000×2)}$。我们使用线性模型参数$w =[2,-3.4]^T$、b=3.2和噪声项$\\epsilon$生成数据集及其标签：\n",
    "$y=Xw+b+\\epsilon=w_0x_0+w_1x_1+b+\\epsilon$\n",
    "\n",
    "$\\epsilon$可以视为模型预测和标签之间的观测误差，我们假设ϵ服从均值为0、标准差为0.01的正态分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random库实现了各种分布的伪随机数生成器；dtype是MindSpore数据类型的对象；mindspore.ops提供对神经网络层的各种操作；pyplot是常用的绘图模块，能很方便让用户绘制 2D 图表；mindspore中的Tensor是张量，可放在gpu上加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import mindspore\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.ops as ops\n",
    "from matplotlib import pyplot as plt\n",
    "from mindspore import Tensor\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "def synthetic_data(w, b, num_examples):  \n",
    "    print((num_examples, len(w)))\n",
    "    # 生成X\n",
    "    X = ops.normal((num_examples, len(w)), Tensor(0, mstype.int32), Tensor(1, mstype.int32)) \n",
    "    # y = Xw + b\n",
    "    y = ops.matmul(X, w) + b                               \n",
    "    # y = Xw + b + 噪声。\n",
    "    y += ops.normal(y.shape, Tensor(0, mstype.int32), Tensor(0.01, mstype.float32))          \n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "mindspore.set_seed(1)\n",
    "true_w = Tensor([2, -3.4], mstype.float32)\n",
    "true_b = 4.2\n",
    "# 人造数据\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 数据加载\n",
    "数据features中的每一行都包含一个二维数据样本，真实值labels中的每一行都包含一维标签值（一个标量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [[ 0.15606569  0.30639967]\n",
      " [-0.5680398  -0.42438623]\n",
      " [-0.8062886  -0.20454668]\n",
      " [-1.2000442  -0.42873764]] \n",
      "label: [[3.487765 ]\n",
      " [4.4927335]\n",
      " [3.2986383]\n",
      " [3.2585537]]\n"
     ]
    }
   ],
   "source": [
    "print('features:', features[0:4],'\\nlabel:', labels[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出第二个特征与真实值的散点图\n",
    "plt.scatter(features[:, (1)].asnumpy(), labels.asnumpy(), 1); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用MindSpore的GeneratorDataset创建可迭代数据。<br>\n",
    "dataset模块提供了加载和处理数据集的API；numpy提供了一系列类NumPy接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "from mindspore import dataset as ds  \n",
    "import numpy as np\n",
    "\n",
    "def data_iter(features, labels):\n",
    "    num_examples = len(features)                                 \n",
    "    for i in range(0, num_examples):\n",
    "        x = features[i]\n",
    "        y = labels[i]\n",
    "        yield np.array(x[:]).astype(np.float32), np.array([y[0]]).astype(np.float32)\n",
    "\n",
    "        \n",
    "batch_size = 10\n",
    "# 创建GeneratorDataset对象\n",
    "dataset = ds.GeneratorDataset(list(data_iter(features, labels)),column_names=['data', 'label'])\n",
    "# 将数据集中连续10条数据合并为一个批处理数据\n",
    "dataset = dataset.batch(batch_size)                \n",
    "# 重复数据集1次\n",
    "dataset = dataset.repeat(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型构建分为定义实现二维线性回归功能的Net、用于计算预测值与标签值之间的均方误差MSELoss、Adam优化器。\n",
    "- 定义模型Net\n",
    "\n",
    "Net输入为X样本，计算出预测值$\\widehat y$并返回。<br>\n",
    "mindspore.nn用于构建神经网络中的预定义构建块或计算单元；Parameter 是 Tensor 的子类，当它们被绑定为Cell的属性时，会自动添加到其参数列表中，并且可以通过Cell的某些方法获取；mindspore.common.initializer用于初始化神经元参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore import Parameter\n",
    "from mindspore.common.initializer import initializer, Zero, Normal\n",
    "\n",
    "\n",
    "def linreg(x, w, b):\n",
    "    # y = Xw+b\n",
    "    return ops.matmul(x, w) + b    \n",
    "\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = Parameter(initializer(Normal(0.01, 0), (2, 1), mstype.float32))\n",
    "        self.b = Parameter(initializer(Zero(), 1, mstype.float32))\n",
    "        \n",
    "    def construct(self, x):\n",
    "        # y_hat = Xw+b\n",
    "        y_hat = linreg(x, self.w, self.b)  \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "# Net用于实现二维线性回归\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用MSE损失函数和Adam优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练的eopch为3\n",
    "num_epochs = 3\n",
    "# 学习率为0.03\n",
    "lr = 0.03\n",
    "# Adam优化器\n",
    "optim = nn.Adam(net.trainable_params(), learning_rate=lr)          \n",
    "# 计算预测值与标签值之间的均方误差\n",
    "loss = nn.MSELoss()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、模型训练\n",
    "Model是模型训练与推理的高阶接口，调用Model.train方法，传入数据集即可完成模型训练，其中LossMonitor()会监控训练的损失，并将epoch、step、loss信息打印出来，若loss变为NAN或INF，则会终止训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train import Model                                       \n",
    "from mindspore.train import LossMonitor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 27.21526527404785\n",
      "epoch: 1 step: 2, loss is 29.336599349975586\n",
      "epoch: 1 step: 3, loss is 63.4631462097168\n",
      "epoch: 1 step: 4, loss is 35.25086212158203\n",
      "epoch: 1 step: 5, loss is 71.31114959716797\n",
      "epoch: 1 step: 6, loss is 42.622318267822266\n",
      "epoch: 1 step: 7, loss is 40.763362884521484\n",
      "epoch: 1 step: 8, loss is 16.727262496948242\n",
      "epoch: 1 step: 9, loss is 23.652950286865234\n",
      "epoch: 1 step: 10, loss is 33.654170989990234\n",
      "epoch: 1 step: 11, loss is 27.018291473388672\n",
      "epoch: 1 step: 12, loss is 26.514074325561523\n",
      "epoch: 1 step: 13, loss is 24.49384117126465\n",
      "epoch: 1 step: 14, loss is 48.61296844482422\n",
      "epoch: 1 step: 15, loss is 29.758121490478516\n",
      "epoch: 1 step: 16, loss is 21.304235458374023\n",
      "epoch: 1 step: 17, loss is 15.565986633300781\n",
      "epoch: 1 step: 18, loss is 36.85604476928711\n",
      "epoch: 1 step: 19, loss is 35.382205963134766\n",
      "epoch: 1 step: 20, loss is 25.852447509765625\n",
      "epoch: 1 step: 21, loss is 9.136430740356445\n",
      "epoch: 1 step: 22, loss is 27.905731201171875\n",
      "epoch: 1 step: 23, loss is 31.979320526123047\n",
      "epoch: 1 step: 24, loss is 27.572046279907227\n",
      "epoch: 1 step: 25, loss is 25.716604232788086\n",
      "epoch: 1 step: 26, loss is 18.555919647216797\n",
      "epoch: 1 step: 27, loss is 14.100408554077148\n",
      "epoch: 1 step: 28, loss is 26.788162231445312\n",
      "epoch: 1 step: 29, loss is 15.072996139526367\n",
      "epoch: 1 step: 30, loss is 19.150911331176758\n",
      "epoch: 1 step: 31, loss is 19.015939712524414\n",
      "epoch: 1 step: 32, loss is 16.49721908569336\n",
      "epoch: 1 step: 33, loss is 24.54644012451172\n",
      "epoch: 1 step: 34, loss is 28.995107650756836\n",
      "epoch: 1 step: 35, loss is 19.336854934692383\n",
      "epoch: 1 step: 36, loss is 22.72515296936035\n",
      "epoch: 1 step: 37, loss is 25.337997436523438\n",
      "epoch: 1 step: 38, loss is 21.988788604736328\n",
      "epoch: 1 step: 39, loss is 10.990190505981445\n",
      "epoch: 1 step: 40, loss is 13.780523300170898\n",
      "epoch: 1 step: 41, loss is 6.2393293380737305\n",
      "epoch: 1 step: 42, loss is 14.617647171020508\n",
      "epoch: 1 step: 43, loss is 22.81882095336914\n",
      "epoch: 1 step: 44, loss is 18.663782119750977\n",
      "epoch: 1 step: 45, loss is 28.471080780029297\n",
      "epoch: 1 step: 46, loss is 26.383655548095703\n",
      "epoch: 1 step: 47, loss is 16.54563331604004\n",
      "epoch: 1 step: 48, loss is 4.348992824554443\n",
      "epoch: 1 step: 49, loss is 12.608301162719727\n",
      "epoch: 1 step: 50, loss is 13.844345092773438\n",
      "epoch: 1 step: 51, loss is 11.507513046264648\n",
      "epoch: 1 step: 52, loss is 13.889551162719727\n",
      "epoch: 1 step: 53, loss is 10.510371208190918\n",
      "epoch: 1 step: 54, loss is 16.251087188720703\n",
      "epoch: 1 step: 55, loss is 18.734956741333008\n",
      "epoch: 1 step: 56, loss is 20.76787757873535\n",
      "epoch: 1 step: 57, loss is 20.44515037536621\n",
      "epoch: 1 step: 58, loss is 17.76775550842285\n",
      "epoch: 1 step: 59, loss is 15.186498641967773\n",
      "epoch: 1 step: 60, loss is 12.228841781616211\n",
      "epoch: 1 step: 61, loss is 5.879114627838135\n",
      "epoch: 1 step: 62, loss is 11.189396858215332\n",
      "epoch: 1 step: 63, loss is 9.96903133392334\n",
      "epoch: 1 step: 64, loss is 10.384302139282227\n",
      "epoch: 1 step: 65, loss is 7.083913326263428\n",
      "epoch: 1 step: 66, loss is 6.81948184967041\n",
      "epoch: 1 step: 67, loss is 12.903343200683594\n",
      "epoch: 1 step: 68, loss is 7.572709560394287\n",
      "epoch: 1 step: 69, loss is 3.8063812255859375\n",
      "epoch: 1 step: 70, loss is 6.538340091705322\n",
      "epoch: 1 step: 71, loss is 7.325401306152344\n",
      "epoch: 1 step: 72, loss is 14.188916206359863\n",
      "epoch: 1 step: 73, loss is 6.562997341156006\n",
      "epoch: 1 step: 74, loss is 7.03503942489624\n",
      "epoch: 1 step: 75, loss is 9.620099067687988\n",
      "epoch: 1 step: 76, loss is 3.0091288089752197\n",
      "epoch: 1 step: 77, loss is 11.34782600402832\n",
      "epoch: 1 step: 78, loss is 12.040689468383789\n",
      "epoch: 1 step: 79, loss is 13.375781059265137\n",
      "epoch: 1 step: 80, loss is 5.005685806274414\n",
      "epoch: 1 step: 81, loss is 6.137840747833252\n",
      "epoch: 1 step: 82, loss is 7.440329074859619\n",
      "epoch: 1 step: 83, loss is 9.558012008666992\n",
      "epoch: 1 step: 84, loss is 13.821830749511719\n",
      "epoch: 1 step: 85, loss is 5.639045238494873\n",
      "epoch: 1 step: 86, loss is 7.139861106872559\n",
      "epoch: 1 step: 87, loss is 4.156195640563965\n",
      "epoch: 1 step: 88, loss is 5.298253536224365\n",
      "epoch: 1 step: 89, loss is 10.114042282104492\n",
      "epoch: 1 step: 90, loss is 5.1939897537231445\n",
      "epoch: 1 step: 91, loss is 7.433219909667969\n",
      "epoch: 1 step: 92, loss is 3.3720524311065674\n",
      "epoch: 1 step: 93, loss is 7.231276035308838\n",
      "epoch: 1 step: 94, loss is 7.266561985015869\n",
      "epoch: 1 step: 95, loss is 3.8319172859191895\n",
      "epoch: 1 step: 96, loss is 3.55168080329895\n",
      "epoch: 1 step: 97, loss is 3.64298939704895\n",
      "epoch: 1 step: 98, loss is 5.807127475738525\n",
      "epoch: 1 step: 99, loss is 3.5787360668182373\n",
      "epoch: 1 step: 100, loss is 5.027612209320068\n",
      "epoch: 2 step: 1, loss is 6.962196350097656\n",
      "epoch: 2 step: 2, loss is 4.5556840896606445\n",
      "epoch: 2 step: 3, loss is 5.342075347900391\n",
      "epoch: 2 step: 4, loss is 4.486537456512451\n",
      "epoch: 2 step: 5, loss is 4.889395236968994\n",
      "epoch: 2 step: 6, loss is 5.2812724113464355\n",
      "epoch: 2 step: 7, loss is 2.938516139984131\n",
      "epoch: 2 step: 8, loss is 3.9489810466766357\n",
      "epoch: 2 step: 9, loss is 2.6869192123413086\n",
      "epoch: 2 step: 10, loss is 2.7264132499694824\n",
      "epoch: 2 step: 11, loss is 7.088787078857422\n",
      "epoch: 2 step: 12, loss is 3.632922649383545\n",
      "epoch: 2 step: 13, loss is 5.932767868041992\n",
      "epoch: 2 step: 14, loss is 2.3370344638824463\n",
      "epoch: 2 step: 15, loss is 3.6797261238098145\n",
      "epoch: 2 step: 16, loss is 3.0671768188476562\n",
      "epoch: 2 step: 17, loss is 4.967508792877197\n",
      "epoch: 2 step: 18, loss is 4.01639461517334\n",
      "epoch: 2 step: 19, loss is 4.094237327575684\n",
      "epoch: 2 step: 20, loss is 6.315237998962402\n",
      "epoch: 2 step: 21, loss is 2.252216100692749\n",
      "epoch: 2 step: 22, loss is 4.821501731872559\n",
      "epoch: 2 step: 23, loss is 1.7606475353240967\n",
      "epoch: 2 step: 24, loss is 3.7248637676239014\n",
      "epoch: 2 step: 25, loss is 2.244218111038208\n",
      "epoch: 2 step: 26, loss is 4.203402042388916\n",
      "epoch: 2 step: 27, loss is 2.20698881149292\n",
      "epoch: 2 step: 28, loss is 2.6857528686523438\n",
      "epoch: 2 step: 29, loss is 2.1136257648468018\n",
      "epoch: 2 step: 30, loss is 0.8118253946304321\n",
      "epoch: 2 step: 31, loss is 1.234073519706726\n",
      "epoch: 2 step: 32, loss is 1.7736514806747437\n",
      "epoch: 2 step: 33, loss is 2.2017874717712402\n",
      "epoch: 2 step: 34, loss is 3.117436408996582\n",
      "epoch: 2 step: 35, loss is 2.5481910705566406\n",
      "epoch: 2 step: 36, loss is 2.8056836128234863\n",
      "epoch: 2 step: 37, loss is 1.9391756057739258\n",
      "epoch: 2 step: 38, loss is 3.060429334640503\n",
      "epoch: 2 step: 39, loss is 1.8379236459732056\n",
      "epoch: 2 step: 40, loss is 1.768932580947876\n",
      "epoch: 2 step: 41, loss is 3.7196412086486816\n",
      "epoch: 2 step: 42, loss is 1.6140480041503906\n",
      "epoch: 2 step: 43, loss is 1.95694100856781\n",
      "epoch: 2 step: 44, loss is 1.5977418422698975\n",
      "epoch: 2 step: 45, loss is 1.3676258325576782\n",
      "epoch: 2 step: 46, loss is 1.616013765335083\n",
      "epoch: 2 step: 47, loss is 3.231724500656128\n",
      "epoch: 2 step: 48, loss is 1.5484251976013184\n",
      "epoch: 2 step: 49, loss is 1.317044973373413\n",
      "epoch: 2 step: 50, loss is 1.9576256275177002\n",
      "epoch: 2 step: 51, loss is 0.7844244241714478\n",
      "epoch: 2 step: 52, loss is 2.269125461578369\n",
      "epoch: 2 step: 53, loss is 1.9816278219223022\n",
      "epoch: 2 step: 54, loss is 0.9278596043586731\n",
      "epoch: 2 step: 55, loss is 1.1426912546157837\n",
      "epoch: 2 step: 56, loss is 1.4165637493133545\n",
      "epoch: 2 step: 57, loss is 2.2507853507995605\n",
      "epoch: 2 step: 58, loss is 0.9760058522224426\n",
      "epoch: 2 step: 59, loss is 1.346488356590271\n",
      "epoch: 2 step: 60, loss is 1.1192424297332764\n",
      "epoch: 2 step: 61, loss is 0.943031907081604\n",
      "epoch: 2 step: 62, loss is 0.873736560344696\n",
      "epoch: 2 step: 63, loss is 1.1059176921844482\n",
      "epoch: 2 step: 64, loss is 0.7149892449378967\n",
      "epoch: 2 step: 65, loss is 1.6973308324813843\n",
      "epoch: 2 step: 66, loss is 1.2775171995162964\n",
      "epoch: 2 step: 67, loss is 1.5566039085388184\n",
      "epoch: 2 step: 68, loss is 0.8766419291496277\n",
      "epoch: 2 step: 69, loss is 0.5857804417610168\n",
      "epoch: 2 step: 70, loss is 1.2646538019180298\n",
      "epoch: 2 step: 71, loss is 1.0398437976837158\n",
      "epoch: 2 step: 72, loss is 1.8960927724838257\n",
      "epoch: 2 step: 73, loss is 0.6992558836936951\n",
      "epoch: 2 step: 74, loss is 0.6999543309211731\n",
      "epoch: 2 step: 75, loss is 0.9262067675590515\n",
      "epoch: 2 step: 76, loss is 0.6524678468704224\n",
      "epoch: 2 step: 77, loss is 1.3097727298736572\n",
      "epoch: 2 step: 78, loss is 0.46024784445762634\n",
      "epoch: 2 step: 79, loss is 0.418347030878067\n",
      "epoch: 2 step: 80, loss is 0.8357232809066772\n",
      "epoch: 2 step: 81, loss is 0.7291088700294495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 82, loss is 0.6625293493270874\n",
      "epoch: 2 step: 83, loss is 1.1811898946762085\n",
      "epoch: 2 step: 84, loss is 0.9212194681167603\n",
      "epoch: 2 step: 85, loss is 0.5011598467826843\n",
      "epoch: 2 step: 86, loss is 0.8492897748947144\n",
      "epoch: 2 step: 87, loss is 0.5195800065994263\n",
      "epoch: 2 step: 88, loss is 0.23887589573860168\n",
      "epoch: 2 step: 89, loss is 0.5381307601928711\n",
      "epoch: 2 step: 90, loss is 0.7873405814170837\n",
      "epoch: 2 step: 91, loss is 0.5578173995018005\n",
      "epoch: 2 step: 92, loss is 0.6823797821998596\n",
      "epoch: 2 step: 93, loss is 0.5008846521377563\n",
      "epoch: 2 step: 94, loss is 0.6183363199234009\n",
      "epoch: 2 step: 95, loss is 0.5521734356880188\n",
      "epoch: 2 step: 96, loss is 0.43062639236450195\n",
      "epoch: 2 step: 97, loss is 0.36775729060173035\n",
      "epoch: 2 step: 98, loss is 0.5707271099090576\n",
      "epoch: 2 step: 99, loss is 0.4306361675262451\n",
      "epoch: 2 step: 100, loss is 0.6082412600517273\n",
      "epoch: 3 step: 1, loss is 0.41276615858078003\n",
      "epoch: 3 step: 2, loss is 0.2938767075538635\n",
      "epoch: 3 step: 3, loss is 0.5000230669975281\n",
      "epoch: 3 step: 4, loss is 0.4498678147792816\n",
      "epoch: 3 step: 5, loss is 0.2024650126695633\n",
      "epoch: 3 step: 6, loss is 0.44310227036476135\n",
      "epoch: 3 step: 7, loss is 0.2692863643169403\n",
      "epoch: 3 step: 8, loss is 0.23688945174217224\n",
      "epoch: 3 step: 9, loss is 0.26066502928733826\n",
      "epoch: 3 step: 10, loss is 0.20629675686359406\n",
      "epoch: 3 step: 11, loss is 0.4594079554080963\n",
      "epoch: 3 step: 12, loss is 0.24997739493846893\n",
      "epoch: 3 step: 13, loss is 0.20404544472694397\n",
      "epoch: 3 step: 14, loss is 0.3983539938926697\n",
      "epoch: 3 step: 15, loss is 0.11410190165042877\n",
      "epoch: 3 step: 16, loss is 0.19838684797286987\n",
      "epoch: 3 step: 17, loss is 0.28151005506515503\n",
      "epoch: 3 step: 18, loss is 0.09904325008392334\n",
      "epoch: 3 step: 19, loss is 0.19915513694286346\n",
      "epoch: 3 step: 20, loss is 0.3846835196018219\n",
      "epoch: 3 step: 21, loss is 0.1914261281490326\n",
      "epoch: 3 step: 22, loss is 0.14309437572956085\n",
      "epoch: 3 step: 23, loss is 0.13308462500572205\n",
      "epoch: 3 step: 24, loss is 0.19154636561870575\n",
      "epoch: 3 step: 25, loss is 0.16405831277370453\n",
      "epoch: 3 step: 26, loss is 0.20904812216758728\n",
      "epoch: 3 step: 27, loss is 0.14697371423244476\n",
      "epoch: 3 step: 28, loss is 0.11689909547567368\n",
      "epoch: 3 step: 29, loss is 0.16094909608364105\n",
      "epoch: 3 step: 30, loss is 0.1434142291545868\n",
      "epoch: 3 step: 31, loss is 0.09004128724336624\n",
      "epoch: 3 step: 32, loss is 0.1290574073791504\n",
      "epoch: 3 step: 33, loss is 0.08969181776046753\n",
      "epoch: 3 step: 34, loss is 0.10273350775241852\n",
      "epoch: 3 step: 35, loss is 0.1156499981880188\n",
      "epoch: 3 step: 36, loss is 0.11508677899837494\n",
      "epoch: 3 step: 37, loss is 0.07429660111665726\n",
      "epoch: 3 step: 38, loss is 0.1544957160949707\n",
      "epoch: 3 step: 39, loss is 0.07589051872491837\n",
      "epoch: 3 step: 40, loss is 0.09762391448020935\n",
      "epoch: 3 step: 41, loss is 0.12021774053573608\n",
      "epoch: 3 step: 42, loss is 0.09902794659137726\n",
      "epoch: 3 step: 43, loss is 0.0917472094297409\n",
      "epoch: 3 step: 44, loss is 0.08389367908239365\n",
      "epoch: 3 step: 45, loss is 0.06908883154392242\n",
      "epoch: 3 step: 46, loss is 0.07097391039133072\n",
      "epoch: 3 step: 47, loss is 0.0744287446141243\n",
      "epoch: 3 step: 48, loss is 0.11547797918319702\n",
      "epoch: 3 step: 49, loss is 0.07198736071586609\n",
      "epoch: 3 step: 50, loss is 0.05325499176979065\n",
      "epoch: 3 step: 51, loss is 0.08582469075918198\n",
      "epoch: 3 step: 52, loss is 0.0729944109916687\n",
      "epoch: 3 step: 53, loss is 0.06155432388186455\n",
      "epoch: 3 step: 54, loss is 0.09096769988536835\n",
      "epoch: 3 step: 55, loss is 0.07064614444971085\n",
      "epoch: 3 step: 56, loss is 0.059208162128925323\n",
      "epoch: 3 step: 57, loss is 0.04447593539953232\n",
      "epoch: 3 step: 58, loss is 0.03973168879747391\n",
      "epoch: 3 step: 59, loss is 0.05330270528793335\n",
      "epoch: 3 step: 60, loss is 0.06218930333852768\n",
      "epoch: 3 step: 61, loss is 0.05494464188814163\n",
      "epoch: 3 step: 62, loss is 0.03596225753426552\n",
      "epoch: 3 step: 63, loss is 0.030687173828482628\n",
      "epoch: 3 step: 64, loss is 0.04707220569252968\n",
      "epoch: 3 step: 65, loss is 0.0495014414191246\n",
      "epoch: 3 step: 66, loss is 0.04258064925670624\n",
      "epoch: 3 step: 67, loss is 0.04463804140686989\n",
      "epoch: 3 step: 68, loss is 0.04123527556657791\n",
      "epoch: 3 step: 69, loss is 0.03562673181295395\n",
      "epoch: 3 step: 70, loss is 0.04039211943745613\n",
      "epoch: 3 step: 71, loss is 0.02102712169289589\n",
      "epoch: 3 step: 72, loss is 0.039723530411720276\n",
      "epoch: 3 step: 73, loss is 0.029802406206727028\n",
      "epoch: 3 step: 74, loss is 0.023850535973906517\n",
      "epoch: 3 step: 75, loss is 0.028385091572999954\n",
      "epoch: 3 step: 76, loss is 0.01661636307835579\n",
      "epoch: 3 step: 77, loss is 0.024331815540790558\n",
      "epoch: 3 step: 78, loss is 0.03208054602146149\n",
      "epoch: 3 step: 79, loss is 0.02161048911511898\n",
      "epoch: 3 step: 80, loss is 0.02142120525240898\n",
      "epoch: 3 step: 81, loss is 0.02458055503666401\n",
      "epoch: 3 step: 82, loss is 0.0213459525257349\n",
      "epoch: 3 step: 83, loss is 0.013226820155978203\n",
      "epoch: 3 step: 84, loss is 0.017200905829668045\n",
      "epoch: 3 step: 85, loss is 0.01626061275601387\n",
      "epoch: 3 step: 86, loss is 0.02225835621356964\n",
      "epoch: 3 step: 87, loss is 0.008702786639332771\n",
      "epoch: 3 step: 88, loss is 0.021927032619714737\n",
      "epoch: 3 step: 89, loss is 0.021337823942303658\n",
      "epoch: 3 step: 90, loss is 0.017082367092370987\n",
      "epoch: 3 step: 91, loss is 0.009913050569593906\n",
      "epoch: 3 step: 92, loss is 0.02714329957962036\n",
      "epoch: 3 step: 93, loss is 0.015559658408164978\n",
      "epoch: 3 step: 94, loss is 0.011197875253856182\n",
      "epoch: 3 step: 95, loss is 0.013860613107681274\n",
      "epoch: 3 step: 96, loss is 0.0207115076482296\n",
      "epoch: 3 step: 97, loss is 0.01054142601788044\n",
      "epoch: 3 step: 98, loss is 0.01156577654182911\n",
      "epoch: 3 step: 99, loss is 0.010938034392893314\n",
      "epoch: 3 step: 100, loss is 0.00852242112159729\n"
     ]
    }
   ],
   "source": [
    "# 模型训练或推理的高阶接口。Model 会根据用户传入的参数封装可训练或推理的实例\n",
    "model = Model(net, loss_fn=loss, optimizer=optim)  \n",
    "# 模型训练接口。训练场景下，LossMonitor监控训练的loss；边训练边推理场景下，监控训练的loss和推理的metrics。如果loss是NAN或INF，则终止训练\n",
    "model.train(num_epochs, dataset, callbacks=[LossMonitor()])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7、模型预测\n",
    "训练3个epoch后输出w和b的估计误差，比较真实参数和通过训练学到的参数来评估训练的成功程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: [ 0.00161636 -0.03869033]\n",
      "b的估计误差: [0.09300137]\n"
     ]
    }
   ],
   "source": [
    "# w的真实值和训练值之差\n",
    "print(f'w的估计误差: {true_w - net.trainable_params()[0].reshape(true_w.shape)}')  \n",
    "# b的真实值和训练值之差\n",
    "print(f'b的估计误差: {true_b - net.trainable_params()[1]}')                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_Night",
   "language": "python",
   "name": "ms_night"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
